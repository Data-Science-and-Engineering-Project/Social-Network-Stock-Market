{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust LightGCN Pipeline\n",
    "\n",
    "A modular, date-driven LightGCN implementation for fund-stock relationship prediction.\n",
    "\n",
    "## Features:\n",
    "- **Date-driven**: Change `TARGET_DATE` to run the pipeline for any date\n",
    "- **Dual data sources**: Load from database or parquet files\n",
    "- **Modular design**: All functions are reusable and well-organized\n",
    "- **Complete pipeline**: Single `run_pipeline()` call runs everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Modify these parameters to customize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ==============================================================================\n",
    "\n",
    "# Data Source Configuration\n",
    "DATA_SOURCE = \"parquet\"  # Options: \"database\" or \"parquet\"\n",
    "PARQUET_DIR = \"../../Data/parquet_files\"  # Relative path to parquet files\n",
    "TARGET_DATE = \"2024-01-01\"  # Date in YYYY-MM-DD format\n",
    "ENV_PATH = r\"C:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.env\"\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "NUM_LAYERS = 3\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Train/Val/Test Split Ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# Training Configuration\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Evaluation Configuration\n",
    "TOP_K_VALUES = [5, 10, 20, 50]\n",
    "\n",
    "# Model Saving\n",
    "SAVE_MODEL = True\n",
    "MODEL_SAVE_DIR = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded environment from: C:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.env\n",
      "✓ Project root: c:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Try to load .env from multiple locations\n",
    "env_paths = [\n",
    "    Path(ENV_PATH),\n",
    "    Path.cwd() / '.env',\n",
    "    Path.cwd().parent / '.env',\n",
    "    Path.cwd().parent.parent / '.env',\n",
    "]\n",
    "\n",
    "for env_path in env_paths:\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"✓ Loaded environment from: {env_path}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"⚠ No .env file found - database connection may fail\")\n",
    "\n",
    "# Calculate project root path\n",
    "notebook_path = Path.cwd()\n",
    "project_root = notebook_path.parent.parent\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "PyTorch Version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# GPU/Device Setup\n",
    "# ==============================================================================\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get available device (CUDA if available, else CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"✓ Using CPU\")\n",
    "    return device\n",
    "\n",
    "# Set device\n",
    "DEVICE = get_device()\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data Loading Functions\n",
    "\n",
    "def validate_date_format(date_str: str) -> bool:\n",
    "    \"\"\"Validate date format is YYYY-MM-DD.\"\"\"\n",
    "    try:\n",
    "        datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_holdings_from_database(date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load holdings data from PostgreSQL database for a specific date.\n",
    "    \n",
    "    Args:\n",
    "        date: Date string in YYYY-MM-DD format\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: nameofissuer, cusip, sshprnamt, cik, period_start, price\n",
    "    \"\"\"\n",
    "    from ETL.data_handlers.db_data_handler.postgres_handler import PostgresHandler\n",
    "    \n",
    "    print(f\"\\n[Database] Loading data for date: {date}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Connect to database\n",
    "    print(\"Connecting to database...\")\n",
    "    handler = PostgresHandler()\n",
    "    \n",
    "    if not handler.connect():\n",
    "        raise ConnectionError(\"Failed to connect to database\")\n",
    "    \n",
    "    print(f\"✓ Connected to database: {handler.database}\")\n",
    "    \n",
    "    # Query with parameterized date\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        hf.nameofissuer,\n",
    "        hf.cusip,\n",
    "        hf.sshprnamt,\n",
    "        hf.cik,\n",
    "        hf.period_start,\n",
    "        tp.price\n",
    "    FROM holdings_filtered_new hf\n",
    "    INNER JOIN ticker_to_cusip ttc ON hf.cusip = ttc.cusip\n",
    "    INNER JOIN ticker_prices tp ON ttc.ticker = tp.ticker \n",
    "        AND hf.period_start = CAST(tp.period_start AS DATE)\n",
    "    WHERE hf.period_start = '{date}'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading data where period_start = '{date}'...\")\n",
    "    df = pd.read_sql_query(query, handler.connection)\n",
    "    print(f\"✓ Loaded {len(df):,} rows\")\n",
    "    \n",
    "    # Close connection\n",
    "    handler.disconnect()\n",
    "    print(\"✓ Database connection closed\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f\"No data found for date: {date}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_holdings_from_parquet(date: str, parquet_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load holdings data from parquet files for a specific date.\n",
    "    \n",
    "    Supports file naming conventions:\n",
    "        - holdings_filtered_new_period_start_{YYYY-MM-DD}.parquet\n",
    "        - holdings_{YYYY-MM-DD}.parquet\n",
    "        - {YYYY-MM-DD}.parquet\n",
    "    \n",
    "    Args:\n",
    "        date: Date string in YYYY-MM-DD format\n",
    "        parquet_dir: Path to directory containing parquet files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with holdings data\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Parquet] Loading data for date: {date}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Resolve parquet directory path\n",
    "    parquet_path = Path(parquet_dir)\n",
    "    if not parquet_path.is_absolute():\n",
    "        parquet_path = Path.cwd() / parquet_dir\n",
    "    \n",
    "    if not parquet_path.exists():\n",
    "        raise FileNotFoundError(f\"Parquet directory not found: {parquet_path}\")\n",
    "    \n",
    "    print(f\"Searching in: {parquet_path}\")\n",
    "    \n",
    "    # Try different naming conventions\n",
    "    naming_patterns = [\n",
    "        f\"holdings_filtered_new_period_start_{date}.parquet\",\n",
    "        f\"holdings_{date}.parquet\",\n",
    "        f\"{date}.parquet\",\n",
    "        f\"holdings_filtered_new_period_start_{date.replace('-', '_')}.parquet\",\n",
    "    ]\n",
    "    \n",
    "    parquet_file = None\n",
    "    for pattern in naming_patterns:\n",
    "        potential_file = parquet_path / pattern\n",
    "        if potential_file.exists():\n",
    "            parquet_file = potential_file\n",
    "            print(f\"✓ Found file: {pattern}\")\n",
    "            break\n",
    "    \n",
    "    if parquet_file is None:\n",
    "        # Try to find any file containing the date\n",
    "        matching_files = list(parquet_path.glob(f\"*{date}*.parquet\"))\n",
    "        if matching_files:\n",
    "            parquet_file = matching_files[0]\n",
    "            print(f\"✓ Found file: {parquet_file.name}\")\n",
    "        else:\n",
    "            available_files = list(parquet_path.glob(\"*.parquet\"))\n",
    "            raise FileNotFoundError(\n",
    "                f\"No parquet file found for date {date}. \"\n",
    "                f\"Available files: {[f.name for f in available_files[:5]]}...\"\n",
    "            )\n",
    "    \n",
    "    # Load the parquet file\n",
    "    print(f\"Loading: {parquet_file.name}...\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Also load ticker_prices and ticker_to_cusip for price information\n",
    "    ticker_prices_file = parquet_path / \"ticker_prices.parquet\"\n",
    "    ticker_to_cusip_file = parquet_path / \"ticker_to_cusip.parquet\"\n",
    "    \n",
    "    if 'price' not in df.columns and ticker_prices_file.exists() and ticker_to_cusip_file.exists():\n",
    "        print(\"Loading price data from ticker_prices.parquet...\")\n",
    "        ticker_prices = pd.read_parquet(ticker_prices_file)\n",
    "        ticker_to_cusip = pd.read_parquet(ticker_to_cusip_file)\n",
    "        \n",
    "        # Merge to get prices\n",
    "        df = df.merge(ticker_to_cusip, on='cusip', how='inner')\n",
    "        \n",
    "        # Convert period_start to proper date format for merging\n",
    "        if 'period_start' in ticker_prices.columns:\n",
    "            ticker_prices['period_start'] = pd.to_datetime(ticker_prices['period_start']).dt.date\n",
    "        if 'period_start' in df.columns:\n",
    "            df['period_start'] = pd.to_datetime(df['period_start']).dt.date\n",
    "        \n",
    "        df = df.merge(\n",
    "            ticker_prices[['ticker', 'period_start', 'price']], \n",
    "            on=['ticker', 'period_start'], \n",
    "            how='inner'\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df):,} rows\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f\"No data found for date: {date}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_holdings_data(date: str, data_source: str, parquet_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unified interface to load holdings data from either database or parquet.\n",
    "    \n",
    "    Args:\n",
    "        date: Date string in YYYY-MM-DD format\n",
    "        data_source: \"database\" or \"parquet\"\n",
    "        parquet_dir: Path to parquet files (required if data_source is \"parquet\")\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with holdings data\n",
    "    \"\"\"\n",
    "    # Validate date format\n",
    "    if not validate_date_format(date):\n",
    "        raise ValueError(f\"Invalid date format: {date}. Expected YYYY-MM-DD\")\n",
    "    \n",
    "    data_source = data_source.lower()\n",
    "    \n",
    "    if data_source == \"database\":\n",
    "        return load_holdings_from_database(date)\n",
    "    elif data_source == \"parquet\":\n",
    "        if parquet_dir is None:\n",
    "            raise ValueError(\"parquet_dir is required when data_source is 'parquet'\")\n",
    "        return load_holdings_from_parquet(date, parquet_dir)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data_source: {data_source}. Expected 'database' or 'parquet'\")\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Functions\n",
    "\n",
    "def filter_zero_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out rows with zero sshprnamt or price values.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    zero_sshprnamt = (df['sshprnamt'] == 0).sum()\n",
    "    zero_price = (df['price'] == 0).sum() if 'price' in df.columns else 0\n",
    "    \n",
    "    # Filter out zeros\n",
    "    if 'price' in df.columns:\n",
    "        df = df[(df['sshprnamt'] != 0) & (df['price'] != 0)].copy()\n",
    "    else:\n",
    "        df = df[df['sshprnamt'] != 0].copy()\n",
    "    \n",
    "    removed = original_count - len(df)\n",
    "    print(f\"  Filtered zeros: {original_count:,} → {len(df):,} rows ({removed:,} removed)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_duplicate_rows_detailed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge rows with duplicate (cik, cusip) pairs.\n",
    "    Sums sshprnamt for duplicates and recalculates weight.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with columns cik, cusip, sshprnamt, price\n",
    "        \n",
    "    Returns:\n",
    "        Merged DataFrame with weight column added\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Calculate weight before merging\n",
    "    df['weight'] = df['sshprnamt'] * df['price']\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = df[df.duplicated(subset=['cik', 'cusip'], keep=False)]\n",
    "    num_duplicate_pairs = len(duplicates.groupby(['cik', 'cusip']))\n",
    "    \n",
    "    # Aggregate\n",
    "    agg_dict = {\n",
    "        'sshprnamt': 'sum',\n",
    "        'price': 'first',\n",
    "        'nameofissuer': 'first',\n",
    "        'period_start': 'first'\n",
    "    }\n",
    "    \n",
    "    df_merged = df.groupby(['cik', 'cusip'], as_index=False).agg(agg_dict)\n",
    "    df_merged['weight'] = df_merged['sshprnamt'] * df_merged['price']\n",
    "    \n",
    "    reduction = original_count - len(df_merged)\n",
    "    print(f\"  Merged duplicates: {original_count:,} → {len(df_merged):,} rows\")\n",
    "    print(f\"    Duplicate pairs merged: {num_duplicate_pairs:,}\")\n",
    "    print(f\"    Reduction: {reduction:,} rows ({100*reduction/original_count:.1f}%)\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame ready for graph construction\n",
    "    \"\"\"\n",
    "    print(\"\\n[Preprocessing]\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Filter zeros\n",
    "    df = filter_zero_values(df)\n",
    "    \n",
    "    # Step 2: Merge duplicates\n",
    "    df = merge_duplicate_rows_detailed(df)\n",
    "    \n",
    "    print(f\"\\n✓ Preprocessing complete: {len(df):,} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph construction functions defined\n"
     ]
    }
   ],
   "source": [
    "# Graph Construction Functions\n",
    "\n",
    "def build_bipartite_graph_fastest(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build a bipartite graph from fund-stock holdings data.\n",
    "    Uses vectorized operations for optimal performance.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns cik, cusip, weight\n",
    "        \n",
    "    Returns:\n",
    "        G: NetworkX bipartite graph\n",
    "        funds: Array of unique fund (CIK) identifiers\n",
    "        stocks: Array of unique stock (CUSIP) identifiers\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    # Ensure weight column exists\n",
    "    if 'weight' not in df.columns:\n",
    "        df['weight'] = df['sshprnamt'] * df['price']\n",
    "    \n",
    "    # Create graph and add nodes\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    funds = df['cik'].unique()\n",
    "    G.add_nodes_from(funds, bipartite=0)\n",
    "    \n",
    "    stocks = df['cusip'].unique()\n",
    "    G.add_nodes_from(stocks, bipartite=1)\n",
    "    \n",
    "    # Create edge list (vectorized)\n",
    "    edges = list(zip(\n",
    "        df['cik'].values,\n",
    "        df['cusip'].values,\n",
    "        df['weight'].values\n",
    "    ))\n",
    "    \n",
    "    # Add edges to graph\n",
    "    G.add_weighted_edges_from(edges, weight='weight')\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"  Graph built: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges ({elapsed:.2f}s)\")\n",
    "    print(f\"    Funds (CIK): {len(funds):,}\")\n",
    "    print(f\"    Stocks (CUSIP): {len(stocks):,}\")\n",
    "    \n",
    "    return G, funds, stocks\n",
    "\n",
    "\n",
    "print(\"✓ Graph construction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph Splitting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph splitting functions defined\n"
     ]
    }
   ],
   "source": [
    "# Graph Splitting Functions\n",
    "\n",
    "def split_dataframe_edges(df: pd.DataFrame, train_ratio: float = 0.8, \n",
    "                          val_ratio: float = 0.1, test_ratio: float = 0.1, \n",
    "                          random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Split DataFrame edges into train, validation, and test sets.\n",
    "    Ensures all nodes appear in the training set.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with edges\n",
    "        train_ratio: Proportion for training (default 0.8)\n",
    "        val_ratio: Proportion for validation (default 0.1)\n",
    "        test_ratio: Proportion for testing (default 0.1)\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_df, val_df, test_df: Split DataFrames\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Shuffle DataFrame\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total = len(df_shuffled)\n",
    "    train_size = int(total * train_ratio)\n",
    "    val_size = int(total * val_ratio)\n",
    "    \n",
    "    # Split\n",
    "    train_df = df_shuffled.iloc[:train_size].copy()\n",
    "    val_df = df_shuffled.iloc[train_size:train_size + val_size].copy()\n",
    "    test_df = df_shuffled.iloc[train_size + val_size:].copy()\n",
    "    \n",
    "    # Verify all nodes are in training set\n",
    "    train_funds = set(train_df['cik'].unique())\n",
    "    train_stocks = set(train_df['cusip'].unique())\n",
    "    \n",
    "    val_funds = set(val_df['cik'].unique())\n",
    "    val_stocks = set(val_df['cusip'].unique())\n",
    "    test_funds = set(test_df['cik'].unique())\n",
    "    test_stocks = set(test_df['cusip'].unique())\n",
    "    \n",
    "    # Check for nodes only in val/test\n",
    "    val_only_funds = val_funds - train_funds\n",
    "    val_only_stocks = val_stocks - train_stocks\n",
    "    test_only_funds = test_funds - train_funds\n",
    "    test_only_stocks = test_stocks - train_stocks\n",
    "    \n",
    "    moved_edges = 0\n",
    "    if val_only_funds or val_only_stocks or test_only_funds or test_only_stocks:\n",
    "        # Move edges with new nodes to train\n",
    "        val_new = val_df[val_df['cik'].isin(val_only_funds) | val_df['cusip'].isin(val_only_stocks)]\n",
    "        test_new = test_df[test_df['cik'].isin(test_only_funds) | test_df['cusip'].isin(test_only_stocks)]\n",
    "        \n",
    "        moved_edges = len(val_new) + len(test_new)\n",
    "        train_df = pd.concat([train_df, val_new, test_new], ignore_index=True)\n",
    "        val_df = val_df[~val_df.index.isin(val_new.index)]\n",
    "        test_df = test_df[~test_df.index.isin(test_new.index)]\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"  Split edges: train={len(train_df):,} val={len(val_df):,} test={len(test_df):,} ({elapsed:.2f}s)\")\n",
    "    if moved_edges > 0:\n",
    "        print(f\"    Moved {moved_edges} edges to train for node coverage\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def build_graphs_from_splits(train_df: pd.DataFrame, val_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build separate graphs for training and validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training edges DataFrame\n",
    "        val_df: Validation edges DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        G_train: Graph with training edges only\n",
    "        G_train_val: Graph with training + validation edges\n",
    "        funds_train: Unique funds in training set\n",
    "        stocks_train: Unique stocks in training set\n",
    "    \"\"\"\n",
    "    print(\"\\n  Building training graph...\")\n",
    "    G_train, funds_train, stocks_train = build_bipartite_graph_fastest(train_df)\n",
    "    \n",
    "    print(\"\\n  Building train+validation graph...\")\n",
    "    train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "    G_train_val, _, _ = build_bipartite_graph_fastest(train_val_df)\n",
    "    \n",
    "    return G_train, G_train_val, funds_train, stocks_train\n",
    "\n",
    "\n",
    "print(\"✓ Graph splitting functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Node Mapping and PyTorch Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Node mapping functions defined\n"
     ]
    }
   ],
   "source": [
    "# Node Mapping and PyTorch Conversion Functions\n",
    "\n",
    "\n",
    "def create_node_mappings(funds, stocks):\n",
    "    \"\"\"\n",
    "    Create mappings from node IDs to indices for LightGCN.\n",
    "    \n",
    "    Args:\n",
    "        funds: Array of fund identifiers\n",
    "        stocks: Array of stock identifiers\n",
    "        \n",
    "    Returns:\n",
    "        node_to_idx: Combined mapping\n",
    "        fund_to_idx: Fund ID to index mapping\n",
    "        stock_to_idx: Stock ID to index mapping\n",
    "    \"\"\"\n",
    "    # Map funds to indices [0, len(funds))\n",
    "    fund_to_idx = {fund: idx for idx, fund in enumerate(funds)}\n",
    "    \n",
    "    # Map stocks to indices [len(funds), len(funds) + len(stocks))\n",
    "    stock_to_idx = {stock: idx + len(funds) for idx, stock in enumerate(stocks)}\n",
    "    \n",
    "    # Combined mapping\n",
    "    node_to_idx = {**fund_to_idx, **stock_to_idx}\n",
    "    \n",
    "    return node_to_idx, fund_to_idx, stock_to_idx\n",
    "\n",
    "\n",
    "def graph_to_edge_index_fastest(G, node_to_idx, device):\n",
    "    \"\"\"\n",
    "    Convert NetworkX graph to PyTorch Geometric edge index format.\n",
    "    Creates bidirectional edges for undirected graph.\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        node_to_idx: Node ID to index mapping\n",
    "        device: PyTorch device\n",
    "        \n",
    "    Returns:\n",
    "        edge_index: Tensor of shape [2, num_edges*2] (bidirectional)\n",
    "        edge_weights: Tensor of edge weights\n",
    "    \"\"\"\n",
    "    # Get all edges\n",
    "    edges_data = list(G.edges(data=True))\n",
    "    \n",
    "    # Extract u, v, weights\n",
    "    u_nodes = [node_to_idx[u] for u, _, _ in edges_data]\n",
    "    v_nodes = [node_to_idx[v] for _, v, _ in edges_data]\n",
    "    weights_list = [data.get('weight', 1.0) for _, _, data in edges_data]\n",
    "    \n",
    "    # Create bidirectional edges\n",
    "    src = u_nodes + v_nodes\n",
    "    dst = v_nodes + u_nodes\n",
    "    weights = weights_list + weights_list\n",
    "    \n",
    "    # Convert to tensors and move to device\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long).to(device)\n",
    "    edge_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    return edge_index, edge_weights\n",
    "\n",
    "\n",
    "print(\"✓ Node mapping functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model definition complete\n"
     ]
    }
   ],
   "source": [
    "# LightGCN Model Definition\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    LightGCN model for bipartite graph recommendation.\n",
    "    Simplified version without feature transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes: int, embedding_dim: int = 64, num_layers: int = 3):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.normal_(self.embedding.weight, std=0.1)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            GCNConv(embedding_dim, embedding_dim, improved=False, cached=False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        \"\"\"Forward pass through LightGCN layers.\"\"\"\n",
    "        x = self.embedding.weight\n",
    "        \n",
    "        # Aggregate embeddings from all layers\n",
    "        embeddings = [x]\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            embeddings.append(x)\n",
    "        \n",
    "        # Average all layer embeddings (LightGCN approach)\n",
    "        final_embedding = torch.stack(embeddings, dim=0).mean(dim=0)\n",
    "        \n",
    "        return final_embedding\n",
    "\n",
    "\n",
    "def initialize_model_fastest(num_nodes: int, embedding_dim: int, \n",
    "                             num_layers: int, device) -> LightGCN:\n",
    "    \"\"\"\n",
    "    Initialize LightGCN model and move to device.\n",
    "    \n",
    "    Args:\n",
    "        num_nodes: Total number of nodes in the graph\n",
    "        embedding_dim: Dimension of node embeddings\n",
    "        num_layers: Number of GCN layers\n",
    "        device: PyTorch device\n",
    "        \n",
    "    Returns:\n",
    "        Initialized LightGCN model on device\n",
    "    \"\"\"\n",
    "    model = LightGCN(\n",
    "        num_nodes=num_nodes, \n",
    "        embedding_dim=embedding_dim, \n",
    "        num_layers=num_layers\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"  Model initialized: {total_params:,} parameters on {device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"✓ Model definition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Training Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    \"\"\"Bayesian Personalized Ranking loss.\"\"\"\n",
    "    return -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()\n",
    "\n",
    "\n",
    "def create_positive_negative_pairs_fastest(edge_index, num_nodes, device, num_negatives=1):\n",
    "    \"\"\"\n",
    "    Create positive and negative edge pairs for training.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: Tensor of edge indices\n",
    "        num_nodes: Total number of nodes\n",
    "        device: PyTorch device\n",
    "        num_negatives: Number of negative samples per positive\n",
    "        \n",
    "    Returns:\n",
    "        pos_edges: Positive edge pairs\n",
    "        neg_edges: Negative edge pairs\n",
    "    \"\"\"\n",
    "    # Extract positive edges\n",
    "    pos_edges = edge_index.t().tolist()\n",
    "    pos_set = set((u, v) for u, v in pos_edges)\n",
    "    \n",
    "    # Sample negative edges\n",
    "    target_neg_count = len(pos_edges) * num_negatives\n",
    "    neg_edges = []\n",
    "    \n",
    "    max_attempts = target_neg_count * 10\n",
    "    attempts = 0\n",
    "    \n",
    "    while len(neg_edges) < target_neg_count and attempts < max_attempts:\n",
    "        # Batch sampling for efficiency\n",
    "        batch_size = min(10000, target_neg_count - len(neg_edges))\n",
    "        u_batch = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        v_batch = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        \n",
    "        for u, v in zip(u_batch, v_batch):\n",
    "            if u != v and (u, v) not in pos_set and (v, u) not in pos_set:\n",
    "                neg_edges.append([u, v])\n",
    "                if len(neg_edges) >= target_neg_count:\n",
    "                    break\n",
    "        attempts += batch_size\n",
    "    \n",
    "    # Convert to tensors\n",
    "    pos_edges = torch.tensor(pos_edges, dtype=torch.long).to(device)\n",
    "    neg_edges = torch.tensor(neg_edges[:target_neg_count], dtype=torch.long).to(device)\n",
    "    \n",
    "    return pos_edges, neg_edges\n",
    "\n",
    "\n",
    "def train_with_validation_fastest(model, train_edge_index, train_val_edge_index,\n",
    "                                  train_pos_edges, train_neg_edges,\n",
    "                                  val_pos_edges, val_neg_edges, device,\n",
    "                                  epochs=50, lr=0.001, patience=10):\n",
    "    \"\"\"\n",
    "    Train model with validation monitoring and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: LightGCN model\n",
    "        train_edge_index: Training graph edge index\n",
    "        train_val_edge_index: Training+validation graph edge index\n",
    "        train_pos_edges: Positive training edges\n",
    "        train_neg_edges: Negative training edges\n",
    "        val_pos_edges: Positive validation edges\n",
    "        val_neg_edges: Negative validation edges\n",
    "        device: PyTorch device\n",
    "        epochs: Maximum training epochs\n",
    "        lr: Learning rate\n",
    "        patience: Early stopping patience\n",
    "        \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "        model: Trained model (best checkpoint)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    print(f\"\\n[Training] epochs={epochs}, lr={lr}, patience={patience}\")\n",
    "    \n",
    "    with tqdm(total=epochs, desc=\"Training\", unit=\"epoch\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            embeddings = model(train_edge_index)\n",
    "            \n",
    "            pos_u = embeddings[train_pos_edges[:, 0]]\n",
    "            pos_v = embeddings[train_pos_edges[:, 1]]\n",
    "            pos_scores = (pos_u * pos_v).sum(dim=1)\n",
    "            \n",
    "            neg_u = embeddings[train_neg_edges[:, 0]]\n",
    "            neg_v = embeddings[train_neg_edges[:, 1]]\n",
    "            neg_scores = (neg_u * neg_v).sum(dim=1)\n",
    "            \n",
    "            train_loss = bpr_loss(pos_scores, neg_scores)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_embeddings = model(train_val_edge_index)\n",
    "                \n",
    "                val_pos_u = val_embeddings[val_pos_edges[:, 0]]\n",
    "                val_pos_v = val_embeddings[val_pos_edges[:, 1]]\n",
    "                val_pos_scores = (val_pos_u * val_pos_v).sum(dim=1)\n",
    "                \n",
    "                val_neg_u = val_embeddings[val_neg_edges[:, 0]]\n",
    "                val_neg_v = val_embeddings[val_neg_edges[:, 1]]\n",
    "                val_neg_scores = (val_neg_u * val_neg_v).sum(dim=1)\n",
    "                \n",
    "                val_loss = bpr_loss(val_pos_scores, val_neg_scores)\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss.item() < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'train': f'{train_loss.item():.4f}',\n",
    "                'val': f'{val_loss.item():.4f}',\n",
    "                'best': f'{best_val_loss:.4f}'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if no_improve >= patience:\n",
    "                print(f\"\\n  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\n✓ Training complete: {len(train_losses)} epochs, best_val_loss={best_val_loss:.4f} ({elapsed:.1f}s)\")\n",
    "    \n",
    "    return train_losses, val_losses, model\n",
    "\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Evaluation Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_test_set_fastest(model, train_val_edge_index, test_df, \n",
    "                              fund_to_idx, stock_to_idx, num_nodes, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with AUC-ROC, precision, recall, F1 scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare test edges\n",
    "    test_pos_pairs = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
    "            test_pos_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
    "    \n",
    "    test_pos_edges = torch.tensor(test_pos_pairs, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Sample negative edges\n",
    "    test_neg_edges = []\n",
    "    test_pos_set = set((u.item(), v.item()) for u, v in test_pos_edges)\n",
    "    \n",
    "    for _ in range(len(test_pos_edges)):\n",
    "        u = random.randint(0, num_nodes - 1)\n",
    "        v = random.randint(0, num_nodes - 1)\n",
    "        if (u, v) not in test_pos_set and u != v:\n",
    "            test_neg_edges.append([u, v])\n",
    "            if len(test_neg_edges) >= len(test_pos_edges):\n",
    "                break\n",
    "    \n",
    "    test_neg_edges = torch.tensor(test_neg_edges[:len(test_pos_edges)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(train_val_edge_index)\n",
    "        \n",
    "        pos_u = embeddings[test_pos_edges[:, 0]]\n",
    "        pos_v = embeddings[test_pos_edges[:, 1]]\n",
    "        pos_scores = (pos_u * pos_v).sum(dim=1).sigmoid()\n",
    "        \n",
    "        neg_u = embeddings[test_neg_edges[:, 0]]\n",
    "        neg_v = embeddings[test_neg_edges[:, 1]]\n",
    "        neg_scores = (neg_u * neg_v).sum(dim=1).sigmoid()\n",
    "        \n",
    "        all_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "        all_labels = torch.cat([\n",
    "            torch.ones(len(pos_scores)),\n",
    "            torch.zeros(len(neg_scores))\n",
    "        ]).numpy()\n",
    "        \n",
    "        auc = roc_auc_score(all_labels, all_scores)\n",
    "        pred_labels = (all_scores > 0.5).astype(int)\n",
    "        precision = precision_score(all_labels, pred_labels)\n",
    "        recall = recall_score(all_labels, pred_labels)\n",
    "        f1 = f1_score(all_labels, pred_labels)\n",
    "    \n",
    "    print(f\"\\n[Test Results]\")\n",
    "    print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'scores': all_scores,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_top_k_recommendations_fastest(model, train_val_edge_index, test_df,\n",
    "                                           fund_to_idx, stock_to_idx, device,\n",
    "                                           k_list=[5, 10, 20, 50]):\n",
    "    \"\"\"\n",
    "    Evaluate model using Top-K recommendation metrics (Hit Rate, NDCG).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Group test edges by fund (cik)\n",
    "    fund_to_stocks = {}\n",
    "    for _, row in test_df.iterrows():\n",
    "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
    "            fund_idx = fund_to_idx[row['cik']]\n",
    "            stock_idx = stock_to_idx[row['cusip']]\n",
    "            if fund_idx not in fund_to_stocks:\n",
    "                fund_to_stocks[fund_idx] = set()\n",
    "            fund_to_stocks[fund_idx].add(stock_idx)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(train_val_edge_index)\n",
    "    \n",
    "    results = {k: {'hit_rate': 0, 'ndcg': 0} for k in k_list}\n",
    "    \n",
    "    # For each fund, recommend top K stocks\n",
    "    for fund_idx, true_stocks in fund_to_stocks.items():\n",
    "        fund_emb = embeddings[fund_idx]\n",
    "        \n",
    "        # Calculate scores for all stocks\n",
    "        stock_indices = list(stock_to_idx.values())\n",
    "        stock_embs = embeddings[stock_indices]\n",
    "        scores = (fund_emb * stock_embs).sum(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Get top K recommendations\n",
    "        top_k_indices = np.argsort(scores)[::-1]  # Descending order\n",
    "        \n",
    "        for k in k_list:\n",
    "            top_k_stocks = set([stock_indices[i] for i in top_k_indices[:k]])\n",
    "            \n",
    "            # Hit Rate\n",
    "            hits = len(top_k_stocks & true_stocks)\n",
    "            results[k]['hit_rate'] += 1 if hits > 0 else 0\n",
    "            \n",
    "            # NDCG\n",
    "            if hits > 0:\n",
    "                dcg = 0\n",
    "                for i, stock_idx in enumerate([stock_indices[j] for j in top_k_indices[:k]]):\n",
    "                    if stock_idx in true_stocks:\n",
    "                        dcg += 1 / np.log2(i + 2)\n",
    "                \n",
    "                idcg = sum(1 / np.log2(i + 2) for i in range(min(len(true_stocks), k)))\n",
    "                results[k]['ndcg'] += dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    num_funds = len(fund_to_stocks)\n",
    "    \n",
    "    print(f\"\\n[Top-K Results]\")\n",
    "    for k in k_list:\n",
    "        hit_rate = results[k]['hit_rate'] / num_funds\n",
    "        ndcg = results[k]['ndcg'] / num_funds\n",
    "        results[k]['hit_rate'] = hit_rate\n",
    "        results[k]['ndcg'] = ndcg\n",
    "        print(f\"  K={k:2d}: Hit Rate={hit_rate:.4f}, NDCG={ndcg:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_optimal_threshold_fastest(model, train_val_edge_index, test_df,\n",
    "                                   fund_to_idx, stock_to_idx, num_nodes, device,\n",
    "                                   plot=True):\n",
    "    \"\"\"\n",
    "    Find optimal threshold using Precision-Recall curve.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare test edges\n",
    "    test_pos_pairs = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
    "            test_pos_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
    "    \n",
    "    test_pos_edges = torch.tensor(test_pos_pairs, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Sample negative edges\n",
    "    test_neg_edges = []\n",
    "    test_pos_set = set((u.item(), v.item()) for u, v in test_pos_edges)\n",
    "    \n",
    "    for _ in range(len(test_pos_edges)):\n",
    "        u = random.randint(0, num_nodes - 1)\n",
    "        v = random.randint(0, num_nodes - 1)\n",
    "        if u != v and (u, v) not in test_pos_set:\n",
    "            test_neg_edges.append([u, v])\n",
    "            if len(test_neg_edges) >= len(test_pos_edges):\n",
    "                break\n",
    "    \n",
    "    test_neg_edges = torch.tensor(test_neg_edges[:len(test_pos_edges)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(train_val_edge_index)\n",
    "        \n",
    "        pos_u = embeddings[test_pos_edges[:, 0]]\n",
    "        pos_v = embeddings[test_pos_edges[:, 1]]\n",
    "        pos_scores = (pos_u * pos_v).sum(dim=1).sigmoid().cpu().numpy()\n",
    "        \n",
    "        neg_u = embeddings[test_neg_edges[:, 0]]\n",
    "        neg_v = embeddings[test_neg_edges[:, 1]]\n",
    "        neg_scores = (neg_u * neg_v).sum(dim=1).sigmoid().cpu().numpy()\n",
    "        \n",
    "        all_scores = np.concatenate([pos_scores, neg_scores])\n",
    "        all_labels = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
    "    \n",
    "    # Calculate Precision-Recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_scores)\n",
    "    avg_precision = average_precision_score(all_labels, all_scores)\n",
    "    \n",
    "    # Find optimal threshold (maximize F1)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "    \n",
    "    # Plot\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(recall, precision, label=f'PR Curve (AP={avg_precision:.4f})')\n",
    "        plt.axvline(x=recall[optimal_idx], color='r', linestyle='--', \n",
    "                    label=f'Optimal Threshold={optimal_threshold:.4f}')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\n[Optimal Threshold Analysis]\")\n",
    "    print(f\"  Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "    \n",
    "    return optimal_threshold, precision, recall, thresholds\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Main Pipeline Function\n",
    "# ==============================================================================\n",
    "\n",
    "def run_pipeline(\n",
    "    target_date: str,\n",
    "    data_source: str = \"parquet\",\n",
    "    parquet_dir: str = None,\n",
    "    embedding_dim: int = 64,\n",
    "    num_layers: int = 3,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 0.001,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    patience: int = 10,\n",
    "    random_seed: int = 42,\n",
    "    top_k_values: list = [5, 10, 20, 50],\n",
    "    device = None,\n",
    "    save_model: bool = False,\n",
    "    model_save_dir: str = \"./models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete LightGCN pipeline for a specific date.\n",
    "    \n",
    "    Args:\n",
    "        target_date: Date in YYYY-MM-DD format\n",
    "        data_source: \"database\" or \"parquet\"\n",
    "        parquet_dir: Path to parquet files (required if data_source is \"parquet\")\n",
    "        embedding_dim: Dimension of node embeddings\n",
    "        num_layers: Number of GCN layers\n",
    "        epochs: Maximum training epochs\n",
    "        lr: Learning rate\n",
    "        train_ratio: Proportion for training\n",
    "        val_ratio: Proportion for validation\n",
    "        test_ratio: Proportion for testing\n",
    "        patience: Early stopping patience\n",
    "        random_seed: Random seed for reproducibility\n",
    "        top_k_values: List of K values for Top-K evaluation\n",
    "        device: PyTorch device (auto-detected if None)\n",
    "        save_model: Whether to save the trained model\n",
    "        model_save_dir: Directory to save model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with model, embeddings, metrics, and other artifacts\n",
    "    \"\"\"\n",
    "    import time\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"LightGCN Pipeline - Target Date: {target_date}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    # Set random seeds\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "    \n",
    "    # ==================== Step 1: Load Data ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 1: Loading Data\")\n",
    "    print(\"=\"*80)\n",
    "    df = load_holdings_data(target_date, data_source, parquet_dir)\n",
    "    \n",
    "    # ==================== Step 2: Preprocess ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 2: Preprocessing\")\n",
    "    print(\"=\"*80)\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # ==================== Step 3: Split Data ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 3: Splitting Data\")\n",
    "    print(\"=\"*80)\n",
    "    train_df, val_df, test_df = split_dataframe_edges(\n",
    "        df, train_ratio, val_ratio, test_ratio, random_seed\n",
    "    )\n",
    "    \n",
    "    # ==================== Step 4: Build Graphs ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 4: Building Graphs\")\n",
    "    print(\"=\"*80)\n",
    "    G_train, G_train_val, funds_train, stocks_train = build_graphs_from_splits(train_df, val_df)\n",
    "    \n",
    "    # ==================== Step 5: Create Node Mappings ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 5: Creating Node Mappings\")\n",
    "    print(\"=\"*80)\n",
    "    node_to_idx, fund_to_idx, stock_to_idx = create_node_mappings(funds_train, stocks_train)\n",
    "    num_nodes = len(node_to_idx)\n",
    "    print(f\"  Total nodes: {num_nodes:,} (funds: {len(fund_to_idx):,}, stocks: {len(stock_to_idx):,})\")\n",
    "    \n",
    "    # ==================== Step 6: Convert to PyTorch ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 6: Converting to PyTorch Geometric Format\")\n",
    "    print(\"=\"*80)\n",
    "    train_edge_index, _ = graph_to_edge_index_fastest(G_train, node_to_idx, device)\n",
    "    train_val_edge_index, _ = graph_to_edge_index_fastest(G_train_val, node_to_idx, device)\n",
    "    print(f\"  Train edges: {train_edge_index.shape[1]:,}\")\n",
    "    print(f\"  Train+Val edges: {train_val_edge_index.shape[1]:,}\")\n",
    "    \n",
    "    # ==================== Step 7: Create Training Pairs ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 7: Creating Training/Validation Pairs\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Training pairs\n",
    "    print(\"  Creating training pairs...\")\n",
    "    train_pos_edges, train_neg_edges = create_positive_negative_pairs_fastest(\n",
    "        train_edge_index, num_nodes, device\n",
    "    )\n",
    "    print(f\"    Train: {len(train_pos_edges):,} pos, {len(train_neg_edges):,} neg\")\n",
    "    \n",
    "    # Validation pairs\n",
    "    print(\"  Creating validation pairs...\")\n",
    "    val_edge_pairs = []\n",
    "    for _, row in val_df.iterrows():\n",
    "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
    "            val_edge_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
    "    \n",
    "    val_pos_edges = torch.tensor(val_edge_pairs, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Sample validation negatives\n",
    "    val_neg_edges = []\n",
    "    train_val_set = set()\n",
    "    for i in range(train_val_edge_index.shape[1]):\n",
    "        u = train_val_edge_index[0, i].item()\n",
    "        v = train_val_edge_index[1, i].item()\n",
    "        train_val_set.add((u, v))\n",
    "        train_val_set.add((v, u))\n",
    "    \n",
    "    while len(val_neg_edges) < len(val_pos_edges):\n",
    "        u = random.randint(0, num_nodes - 1)\n",
    "        v = random.randint(0, num_nodes - 1)\n",
    "        if u != v and (u, v) not in train_val_set:\n",
    "            val_neg_edges.append([u, v])\n",
    "    \n",
    "    val_neg_edges = torch.tensor(val_neg_edges[:len(val_pos_edges)], dtype=torch.long).to(device)\n",
    "    print(f\"    Val: {len(val_pos_edges):,} pos, {len(val_neg_edges):,} neg\")\n",
    "    \n",
    "    # ==================== Step 8: Initialize Model ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 8: Initializing Model\")\n",
    "    print(\"=\"*80)\n",
    "    model = initialize_model_fastest(num_nodes, embedding_dim, num_layers, device)\n",
    "    \n",
    "    # ==================== Step 9: Train Model ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 9: Training\")\n",
    "    print(\"=\"*80)\n",
    "    train_losses, val_losses, model = train_with_validation_fastest(\n",
    "        model, train_edge_index, train_val_edge_index,\n",
    "        train_pos_edges, train_neg_edges,\n",
    "        val_pos_edges, val_neg_edges, device,\n",
    "        epochs=epochs, lr=lr, patience=patience\n",
    "    )\n",
    "    \n",
    "    # ==================== Step 10: Evaluate ====================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Step 10: Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_results = evaluate_test_set_fastest(\n",
    "        model, train_val_edge_index, test_df,\n",
    "        fund_to_idx, stock_to_idx, num_nodes, device\n",
    "    )\n",
    "    \n",
    "    top_k_results = evaluate_top_k_recommendations_fastest(\n",
    "        model, train_val_edge_index, test_df,\n",
    "        fund_to_idx, stock_to_idx, device, top_k_values\n",
    "    )\n",
    "    \n",
    "    optimal_threshold, _, _, _ = find_optimal_threshold_fastest(\n",
    "        model, train_val_edge_index, test_df,\n",
    "        fund_to_idx, stock_to_idx, num_nodes, device\n",
    "    )\n",
    "    \n",
    "    # ==================== Step 11: Save Model (Optional) ====================\n",
    "    if save_model:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Step 11: Saving Model\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        save_path = Path(model_save_dir)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        model_file = save_path / f\"lightgcn_{target_date}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'num_nodes': num_nodes,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'fund_to_idx': fund_to_idx,\n",
    "            'stock_to_idx': stock_to_idx,\n",
    "            'target_date': target_date,\n",
    "            'test_results': test_results,\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        }, model_file)\n",
    "        print(f\"  Model saved to: {model_file}\")\n",
    "    \n",
    "    # ==================== Summary ====================\n",
    "    pipeline_elapsed = time.time() - pipeline_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Pipeline Complete\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Target Date: {target_date}\")\n",
    "    print(f\"  Data Source: {data_source}\")\n",
    "    print(f\"  Total Time: {pipeline_elapsed:.1f}s ({pipeline_elapsed/60:.1f}m)\")\n",
    "    print(f\"  Final AUC-ROC: {test_results['auc']:.4f}\")\n",
    "    print(f\"  Final F1: {test_results['f1']:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Return all results\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_results': test_results,\n",
    "        'top_k_results': top_k_results,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'fund_to_idx': fund_to_idx,\n",
    "        'stock_to_idx': stock_to_idx,\n",
    "        'node_to_idx': node_to_idx,\n",
    "        'num_nodes': num_nodes,\n",
    "        'target_date': target_date,\n",
    "        'train_edge_index': train_edge_index,\n",
    "        'train_val_edge_index': train_val_edge_index,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Main pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Execute Pipeline\n",
    "\n",
    "Run the complete pipeline with the configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LightGCN Pipeline - Target Date: 2024-01-01\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Step 1: Loading Data\n",
      "================================================================================\n",
      "\n",
      "[Parquet] Loading data for date: 2024-01-01\n",
      "============================================================\n",
      "Searching in: c:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\Notebooks\\LightGCN\\..\\..\\Data\\parquet_files\n",
      "✓ Found file: holdings_filtered_new_period_start_2024-01-01.parquet\n",
      "Loading: holdings_filtered_new_period_start_2024-01-01.parquet...\n"
     ]
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Execute the Pipeline\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Run the pipeline with configuration from the first cell\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_SOURCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPARQUET_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEMBEDDING_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVAL_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEST_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEARLY_STOPPING_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRANDOM_SEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K_VALUES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_SAVE_DIR\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(target_date, data_source, parquet_dir, embedding_dim, num_layers, epochs, lr, train_ratio, val_ratio, test_ratio, patience, random_seed, top_k_values, device, save_model, model_save_dir)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Loading Data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m df = \u001b[43mload_holdings_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparquet_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# ==================== Step 2: Preprocess ====================\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mload_holdings_data\u001b[39m\u001b[34m(date, data_source, parquet_dir)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parquet_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mparquet_dir is required when data_source is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_holdings_from_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparquet_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown data_source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdatabase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mload_holdings_from_parquet\u001b[39m\u001b[34m(date, parquet_dir)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Load the parquet file\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Also load ticker_prices and ticker_to_cusip for price information\u001b[39;00m\n\u001b[32m    129\u001b[39m ticker_prices_file = parquet_path / \u001b[33m\"\u001b[39m\u001b[33mticker_prices.parquet\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\pyarrow\\types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "# Execute the Pipeline\n",
    "\n",
    "# Run the pipeline with configuration from the first cell\n",
    "results = run_pipeline(\n",
    "    target_date=TARGET_DATE,\n",
    "    data_source=DATA_SOURCE,\n",
    "    parquet_dir=PARQUET_DIR,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    patience=EARLY_STOPPING_PATIENCE,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    top_k_values=TOP_K_VALUES,\n",
    "    device=DEVICE,\n",
    "    save_model=SAVE_MODEL,\n",
    "    model_save_dir=MODEL_SAVE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Access Results\n",
    "# ==============================================================================\n",
    "\n",
    "# Model\n",
    "model = results['model']\n",
    "\n",
    "# Metrics\n",
    "print(\"Test Set Results:\")\n",
    "print(f\"  AUC-ROC: {results['test_results']['auc']:.4f}\")\n",
    "print(f\"  Precision: {results['test_results']['precision']:.4f}\")\n",
    "print(f\"  Recall: {results['test_results']['recall']:.4f}\")\n",
    "print(f\"  F1 Score: {results['test_results']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nTop-K Results:\")\n",
    "for k, metrics in results['top_k_results'].items():\n",
    "    print(f\"  K={k}: Hit Rate={metrics['hit_rate']:.4f}, NDCG={metrics['ndcg']:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Threshold: {results['optimal_threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Plot Training Curves\n",
    "# ==============================================================================\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results['train_losses'], label='Train Loss')\n",
    "plt.plot(results['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BPR Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar([f'K={k}' for k in TOP_K_VALUES], \n",
    "        [results['top_k_results'][k]['hit_rate'] for k in TOP_K_VALUES],\n",
    "        alpha=0.7, label='Hit Rate')\n",
    "plt.bar([f'K={k}' for k in TOP_K_VALUES], \n",
    "        [results['top_k_results'][k]['ndcg'] for k in TOP_K_VALUES],\n",
    "        alpha=0.7, label='NDCG')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Top-K Recommendation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

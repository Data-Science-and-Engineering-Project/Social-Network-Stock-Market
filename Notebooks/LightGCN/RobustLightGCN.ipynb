{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Robust LightGCN Pipeline\n",
        "\n",
        "A modular, date-driven LightGCN implementation for fund-stock relationship prediction.\n",
        "\n",
        "## Features:\n",
        "- **Date-driven**: Change `TARGET_DATE` to run the pipeline for any date\n",
        "- **Dual data sources**: Load from database or parquet files\n",
        "- **Modular design**: All functions are reusable and well-organized\n",
        "- **Complete pipeline**: Single `run_pipeline()` call runs everything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "Modify these parameters to customize the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURATION - Modify these parameters as needed\n",
        "# ==============================================================================\n",
        "\n",
        "# Data Source Configuration\n",
        "DATA_SOURCE = \"parquet\"  # Options: \"database\" or \"parquet\"\n",
        "PARQUET_DIR = \"../../Data/parquet_files\"  # Relative path to parquet files\n",
        "TARGET_DATE = \"2024-01-01\"  # Date in YYYY-MM-DD format\n",
        "ENV_PATH = r\"C:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.env\"\n",
        "# Model Hyperparameters\n",
        "EMBEDDING_DIM = 128\n",
        "NUM_LAYERS = 3\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Train/Val/Test Split Ratios\n",
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "# Training Configuration\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Evaluation Configuration\n",
        "TOP_K_VALUES = [5, 10, 20, 50]\n",
        "\n",
        "# Model Saving\n",
        "SAVE_MODEL = True\n",
        "MODEL_SAVE_DIR = \"./models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports complete\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "# Graph libraries\n",
        "import networkx as nx\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, precision_score, recall_score, f1_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Imports complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded environment from: C:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\\.env\n",
            "✓ Project root: c:\\Users\\potda\\Daniel\\BGU\\Year_D\\Final_Project\\Social-Network-Stock-Market\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Try to load .env from multiple locations\n",
        "env_paths = [\n",
        "    Path(ENV_PATH),\n",
        "    Path.cwd() / '.env',\n",
        "    Path.cwd().parent / '.env',\n",
        "    Path.cwd().parent.parent / '.env',\n",
        "]\n",
        "\n",
        "for env_path in env_paths:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"✓ Loaded environment from: {env_path}\")\n",
        "        break\n",
        "else:\n",
        "    print(\"⚠ No .env file found - database connection may fail\")\n",
        "\n",
        "# Calculate project root path\n",
        "notebook_path = Path.cwd()\n",
        "project_root = notebook_path.parent.parent\n",
        "\n",
        "# Add project root to Python path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"✓ Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Using GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
            "PyTorch Version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# GPU/Device Setup\n",
        "# ==============================================================================\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get available device (CUDA if available, else CPU).\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"✓ Using CPU\")\n",
        "    return device\n",
        "\n",
        "# Set device\n",
        "DEVICE = get_device()\n",
        "print(f\"PyTorch Version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading Functions\n",
        "\n",
        "def validate_date_format(date_str: str) -> bool:\n",
        "    \"\"\"Validate date format is YYYY-MM-DD.\"\"\"\n",
        "    try:\n",
        "        datetime.strptime(date_str, '%Y-%m-%d')\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def load_holdings_from_database(date: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load holdings data from PostgreSQL database for a specific date.\n",
        "    \n",
        "    Args:\n",
        "        date: Date string in YYYY-MM-DD format\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with columns: nameofissuer, cusip, sshprnamt, cik, period_start, price\n",
        "    \"\"\"\n",
        "    from ETL.data_handlers.db_data_handler.postgres_handler import PostgresHandler\n",
        "    \n",
        "    # Connect to database\n",
        "    handler = PostgresHandler()\n",
        "    \n",
        "    if not handler.connect():\n",
        "        raise ConnectionError(\"Failed to connect to database\")\n",
        "    \n",
        "    # Query with parameterized date\n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        hf.nameofissuer,\n",
        "        hf.cusip,\n",
        "        hf.sshprnamt,\n",
        "        hf.cik,\n",
        "        hf.period_start,\n",
        "        tp.price\n",
        "    FROM holdings_filtered_new hf\n",
        "    INNER JOIN ticker_to_cusip ttc ON hf.cusip = ttc.cusip\n",
        "    INNER JOIN ticker_prices tp ON ttc.ticker = tp.ticker \n",
        "        AND hf.period_start = CAST(tp.period_start AS DATE)\n",
        "    WHERE hf.period_start = '{date}'\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_sql_query(query, handler.connection)\n",
        "    \n",
        "    # Close connection\n",
        "    handler.disconnect()\n",
        "    \n",
        "    if len(df) == 0:\n",
        "        raise ValueError(f\"No data found for date: {date}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def load_holdings_from_parquet(date: str, parquet_dir: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load holdings data from parquet files for a specific date.\n",
        "    \n",
        "    Supports file naming conventions:\n",
        "        - holdings_filtered_new_period_start_{YYYY-MM-DD}.parquet\n",
        "        - holdings_{YYYY-MM-DD}.parquet\n",
        "        - {YYYY-MM-DD}.parquet\n",
        "    \n",
        "    Args:\n",
        "        date: Date string in YYYY-MM-DD format\n",
        "        parquet_dir: Path to directory containing parquet files\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with holdings data\n",
        "    \"\"\"\n",
        "    # Resolve parquet directory path\n",
        "    parquet_path = Path(parquet_dir)\n",
        "    if not parquet_path.is_absolute():\n",
        "        parquet_path = Path.cwd() / parquet_dir\n",
        "    \n",
        "    if not parquet_path.exists():\n",
        "        raise FileNotFoundError(f\"Parquet directory not found: {parquet_path}\")\n",
        "    \n",
        "    # Try different naming conventions\n",
        "    naming_patterns = [\n",
        "        f\"holdings_filtered_new_period_start_{date}.parquet\",\n",
        "        f\"holdings_{date}.parquet\",\n",
        "        f\"{date}.parquet\",\n",
        "        f\"holdings_filtered_new_period_start_{date.replace('-', '_')}.parquet\",\n",
        "    ]\n",
        "    \n",
        "    parquet_file = None\n",
        "    for pattern in naming_patterns:\n",
        "        potential_file = parquet_path / pattern\n",
        "        if potential_file.exists():\n",
        "            parquet_file = potential_file\n",
        "            break\n",
        "    \n",
        "    if parquet_file is None:\n",
        "        # Try to find any file containing the date\n",
        "        matching_files = list(parquet_path.glob(f\"*{date}*.parquet\"))\n",
        "        if matching_files:\n",
        "            parquet_file = matching_files[0]\n",
        "        else:\n",
        "            available_files = list(parquet_path.glob(\"*.parquet\"))\n",
        "            raise FileNotFoundError(\n",
        "                f\"No parquet file found for date {date}. \"\n",
        "                f\"Available files: {[f.name for f in available_files[:5]]}...\"\n",
        "            )\n",
        "    \n",
        "    # Load the parquet file\n",
        "    with tqdm(total=1, desc=f\"Loading {parquet_file.name}\") as pbar:\n",
        "        df = pd.read_parquet(parquet_file)\n",
        "        pbar.update(1)\n",
        "    \n",
        "    # Also load ticker_prices and ticker_to_cusip for price information\n",
        "    ticker_prices_file = parquet_path / \"ticker_prices.parquet\"\n",
        "    ticker_to_cusip_file = parquet_path / \"ticker_to_cusip.parquet\"\n",
        "    \n",
        "    if 'price' not in df.columns and ticker_prices_file.exists() and ticker_to_cusip_file.exists():\n",
        "        with tqdm(total=2, desc=\"Loading price data\") as pbar:\n",
        "            ticker_prices = pd.read_parquet(ticker_prices_file)\n",
        "            pbar.update(1)\n",
        "            ticker_to_cusip = pd.read_parquet(ticker_to_cusip_file)\n",
        "            pbar.update(1)\n",
        "        \n",
        "        # Merge to get prices\n",
        "        df = df.merge(ticker_to_cusip, on='cusip', how='inner')\n",
        "        \n",
        "        # Convert period_start to proper date format for merging\n",
        "        if 'period_start' in ticker_prices.columns:\n",
        "            ticker_prices['period_start'] = pd.to_datetime(ticker_prices['period_start']).dt.date\n",
        "        if 'period_start' in df.columns:\n",
        "            df['period_start'] = pd.to_datetime(df['period_start']).dt.date\n",
        "        \n",
        "        df = df.merge(\n",
        "            ticker_prices[['ticker', 'period_start', 'price']], \n",
        "            on=['ticker', 'period_start'], \n",
        "            how='inner'\n",
        "        )\n",
        "    \n",
        "    if len(df) == 0:\n",
        "        raise ValueError(f\"No data found for date: {date}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def load_holdings_data(date: str, data_source: str, parquet_dir: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Unified interface to load holdings data from either database or parquet.\n",
        "    \n",
        "    Args:\n",
        "        date: Date string in YYYY-MM-DD format\n",
        "        data_source: \"database\" or \"parquet\"\n",
        "        parquet_dir: Path to parquet files (required if data_source is \"parquet\")\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with holdings data\n",
        "    \"\"\"\n",
        "    # Validate date format\n",
        "    if not validate_date_format(date):\n",
        "        raise ValueError(f\"Invalid date format: {date}. Expected YYYY-MM-DD\")\n",
        "    \n",
        "    data_source = data_source.lower()\n",
        "    \n",
        "    if data_source == \"database\":\n",
        "        return load_holdings_from_database(date)\n",
        "    elif data_source == \"parquet\":\n",
        "        if parquet_dir is None:\n",
        "            raise ValueError(\"parquet_dir is required when data_source is 'parquet'\")\n",
        "        return load_holdings_from_parquet(date, parquet_dir)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown data_source: {data_source}. Expected 'database' or 'parquet'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing Functions\n",
        "\n",
        "def filter_zero_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filter out rows with zero sshprnamt or price values.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        Filtered DataFrame\n",
        "    \"\"\"\n",
        "    original_count = len(df)\n",
        "    \n",
        "    # Filter out zeros\n",
        "    if 'price' in df.columns:\n",
        "        df = df[(df['sshprnamt'] != 0) & (df['price'] != 0)].copy()\n",
        "    else:\n",
        "        df = df[df['sshprnamt'] != 0].copy()\n",
        "    \n",
        "    removed = original_count - len(df)\n",
        "    if removed > 0:\n",
        "        print(f\"Filtered zeros: {removed:,} rows removed ({original_count:,} → {len(df):,})\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def merge_duplicate_rows_detailed(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merge rows with duplicate (cik, cusip) pairs.\n",
        "    Sums sshprnamt for duplicates and recalculates weight.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame with columns cik, cusip, sshprnamt, price\n",
        "        \n",
        "    Returns:\n",
        "        Merged DataFrame with weight column added\n",
        "    \"\"\"\n",
        "    original_count = len(df)\n",
        "    \n",
        "    # Calculate weight before merging\n",
        "    df['weight'] = df['sshprnamt'] * df['price']\n",
        "    \n",
        "    # Aggregate\n",
        "    agg_dict = {\n",
        "        'sshprnamt': 'sum',\n",
        "        'price': 'first',\n",
        "        'nameofissuer': 'first',\n",
        "        'period_start': 'first'\n",
        "    }\n",
        "    \n",
        "    df_merged = df.groupby(['cik', 'cusip'], as_index=False).agg(agg_dict)\n",
        "    df_merged['weight'] = df_merged['sshprnamt'] * df_merged['price']\n",
        "    \n",
        "    reduction = original_count - len(df_merged)\n",
        "    if reduction > 0:\n",
        "        print(f\"Merged duplicates: {reduction:,} rows merged ({original_count:,} → {len(df_merged):,})\")\n",
        "    \n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline.\n",
        "    \n",
        "    Args:\n",
        "        df: Raw input DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        Preprocessed DataFrame ready for graph construction\n",
        "    \"\"\"\n",
        "    # Step 1: Filter zeros\n",
        "    df = filter_zero_values(df)\n",
        "    \n",
        "    # Step 2: Merge duplicates\n",
        "    df = merge_duplicate_rows_detailed(df)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Graph Construction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph Construction Functions\n",
        "\n",
        "def build_bipartite_graph_fastest(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Build a bipartite graph from fund-stock holdings data.\n",
        "    Uses vectorized operations for optimal performance.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with columns cik, cusip, weight\n",
        "        \n",
        "    Returns:\n",
        "        G: NetworkX bipartite graph\n",
        "        funds: Array of unique fund (CIK) identifiers\n",
        "        stocks: Array of unique stock (CUSIP) identifiers\n",
        "    \"\"\"\n",
        "    # Ensure weight column exists\n",
        "    if 'weight' not in df.columns:\n",
        "        df['weight'] = df['sshprnamt'] * df['price']\n",
        "    \n",
        "    # Create graph and add nodes\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    funds = df['cik'].unique()\n",
        "    G.add_nodes_from(funds, bipartite=0)\n",
        "    \n",
        "    stocks = df['cusip'].unique()\n",
        "    G.add_nodes_from(stocks, bipartite=1)\n",
        "    \n",
        "    # Create edge list (vectorized)\n",
        "    with tqdm(total=1, desc=\"Building graph\") as pbar:\n",
        "        edges = list(zip(\n",
        "            df['cik'].values,\n",
        "            df['cusip'].values,\n",
        "            df['weight'].values\n",
        "        ))\n",
        "        \n",
        "        # Add edges to graph\n",
        "        G.add_weighted_edges_from(edges, weight='weight')\n",
        "        pbar.update(1)\n",
        "    \n",
        "    return G, funds, stocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Graph Splitting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph Splitting Functions\n",
        "\n",
        "def split_dataframe_edges(df: pd.DataFrame, train_ratio: float = 0.8, \n",
        "                          val_ratio: float = 0.1, test_ratio: float = 0.1, \n",
        "                          random_seed: int = 42):\n",
        "    \"\"\"\n",
        "    Split DataFrame edges into train, validation, and test sets.\n",
        "    Ensures all nodes appear in the training set.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with edges\n",
        "        train_ratio: Proportion for training (default 0.8)\n",
        "        val_ratio: Proportion for validation (default 0.1)\n",
        "        test_ratio: Proportion for testing (default 0.1)\n",
        "        random_seed: Random seed for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        train_df, val_df, test_df: Split DataFrames\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    \n",
        "    # Shuffle DataFrame\n",
        "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
        "    \n",
        "    # Calculate split sizes\n",
        "    total = len(df_shuffled)\n",
        "    train_size = int(total * train_ratio)\n",
        "    val_size = int(total * val_ratio)\n",
        "    \n",
        "    # Split\n",
        "    train_df = df_shuffled.iloc[:train_size].copy()\n",
        "    val_df = df_shuffled.iloc[train_size:train_size + val_size].copy()\n",
        "    test_df = df_shuffled.iloc[train_size + val_size:].copy()\n",
        "    \n",
        "    # Verify all nodes are in training set\n",
        "    train_funds = set(train_df['cik'].unique())\n",
        "    train_stocks = set(train_df['cusip'].unique())\n",
        "    \n",
        "    val_funds = set(val_df['cik'].unique())\n",
        "    val_stocks = set(val_df['cusip'].unique())\n",
        "    test_funds = set(test_df['cik'].unique())\n",
        "    test_stocks = set(test_df['cusip'].unique())\n",
        "    \n",
        "    # Check for nodes only in val/test\n",
        "    val_only_funds = val_funds - train_funds\n",
        "    val_only_stocks = val_stocks - train_stocks\n",
        "    test_only_funds = test_funds - train_funds\n",
        "    test_only_stocks = test_stocks - train_stocks\n",
        "    \n",
        "    if val_only_funds or val_only_stocks or test_only_funds or test_only_stocks:\n",
        "        # Move edges with new nodes to train\n",
        "        val_new = val_df[val_df['cik'].isin(val_only_funds) | val_df['cusip'].isin(val_only_stocks)]\n",
        "        test_new = test_df[test_df['cik'].isin(test_only_funds) | test_df['cusip'].isin(test_only_stocks)]\n",
        "        \n",
        "        train_df = pd.concat([train_df, val_new, test_new], ignore_index=True)\n",
        "        val_df = val_df[~val_df.index.isin(val_new.index)]\n",
        "        test_df = test_df[~test_df.index.isin(test_new.index)]\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def build_graphs_from_splits(train_df: pd.DataFrame, val_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Build separate graphs for training and validation.\n",
        "    \n",
        "    Args:\n",
        "        train_df: Training edges DataFrame\n",
        "        val_df: Validation edges DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        G_train: Graph with training edges only\n",
        "        G_train_val: Graph with training + validation edges\n",
        "        funds_train: Unique funds in training set\n",
        "        stocks_train: Unique stocks in training set\n",
        "    \"\"\"\n",
        "    G_train, funds_train, stocks_train = build_bipartite_graph_fastest(train_df)\n",
        "    \n",
        "    train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    G_train_val, _, _ = build_bipartite_graph_fastest(train_val_df)\n",
        "    \n",
        "    return G_train, G_train_val, funds_train, stocks_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Node Mapping and PyTorch Conversion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Node Mapping and PyTorch Conversion Functions\n",
        "\n",
        "\n",
        "def create_node_mappings(funds, stocks):\n",
        "    \"\"\"\n",
        "    Create mappings from node IDs to indices for LightGCN.\n",
        "    \n",
        "    Args:\n",
        "        funds: Array of fund identifiers\n",
        "        stocks: Array of stock identifiers\n",
        "        \n",
        "    Returns:\n",
        "        node_to_idx: Combined mapping\n",
        "        fund_to_idx: Fund ID to index mapping\n",
        "        stock_to_idx: Stock ID to index mapping\n",
        "    \"\"\"\n",
        "    # Map funds to indices [0, len(funds))\n",
        "    fund_to_idx = {fund: idx for idx, fund in enumerate(funds)}\n",
        "    \n",
        "    # Map stocks to indices [len(funds), len(funds) + len(stocks))\n",
        "    stock_to_idx = {stock: idx + len(funds) for idx, stock in enumerate(stocks)}\n",
        "    \n",
        "    # Combined mapping\n",
        "    node_to_idx = {**fund_to_idx, **stock_to_idx}\n",
        "    \n",
        "    return node_to_idx, fund_to_idx, stock_to_idx\n",
        "\n",
        "\n",
        "def graph_to_edge_index_fastest(G, node_to_idx, device):\n",
        "    \"\"\"\n",
        "    Convert NetworkX graph to PyTorch Geometric edge index format.\n",
        "    Creates bidirectional edges for undirected graph.\n",
        "    \n",
        "    Args:\n",
        "        G: NetworkX graph\n",
        "        node_to_idx: Node ID to index mapping\n",
        "        device: PyTorch device\n",
        "        \n",
        "    Returns:\n",
        "        edge_index: Tensor of shape [2, num_edges*2] (bidirectional)\n",
        "        edge_weights: Tensor of edge weights\n",
        "    \"\"\"\n",
        "    # Get all edges\n",
        "    edges_data = list(G.edges(data=True))\n",
        "    \n",
        "    # Extract u, v, weights\n",
        "    with tqdm(total=len(edges_data), desc=\"Converting to PyTorch format\") as pbar:\n",
        "        u_nodes = [node_to_idx[u] for u, _, _ in edges_data]\n",
        "        pbar.update(len(edges_data))\n",
        "        v_nodes = [node_to_idx[v] for _, v, _ in edges_data]\n",
        "        weights_list = [data.get('weight', 1.0) for _, _, data in edges_data]\n",
        "    \n",
        "    # Create bidirectional edges\n",
        "    src = u_nodes + v_nodes\n",
        "    dst = v_nodes + u_nodes\n",
        "    weights = weights_list + weights_list\n",
        "    \n",
        "    # Convert to tensors and move to device\n",
        "    edge_index = torch.tensor([src, dst], dtype=torch.long).to(device)\n",
        "    edge_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
        "    \n",
        "    return edge_index, edge_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LightGCN Model Definition\n",
        "\n",
        "class LightGCN(nn.Module):\n",
        "    \"\"\"\n",
        "    LightGCN model for bipartite graph recommendation.\n",
        "    Simplified version without feature transformation.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes: int, embedding_dim: int = 64, num_layers: int = 3):\n",
        "        super(LightGCN, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
        "        nn.init.normal_(self.embedding.weight, std=0.1)\n",
        "        \n",
        "        # GCN layers\n",
        "        self.convs = nn.ModuleList([\n",
        "            GCNConv(embedding_dim, embedding_dim, improved=False, cached=False)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, edge_index):\n",
        "        \"\"\"Forward pass through LightGCN layers.\"\"\"\n",
        "        x = self.embedding.weight\n",
        "        \n",
        "        # Aggregate embeddings from all layers\n",
        "        embeddings = [x]\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            embeddings.append(x)\n",
        "        \n",
        "        # Average all layer embeddings (LightGCN approach)\n",
        "        final_embedding = torch.stack(embeddings, dim=0).mean(dim=0)\n",
        "        \n",
        "        return final_embedding\n",
        "\n",
        "\n",
        "def initialize_model_fastest(num_nodes: int, embedding_dim: int, \n",
        "                             num_layers: int, device) -> LightGCN:\n",
        "    \"\"\"\n",
        "    Initialize LightGCN model and move to device.\n",
        "    \n",
        "    Args:\n",
        "        num_nodes: Total number of nodes in the graph\n",
        "        embedding_dim: Dimension of node embeddings\n",
        "        num_layers: Number of GCN layers\n",
        "        device: PyTorch device\n",
        "        \n",
        "    Returns:\n",
        "        Initialized LightGCN model on device\n",
        "    \"\"\"\n",
        "    model = LightGCN(\n",
        "        num_nodes=num_nodes, \n",
        "        embedding_dim=embedding_dim, \n",
        "        num_layers=num_layers\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training functions defined\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Training Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def bpr_loss(pos_scores, neg_scores):\n",
        "    \"\"\"Bayesian Personalized Ranking loss.\"\"\"\n",
        "    return -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()\n",
        "\n",
        "\n",
        "def create_positive_negative_pairs_fastest(edge_index, num_nodes, device, num_negatives=1):\n",
        "    \"\"\"\n",
        "    Create positive and negative edge pairs for training.\n",
        "    \n",
        "    Args:\n",
        "        edge_index: Tensor of edge indices\n",
        "        num_nodes: Total number of nodes\n",
        "        device: PyTorch device\n",
        "        num_negatives: Number of negative samples per positive\n",
        "        \n",
        "    Returns:\n",
        "        pos_edges: Positive edge pairs\n",
        "        neg_edges: Negative edge pairs\n",
        "    \"\"\"\n",
        "    # Extract positive edges\n",
        "    pos_edges = edge_index.t().tolist()\n",
        "    pos_set = set((u, v) for u, v in pos_edges)\n",
        "    \n",
        "    # Sample negative edges\n",
        "    target_neg_count = len(pos_edges) * num_negatives\n",
        "    neg_edges = []\n",
        "    \n",
        "    max_attempts = target_neg_count * 10\n",
        "    attempts = 0\n",
        "    \n",
        "    with tqdm(total=target_neg_count, desc=\"Sampling negative edges\") as pbar:\n",
        "        while len(neg_edges) < target_neg_count and attempts < max_attempts:\n",
        "            # Batch sampling for efficiency\n",
        "            batch_size = min(10000, target_neg_count - len(neg_edges))\n",
        "            u_batch = np.random.randint(0, num_nodes, size=batch_size)\n",
        "            v_batch = np.random.randint(0, num_nodes, size=batch_size)\n",
        "            \n",
        "            for u, v in zip(u_batch, v_batch):\n",
        "                if u != v and (u, v) not in pos_set and (v, u) not in pos_set:\n",
        "                    neg_edges.append([u, v])\n",
        "                    pbar.update(1)\n",
        "                    if len(neg_edges) >= target_neg_count:\n",
        "                        break\n",
        "            attempts += batch_size\n",
        "    \n",
        "    # Convert to tensors\n",
        "    pos_edges = torch.tensor(pos_edges, dtype=torch.long).to(device)\n",
        "    neg_edges = torch.tensor(neg_edges[:target_neg_count], dtype=torch.long).to(device)\n",
        "    \n",
        "    return pos_edges, neg_edges\n",
        "\n",
        "\n",
        "def train_with_validation_fastest(model, train_edge_index, train_val_edge_index,\n",
        "                                  train_pos_edges, train_neg_edges,\n",
        "                                  val_pos_edges, val_neg_edges, device,\n",
        "                                  epochs=50, lr=0.001, patience=10):\n",
        "    \"\"\"\n",
        "    Train model with validation monitoring and early stopping.\n",
        "    \n",
        "    Args:\n",
        "        model: LightGCN model\n",
        "        train_edge_index: Training graph edge index\n",
        "        train_val_edge_index: Training+validation graph edge index\n",
        "        train_pos_edges: Positive training edges\n",
        "        train_neg_edges: Negative training edges\n",
        "        val_pos_edges: Positive validation edges\n",
        "        val_neg_edges: Negative validation edges\n",
        "        device: PyTorch device\n",
        "        epochs: Maximum training epochs\n",
        "        lr: Learning rate\n",
        "        patience: Early stopping patience\n",
        "        \n",
        "    Returns:\n",
        "        train_losses: List of training losses\n",
        "        val_losses: List of validation losses\n",
        "        model: Trained model (best checkpoint)\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    no_improve = 0\n",
        "    \n",
        "    with tqdm(total=epochs, desc=\"Training\", unit=\"epoch\") as pbar:\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            embeddings = model(train_edge_index)\n",
        "            \n",
        "            pos_u = embeddings[train_pos_edges[:, 0]]\n",
        "            pos_v = embeddings[train_pos_edges[:, 1]]\n",
        "            pos_scores = (pos_u * pos_v).sum(dim=1)\n",
        "            \n",
        "            neg_u = embeddings[train_neg_edges[:, 0]]\n",
        "            neg_v = embeddings[train_neg_edges[:, 1]]\n",
        "            neg_scores = (neg_u * neg_v).sum(dim=1)\n",
        "            \n",
        "            train_loss = bpr_loss(pos_scores, neg_scores)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses.append(train_loss.item())\n",
        "            \n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_embeddings = model(train_val_edge_index)\n",
        "                \n",
        "                val_pos_u = val_embeddings[val_pos_edges[:, 0]]\n",
        "                val_pos_v = val_embeddings[val_pos_edges[:, 1]]\n",
        "                val_pos_scores = (val_pos_u * val_pos_v).sum(dim=1)\n",
        "                \n",
        "                val_neg_u = val_embeddings[val_neg_edges[:, 0]]\n",
        "                val_neg_v = val_embeddings[val_neg_edges[:, 1]]\n",
        "                val_neg_scores = (val_neg_u * val_neg_v).sum(dim=1)\n",
        "                \n",
        "                val_loss = bpr_loss(val_pos_scores, val_neg_scores)\n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Save best model\n",
        "            if val_loss.item() < best_val_loss:\n",
        "                best_val_loss = val_loss.item()\n",
        "                best_model_state = model.state_dict().copy()\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "            \n",
        "            # Print losses every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                print(f\"Epoch {epoch+1}: train loss = {train_loss.item():.4f}, val loss = {val_loss.item():.4f}, best val = {best_val_loss:.4f}\")\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'train': f'{train_loss.item():.4f}',\n",
        "                'val': f'{val_loss.item():.4f}',\n",
        "                'best': f'{best_val_loss:.4f}'\n",
        "            })\n",
        "            pbar.update(1)\n",
        "            \n",
        "            if no_improve >= patience:\n",
        "                pbar.set_postfix({'status': f'Early stop @ epoch {epoch+1}'})\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    return train_losses, val_losses, model\n",
        "\n",
        "\n",
        "print(\"✓ Training functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Evaluation Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate_test_set_fastest(model, train_val_edge_index, test_df, \n",
        "                              fund_to_idx, stock_to_idx, num_nodes, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with AUC-ROC, precision, recall, F1 scores\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Prepare test edges\n",
        "    test_pos_pairs = []\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Preparing test edges\"):\n",
        "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
        "            test_pos_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
        "    \n",
        "    test_pos_edges = torch.tensor(test_pos_pairs, dtype=torch.long).to(device)\n",
        "    \n",
        "    # Sample negative edges\n",
        "    test_neg_edges = []\n",
        "    test_pos_set = set((u.item(), v.item()) for u, v in test_pos_edges)\n",
        "    \n",
        "    with tqdm(total=len(test_pos_edges), desc=\"Sampling test negatives\") as pbar:\n",
        "        for _ in range(len(test_pos_edges)):\n",
        "            u = random.randint(0, num_nodes - 1)\n",
        "            v = random.randint(0, num_nodes - 1)\n",
        "            if (u, v) not in test_pos_set and u != v:\n",
        "                test_neg_edges.append([u, v])\n",
        "                pbar.update(1)\n",
        "                if len(test_neg_edges) >= len(test_pos_edges):\n",
        "                    break\n",
        "    \n",
        "    test_neg_edges = torch.tensor(test_neg_edges[:len(test_pos_edges)], dtype=torch.long).to(device)\n",
        "    \n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(train_val_edge_index)\n",
        "        \n",
        "        pos_u = embeddings[test_pos_edges[:, 0]]\n",
        "        pos_v = embeddings[test_pos_edges[:, 1]]\n",
        "        pos_scores = (pos_u * pos_v).sum(dim=1).sigmoid()\n",
        "        \n",
        "        neg_u = embeddings[test_neg_edges[:, 0]]\n",
        "        neg_v = embeddings[test_neg_edges[:, 1]]\n",
        "        neg_scores = (neg_u * neg_v).sum(dim=1).sigmoid()\n",
        "        \n",
        "        all_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
        "        all_labels = torch.cat([\n",
        "            torch.ones(len(pos_scores)),\n",
        "            torch.zeros(len(neg_scores))\n",
        "        ]).numpy()\n",
        "        \n",
        "        auc = roc_auc_score(all_labels, all_scores)\n",
        "        pred_labels = (all_scores > 0.5).astype(int)\n",
        "        precision = precision_score(all_labels, pred_labels)\n",
        "        recall = recall_score(all_labels, pred_labels)\n",
        "        f1 = f1_score(all_labels, pred_labels)\n",
        "    \n",
        "    return {\n",
        "        'auc': auc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'scores': all_scores,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_top_k_recommendations_fastest(model, train_val_edge_index, test_df,\n",
        "                                           fund_to_idx, stock_to_idx, device,\n",
        "                                           k_list=[5, 10, 20, 50]):\n",
        "    \"\"\"\n",
        "    Evaluate model using Top-K recommendation metrics (Hit Rate, NDCG).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Group test edges by fund (cik)\n",
        "    fund_to_stocks = {}\n",
        "    for _, row in test_df.iterrows():\n",
        "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
        "            fund_idx = fund_to_idx[row['cik']]\n",
        "            stock_idx = stock_to_idx[row['cusip']]\n",
        "            if fund_idx not in fund_to_stocks:\n",
        "                fund_to_stocks[fund_idx] = set()\n",
        "            fund_to_stocks[fund_idx].add(stock_idx)\n",
        "    \n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(train_val_edge_index)\n",
        "    \n",
        "    results = {k: {'hit_rate': 0, 'ndcg': 0} for k in k_list}\n",
        "    \n",
        "    # For each fund, recommend top K stocks\n",
        "    stock_indices = list(stock_to_idx.values())\n",
        "    for fund_idx, true_stocks in tqdm(fund_to_stocks.items(), desc=\"Evaluating Top-K\"):\n",
        "        fund_emb = embeddings[fund_idx]\n",
        "        \n",
        "        # Calculate scores for all stocks\n",
        "        stock_embs = embeddings[stock_indices]\n",
        "        scores = (fund_emb * stock_embs).sum(dim=1).cpu().numpy()\n",
        "        \n",
        "        # Get top K recommendations\n",
        "        top_k_indices = np.argsort(scores)[::-1]  # Descending order\n",
        "        \n",
        "        for k in k_list:\n",
        "            top_k_stocks = set([stock_indices[i] for i in top_k_indices[:k]])\n",
        "            \n",
        "            # Hit Rate\n",
        "            hits = len(top_k_stocks & true_stocks)\n",
        "            results[k]['hit_rate'] += 1 if hits > 0 else 0\n",
        "            \n",
        "            # NDCG\n",
        "            if hits > 0:\n",
        "                dcg = 0\n",
        "                for i, stock_idx in enumerate([stock_indices[j] for j in top_k_indices[:k]]):\n",
        "                    if stock_idx in true_stocks:\n",
        "                        dcg += 1 / np.log2(i + 2)\n",
        "                \n",
        "                idcg = sum(1 / np.log2(i + 2) for i in range(min(len(true_stocks), k)))\n",
        "                results[k]['ndcg'] += dcg / idcg if idcg > 0 else 0\n",
        "    \n",
        "    num_funds = len(fund_to_stocks)\n",
        "    \n",
        "    for k in k_list:\n",
        "        hit_rate = results[k]['hit_rate'] / num_funds\n",
        "        ndcg = results[k]['ndcg'] / num_funds\n",
        "        results[k]['hit_rate'] = hit_rate\n",
        "        results[k]['ndcg'] = ndcg\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def find_optimal_threshold_fastest(model, train_val_edge_index, test_df,\n",
        "                                   fund_to_idx, stock_to_idx, num_nodes, device,\n",
        "                                   plot=True):\n",
        "    \"\"\"\n",
        "    Find optimal threshold using Precision-Recall curve.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Prepare test edges (reuse from test_results if available)\n",
        "    test_pos_pairs = []\n",
        "    for _, row in test_df.iterrows():\n",
        "        if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
        "            test_pos_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
        "    \n",
        "    test_pos_edges = torch.tensor(test_pos_pairs, dtype=torch.long).to(device)\n",
        "    \n",
        "    # Sample negative edges\n",
        "    test_neg_edges = []\n",
        "    test_pos_set = set((u.item(), v.item()) for u, v in test_pos_edges)\n",
        "    \n",
        "    with tqdm(total=len(test_pos_edges), desc=\"Sampling negatives for threshold\") as pbar:\n",
        "        for _ in range(len(test_pos_edges)):\n",
        "            u = random.randint(0, num_nodes - 1)\n",
        "            v = random.randint(0, num_nodes - 1)\n",
        "            if u != v and (u, v) not in test_pos_set:\n",
        "                test_neg_edges.append([u, v])\n",
        "                pbar.update(1)\n",
        "                if len(test_neg_edges) >= len(test_pos_edges):\n",
        "                    break\n",
        "    \n",
        "    test_neg_edges = torch.tensor(test_neg_edges[:len(test_pos_edges)], dtype=torch.long).to(device)\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(train_val_edge_index)\n",
        "        \n",
        "        pos_u = embeddings[test_pos_edges[:, 0]]\n",
        "        pos_v = embeddings[test_pos_edges[:, 1]]\n",
        "        pos_scores = (pos_u * pos_v).sum(dim=1).sigmoid().cpu().numpy()\n",
        "        \n",
        "        neg_u = embeddings[test_neg_edges[:, 0]]\n",
        "        neg_v = embeddings[test_neg_edges[:, 1]]\n",
        "        neg_scores = (neg_u * neg_v).sum(dim=1).sigmoid().cpu().numpy()\n",
        "        \n",
        "        all_scores = np.concatenate([pos_scores, neg_scores])\n",
        "        all_labels = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
        "    \n",
        "    # Calculate Precision-Recall curve\n",
        "    precision, recall, thresholds = precision_recall_curve(all_labels, all_scores)\n",
        "    avg_precision = average_precision_score(all_labels, all_scores)\n",
        "    \n",
        "    # Find optimal threshold (maximize F1)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "    \n",
        "    # Plot\n",
        "    if plot:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(recall, precision, label=f'PR Curve (AP={avg_precision:.4f})')\n",
        "        plt.axvline(x=recall[optimal_idx], color='r', linestyle='--', \n",
        "                    label=f'Optimal Threshold={optimal_threshold:.4f}')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    \n",
        "    return optimal_threshold, precision, recall, thresholds\n",
        "\n",
        "\n",
        "print(\"✓ Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Main Pipeline Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Pipeline functions ready - use individual cells below to run step by step\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Main Pipeline Function (Optional - for running everything at once)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"✓ Pipeline functions ready - use individual cells below to run step by step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Pipeline Execution Steps\n",
        "\n",
        "Run each cell below step by step. Each cell executes independently and shows output immediately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 1: Initialize and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Target Date: 2024-01-01\n",
            "\n",
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading holdings_filtered_new_period_start_2024-01-01.parquet: 100%|██████████| 1/1 [00:11<00:00, 11.70s/it]\n",
            "Loading price data: 100%|██████████| 2/2 [00:00<00:00, 74.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 1,723,925 rows\n",
            "Columns: ['nameofissuer', 'cusip', 'sshprnamt', 'cik', 'year', 'quarter', 'period_start', 'name', 'ticker', 'trading_start_date', 'trading_end_date', 'price']\n"
          ]
        }
      ],
      "source": [
        "# Initialize device and random seeds\n",
        "import time\n",
        "\n",
        "if 'DEVICE' not in globals():\n",
        "    DEVICE = get_device()\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Target Date: {TARGET_DATE}\")\n",
        "\n",
        "# Load data\n",
        "print(\"\\nLoading data...\")\n",
        "df = load_holdings_data(TARGET_DATE, DATA_SOURCE, PARQUET_DIR)\n",
        "print(f\"✓ Loaded {len(df):,} rows\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered zeros: 3,730 rows removed (1,723,925 → 1,720,195)\n",
            "Merged duplicates: 637,286 rows merged (1,720,195 → 1,082,909)\n",
            "✓ Preprocessed: 1,082,909 rows remaining\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "df = preprocess_data(df)\n",
        "print(f\"✓ Preprocessed: {len(df):,} rows remaining\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Split complete: train=866,406, val=108,254, test=108,249\n"
          ]
        }
      ],
      "source": [
        "# Split into train/val/test\n",
        "train_df, val_df, test_df = split_dataframe_edges(\n",
        "    df, TRAIN_RATIO, VAL_RATIO, TEST_RATIO, RANDOM_SEED\n",
        ")\n",
        "print(f\"✓ Split complete: train={len(train_df):,}, val={len(val_df):,}, test={len(test_df):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Build Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building graph: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Building graph: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Graphs built: train=866,406 edges, train+val=974,660 edges\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Build graphs\n",
        "G_train, G_train_val, funds_train, stocks_train = build_graphs_from_splits(train_df, val_df)\n",
        "print(f\"✓ Graphs built: train={G_train.number_of_edges():,} edges, train+val={G_train_val.number_of_edges():,} edges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Create Node Mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Mappings created: 9,969 nodes (7,058 funds, 2,911 stocks)\n"
          ]
        }
      ],
      "source": [
        "# Create node mappings\n",
        "node_to_idx, fund_to_idx, stock_to_idx = create_node_mappings(funds_train, stocks_train)\n",
        "num_nodes = len(node_to_idx)\n",
        "print(f\"✓ Mappings created: {num_nodes:,} nodes ({len(fund_to_idx):,} funds, {len(stock_to_idx):,} stocks)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Convert to PyTorch Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting to PyTorch format: 100%|██████████| 866406/866406 [00:00<00:00, 1948183.55it/s]\n",
            "Converting to PyTorch format: 100%|██████████| 974660/974660 [00:00<00:00, 1883140.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Converted: train=1,732,812 edges, train+val=1,949,320 edges\n"
          ]
        }
      ],
      "source": [
        "# Convert graphs to PyTorch format\n",
        "train_edge_index, _ = graph_to_edge_index_fastest(G_train, node_to_idx, DEVICE)\n",
        "train_val_edge_index, _ = graph_to_edge_index_fastest(G_train_val, node_to_idx, DEVICE)\n",
        "print(f\"✓ Converted: train={train_edge_index.shape[1]:,} edges, train+val={train_val_edge_index.shape[1]:,} edges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Create Training Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating training pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling negative edges: 100%|██████████| 1732812/1732812 [00:02<00:00, 660071.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training pairs: 1,732,812 pos, 1,732,812 neg\n",
            "Creating validation pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing val edges: 100%|██████████| 108254/108254 [00:01<00:00, 54721.68it/s]\n",
            "Building edge set: 100%|██████████| 1949320/1949320 [08:16<00:00, 3928.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling negative edges...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling negatives: 100%|██████████| 108254/108254 [00:00<00:00, 483879.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Validation pairs: 108,254 pos, 108,254 neg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create training pairs\n",
        "print(\"Creating training pairs...\")\n",
        "train_pos_edges, train_neg_edges = create_positive_negative_pairs_fastest(\n",
        "    train_edge_index, num_nodes, DEVICE\n",
        ")\n",
        "print(f\"✓ Training pairs: {len(train_pos_edges):,} pos, {len(train_neg_edges):,} neg\")\n",
        "\n",
        "# Create validation pairs\n",
        "print(\"Creating validation pairs...\")\n",
        "val_edge_pairs = []\n",
        "for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing val edges\"):\n",
        "    if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
        "        val_edge_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
        "\n",
        "val_pos_edges = torch.tensor(val_edge_pairs, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "# Sample validation negatives\n",
        "val_neg_edges = []\n",
        "train_val_set = set()\n",
        "for i in tqdm(range(train_val_edge_index.shape[1]), desc=\"Building edge set\"):\n",
        "    u = train_val_edge_index[0, i].item()\n",
        "    v = train_val_edge_index[1, i].item()\n",
        "    train_val_set.add((u, v))\n",
        "    train_val_set.add((v, u))\n",
        "\n",
        "print(\"Sampling negative edges...\")\n",
        "with tqdm(total=len(val_pos_edges), desc=\"Sampling negatives\") as pbar:\n",
        "    while len(val_neg_edges) < len(val_pos_edges):\n",
        "        u = random.randint(0, num_nodes - 1)\n",
        "        v = random.randint(0, num_nodes - 1)\n",
        "        if u != v and (u, v) not in train_val_set:\n",
        "            val_neg_edges.append([u, v])\n",
        "            pbar.update(1)\n",
        "\n",
        "val_neg_edges = torch.tensor(val_neg_edges[:len(val_pos_edges)], dtype=torch.long).to(DEVICE)\n",
        "print(f\"✓ Validation pairs: {len(val_pos_edges):,} pos, {len(val_neg_edges):,} neg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating training pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling negative edges: 100%|██████████| 1732812/1732812 [00:03<00:00, 576172.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training pairs: 1,732,812 pos, 1,732,812 neg\n",
            "Creating validation pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing val edges: 100%|██████████| 108254/108254 [00:01<00:00, 55576.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling negative edges...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling negatives: 100%|██████████| 108254/108254 [00:00<00:00, 361676.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Validation pairs: 108,254 pos, 108,254 neg\n"
          ]
        }
      ],
      "source": [
        "# Create training pairs\n",
        "print(\"Creating training pairs...\")\n",
        "train_pos_edges, train_neg_edges = create_positive_negative_pairs_fastest(\n",
        "    train_edge_index, num_nodes, DEVICE\n",
        ")\n",
        "print(f\"✓ Training pairs: {len(train_pos_edges):,} pos, {len(train_neg_edges):,} neg\")\n",
        "\n",
        "# Create validation pairs\n",
        "print(\"Creating validation pairs...\")\n",
        "val_edge_pairs = []\n",
        "for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing val edges\"):\n",
        "    if row['cik'] in fund_to_idx and row['cusip'] in stock_to_idx:\n",
        "        val_edge_pairs.append([fund_to_idx[row['cik']], stock_to_idx[row['cusip']]])\n",
        "\n",
        "val_pos_edges = torch.tensor(val_edge_pairs, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "# Sample validation negatives\n",
        "val_neg_edges = []\n",
        "\n",
        "# Vectorized version: Build edge set from train_val_edge_index\n",
        "# Convert tensor to numpy array in one operation (much faster than looping)\n",
        "edges_np = train_val_edge_index.t().cpu().numpy()  # Shape: [num_edges, 2]\n",
        "train_val_set = set(map(tuple, edges_np))\n",
        "\n",
        "# Add reverse edges (u,v) -> (v,u) to ensure bidirectional coverage\n",
        "train_val_set |= {(v, u) for (u, v) in train_val_set}\n",
        "\n",
        "print(\"Sampling negative edges...\")\n",
        "with tqdm(total=len(val_pos_edges), desc=\"Sampling negatives\") as pbar:\n",
        "    while len(val_neg_edges) < len(val_pos_edges):\n",
        "        u = random.randint(0, num_nodes - 1)\n",
        "        v = random.randint(0, num_nodes - 1)\n",
        "        if u != v and (u, v) not in train_val_set:\n",
        "            val_neg_edges.append([u, v])\n",
        "            pbar.update(1)\n",
        "\n",
        "val_neg_edges = torch.tensor(val_neg_edges[:len(val_pos_edges)], dtype=torch.long).to(DEVICE)\n",
        "print(f\"✓ Validation pairs: {len(val_pos_edges):,} pos, {len(val_neg_edges):,} neg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model initialized: 1,325,568 parameters on cuda\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = initialize_model_fastest(num_nodes, EMBEDDING_DIM, NUM_LAYERS, DEVICE)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✓ Model initialized: {total_params:,} parameters on {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9: Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|▏         | 1/50 [00:09<08:00,  9.80s/epoch, train=0.6872, val=0.6807, best=0.6807]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train loss = 0.6872, val loss = 0.6807, best val = 0.6807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|█         | 5/50 [00:45<06:37,  8.84s/epoch, train=0.6295, val=0.5956, best=0.5956]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: train loss = 0.6295, val loss = 0.5956, best val = 0.5956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|██        | 10/50 [01:32<06:14,  9.37s/epoch, train=0.3529, val=0.2793, best=0.2793]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: train loss = 0.3529, val loss = 0.2793, best val = 0.2793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  30%|███       | 15/50 [02:18<05:27,  9.35s/epoch, train=0.1625, val=0.1674, best=0.1567]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: train loss = 0.1625, val loss = 0.1674, best val = 0.1567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  40%|████      | 20/50 [03:06<04:45,  9.52s/epoch, train=0.2268, val=0.2155, best=0.1567]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: train loss = 0.2268, val loss = 0.2155, best val = 0.1567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  48%|████▊     | 24/50 [03:42<04:01,  9.28s/epoch, status=Early stop @ epoch 24]         \n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "train_losses, val_losses, model = train_with_validation_fastest(\n",
        "    model, train_edge_index, train_val_edge_index,\n",
        "    train_pos_edges, train_neg_edges,\n",
        "    val_pos_edges, val_neg_edges, DEVICE,\n",
        "    epochs=EPOCHS, lr=LEARNING_RATE, patience=EARLY_STOPPING_PATIENCE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10: Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing test edges: 100%|██████████| 108249/108249 [00:02<00:00, 43915.88it/s]\n",
            "Sampling test negatives: 100%|█████████▉| 108105/108249 [00:03<00:00, 27274.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Top-K recommendations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Top-K: 100%|██████████| 6041/6041 [00:04<00:00, 1503.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding optimal threshold...\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_results = evaluate_test_set_fastest(\n",
        "    model, train_val_edge_index, test_df,\n",
        "    fund_to_idx, stock_to_idx, num_nodes, DEVICE\n",
        ")\n",
        "\n",
        "# Top-K evaluation\n",
        "print(\"\\nEvaluating Top-K recommendations...\")\n",
        "top_k_results = evaluate_top_k_recommendations_fastest(\n",
        "    model, train_val_edge_index, test_df,\n",
        "    fund_to_idx, stock_to_idx, DEVICE, TOP_K_VALUES\n",
        ")\n",
        "\n",
        "# Find optimal threshold\n",
        "print(\"\\nFinding optimal threshold...\")\n",
        "optimal_threshold, _, _, _ = find_optimal_threshold_fastest(\n",
        "    model, train_val_edge_index, test_df,\n",
        "    fund_to_idx, stock_to_idx, num_nodes, DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 11: Save Model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model if configured\n",
        "if SAVE_MODEL:\n",
        "    save_path = Path(MODEL_SAVE_DIR)\n",
        "    save_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    model_file = save_path / f\"lightgcn_{TARGET_DATE}.pth\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'num_nodes': num_nodes,\n",
        "        'embedding_dim': EMBEDDING_DIM,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'fund_to_idx': fund_to_idx,\n",
        "        'stock_to_idx': stock_to_idx,\n",
        "        'target_date': TARGET_DATE,\n",
        "        'test_results': test_results,\n",
        "        'optimal_threshold': optimal_threshold\n",
        "    }, model_file)\n",
        "    print(f\"✓ Model saved to: {model_file}\")\n",
        "else:\n",
        "    print(\"Model saving disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12: View Results and Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "print(\"=\"*60)\n",
        "print(\"Final Results\")\n",
        "print(\"=\"*60)\n",
        "print(f\"AUC-ROC: {test_results['auc']:.4f}\")\n",
        "print(f\"Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_results['f1']:.4f}\")\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(\"\\nTop-K Results:\")\n",
        "for k, metrics in top_k_results.items():\n",
        "    print(f\"  K={k:2d}: Hit Rate={metrics['hit_rate']:.4f}, NDCG={metrics['ndcg']:.4f}\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('BPR Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar([f'K={k}' for k in TOP_K_VALUES], \n",
        "        [top_k_results[k]['hit_rate'] for k in TOP_K_VALUES],\n",
        "        alpha=0.7, label='Hit Rate')\n",
        "plt.bar([f'K={k}' for k in TOP_K_VALUES], \n",
        "        [top_k_results[k]['ndcg'] for k in TOP_K_VALUES],\n",
        "        alpha=0.7, label='NDCG')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Top-K Recommendation Metrics')\n",
        "plt.legend()\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 2: Preprocess Data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd74b1",
   "metadata": {},
   "source": [
    "# Stock Market Social Network Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25538a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Status: CUDA compatibility issue: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Using device: cpu (CPU fallback)\n",
      "Note: For GPU support, ensure PyTorch is installed with CUDA support matching your GPU\n",
      "Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.centrality import degree_centrality, closeness_centrality\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
    "from networkx.algorithms.link_analysis.hits_alg import hits\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Setup with compatibility checks\n",
    "def check_cuda_compatibility():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False, \"CUDA not available\"\n",
    "    \n",
    "    try:\n",
    "        # Test basic CUDA operation\n",
    "        test_tensor = torch.zeros(1).cuda()\n",
    "        test_tensor = test_tensor + 1\n",
    "        return True, \"CUDA compatible\"\n",
    "    except Exception as e:\n",
    "        return False, f\"CUDA compatibility issue: {str(e)}\"\n",
    "\n",
    "cuda_compatible, cuda_message = check_cuda_compatibility()\n",
    "print(f\"CUDA Status: {cuda_message}\")\n",
    "\n",
    "if cuda_compatible:\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'PyTorch Version: {torch.__version__}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Using device: {device} (CPU fallback)')\n",
    "    print('Note: For GPU support, ensure PyTorch is installed with CUDA support matching your GPU')\n",
    "    print('Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d499",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning\n",
    "Load and preprocess the raw fund-stock holding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd713ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after cleaning: 3996\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the data\n",
    "file_paths = [\n",
    "    'short_Infotable_Q1_2018_A.csv',\n",
    "    'short_Infotable_Q2_2018_A.csv',\n",
    "    'short_Infotable_Q3_2018_A.csv',\n",
    "    'short_Infotable_Q4_2018_A.csv'\n",
    "]\n",
    "dfs = []\n",
    "for i, file in enumerate(file_paths):\n",
    "    df = pd.read_csv(file)\n",
    "    df['QUARTER'] = f'Q{i+1}_2018'\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "data = data[['CIK', 'CUSIP', 'VALUE', 'SSHPRNAMT', 'PERIOD_DATE', 'QUARTER']]\n",
    "data['PERIOD_DATE'] = pd.to_datetime(data['PERIOD_DATE'])\n",
    "data = data.dropna(subset=['CIK', 'CUSIP', 'VALUE'])\n",
    "data['CIK'] = data['CIK'].astype(str)\n",
    "data['CUSIP'] = data['CUSIP'].astype(str)\n",
    "data = data.sort_values(by='PERIOD_DATE')\n",
    "print(f'Total records after cleaning: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78fae4",
   "metadata": {},
   "source": [
    "## 3. Graph Construction\n",
    "Build bipartite and projected graphs for funds and stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ecbda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_and_features_up_to(max_date):\n",
    "    df_up_to = data[data['PERIOD_DATE'] <= max_date].copy()\n",
    "    \n",
    "    funds_up_to = df_up_to['CIK'].unique()\n",
    "    stocks_up_to = df_up_to['CUSIP'].unique()\n",
    "    \n",
    "    G_bip = nx.Graph()\n",
    "    G_bip.add_nodes_from(funds_up_to, bipartite=0)\n",
    "    G_bip.add_nodes_from(stocks_up_to, bipartite=1)\n",
    "    \n",
    "    edges = [(row.CIK, row.CUSIP, \n",
    "            {'value': row.VALUE, 'amount': row.SSHPRNAMT, 'time': row.PERIOD_DATE})\n",
    "            for row in df_up_to.itertuples(index=False)]\n",
    "    G_bip.add_edges_from(edges)\n",
    "        \n",
    "    # Fund-Fund projection with weights (weighted by shared stocks)\n",
    "    G_fund = bipartite.weighted_projected_graph(G_bip, funds_up_to)\n",
    "    \n",
    "    # Convert to directed based on time, only for existing edges\n",
    "    G_fund_directed = nx.DiGraph()\n",
    "    for u, v, data_dict in G_fund.edges(data=True):\n",
    "        shared = set(G_bip.neighbors(u)) & set(G_bip.neighbors(v))\n",
    "        if not shared:\n",
    "            continue\n",
    "        \n",
    "        times_u = [G_bip.edges[u,s]['time'] for s in shared]\n",
    "        times_v = [G_bip.edges[v,s]['time'] for s in shared]\n",
    "        avg_u = np.mean([t.timestamp() for t in times_u])\n",
    "        avg_v = np.mean([t.timestamp() for t in times_v])\n",
    "        \n",
    "        weight = data_dict.get('weight', 1)\n",
    "        \n",
    "        if avg_u < avg_v:\n",
    "            G_fund_directed.add_edge(u, v, weight=weight)\n",
    "        else:\n",
    "            G_fund_directed.add_edge(v, u, weight=weight)\n",
    "    \n",
    "    G_fund = G_fund_directed  # Replace with directed version\n",
    "    \n",
    "    # Topological features (על G_fund המכוון)\n",
    "    degree_cent = degree_centrality(G_fund)\n",
    "    pagerank_cent = nx.pagerank(G_fund)\n",
    "    hubs, authorities = hits(G_fund)\n",
    "    largest_cc = max(nx.connected_components(G_fund.to_undirected()), key=len)\n",
    "    closeness_cent = closeness_centrality(G_fund.to_undirected().subgraph(largest_cc))\n",
    "    \n",
    "    fund_features = pd.DataFrame({\n",
    "        'fund': list(G_fund.nodes()),\n",
    "        'degree': [degree_cent.get(n, 0) for n in G_fund.nodes()],\n",
    "        'pagerank': [pagerank_cent.get(n, 0) for n in G_fund.nodes()],\n",
    "        'hub': [hubs.get(n, 0) for n in G_fund.nodes()],\n",
    "        'authority': [authorities.get(n, 0) for n in G_fund.nodes()],\n",
    "        'closeness': [closeness_cent.get(n, 0) for n in G_fund.nodes()]\n",
    "    }).set_index('fund')\n",
    "    \n",
    "    # Community (Leiden)\n",
    "    vertex_names = list(G_fund.nodes())\n",
    "    vertex_to_idx = {v: i for i, v in enumerate(vertex_names)}\n",
    "\n",
    "    # המר edges לאינדקסים\n",
    "    edge_list = [(vertex_to_idx[u], vertex_to_idx[v]) \n",
    "                for u, v in G_fund.to_undirected().edges()]\n",
    "\n",
    "    ig_G = ig.Graph(n=len(vertex_names), edges=edge_list)\n",
    "    ig_G.vs['_nx_name'] = vertex_names\n",
    "    partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "    communities = {ig_G.vs[i]['_nx_name']: p for p, cl in enumerate(partition) for i in cl}\n",
    "    fund_features['community'] = fund_features.index.map(communities).fillna(-1)\n",
    "    \n",
    "    return G_bip, G_fund, fund_features, df_up_to, funds_up_to, stocks_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6970f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full graph and features for all data (for prediction on any fund)\n",
    "full_max_date = data['PERIOD_DATE'].max()\n",
    "G_full, G_fund_full, fund_features_full, df_full, funds_full, stocks_full = build_graph_and_features_up_to(full_max_date)\n",
    "fund_idx_full = {f: i for i, f in enumerate(funds_full)}\n",
    "stock_idx_full = {s: i for i, s in enumerate(stocks_full)}\n",
    "# For Q4-only funds (unbiased prediction)\n",
    "q4_min_date = data[data['QUARTER'] == 'Q4_2018']['PERIOD_DATE'].min()\n",
    "q4_max_date = data[data['QUARTER'] == 'Q4_2018']['PERIOD_DATE'].max()\n",
    "funds_q4 = set(data[(data['PERIOD_DATE'] >= q4_min_date) & (data['PERIOD_DATE'] <= q4_max_date)]['CIK'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe222b13",
   "metadata": {},
   "source": [
    "## 4. Training Phase (Up to Q3)\n",
    "Split the data temporally and prepare for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28204d90",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7cdaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipartite Graph: 2850 edges\n",
      "Fund-Fund Graph: 36 edges\n"
     ]
    }
   ],
   "source": [
    "# Use only up to Q2 for training (temporal split)\n",
    "train_max_date = pd.to_datetime('2018-06-30')  # End of Q2\n",
    "G_bip_train, G_fund_train, fund_features_train, df_train, funds_train, stocks_train = build_graph_and_features_up_to(train_max_date)\n",
    "fund_idx_train = {f: i for i, f in enumerate(funds_train)}\n",
    "stock_idx_train = {s: i for i, s in enumerate(stocks_train)}\n",
    "print(f\"Bipartite Graph: {G_bip_train.number_of_edges()} edges\")\n",
    "print(f\"Fund-Fund Graph: {G_fund_train.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f52390",
   "metadata": {},
   "source": [
    "## 5. GraphSAGE Embedding Training\n",
    "Train GraphSAGE on the training graph to generate node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c29f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features initialized on cpu\n",
      "Edge index initialized on cpu\n",
      "Model initialized on cpu\n",
      "Starting training on cpu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3611\n",
      "Epoch 10, Loss: 0.0000\n",
      "Early stopping at epoch 11\n",
      "GraphSAGE training completed on cpu.\n",
      "dynamic_emb_train shape: (9, 8)\n",
      "stock_emb_train shape: (1891, 8)\n"
     ]
    }
   ],
   "source": [
    "# Node features - use simple random vectors (can be improved)\n",
    "num_nodes = len(funds_train) + len(stocks_train)\n",
    "\n",
    "# Try to move to GPU, fallback to CPU if error\n",
    "try:\n",
    "    x = torch.randn(num_nodes, 16).to(device)\n",
    "    print(f\"Node features initialized on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving to GPU, using CPU: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "    x = torch.randn(num_nodes, 16).to(device)\n",
    "\n",
    "# Map indices for homogeneous graph (train only)\n",
    "edge_index_homo = []\n",
    "for u, v in G_bip_train.edges():\n",
    "    u_idx = fund_idx_train[u] if u in fund_idx_train else stock_idx_train.get(u, -1)\n",
    "    v_idx = fund_idx_train[v] if v in fund_idx_train else stock_idx_train.get(v, -1)\n",
    "    if u_idx != -1 and v_idx != -1:\n",
    "        edge_index_homo.append([u_idx, v_idx])\n",
    "        edge_index_homo.append([v_idx, u_idx])  # undirected\n",
    "\n",
    "try:\n",
    "    edge_index_homo = torch.tensor(edge_index_homo, dtype=torch.long).t().contiguous().to(device)\n",
    "    print(f\"Edge index initialized on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving edge index to GPU, using CPU: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "    x = x.cpu()\n",
    "    edge_index_homo = torch.tensor(edge_index_homo, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "# Define GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "try:\n",
    "    model = GraphSAGE(16, 32, 8).to(device)\n",
    "    print(f\"Model initialized on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving model to GPU, using CPU: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "    x = x.cpu()\n",
    "    edge_index_homo = edge_index_homo.cpu()\n",
    "    model = GraphSAGE(16, 32, 8).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train GraphSAGE with early stopping\n",
    "model.train()\n",
    "prev_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"Starting training on {device}...\")\n",
    "for epoch in range(50):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, edge_index_homo)\n",
    "        # Loss on existing edges (link prediction style)\n",
    "        pos_score = (out[edge_index_homo[0]] * out[edge_index_homo[1]]).sum(dim=1).sigmoid()\n",
    "        loss = -torch.log(pos_score + 1e-15).mean()  # add epsilon to avoid log(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if abs(prev_loss - loss.item()) < 1e-6:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        else:\n",
    "            no_improve = 0\n",
    "        prev_loss = loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA' in str(e):\n",
    "            print(f\"CUDA error during training, switching to CPU: {e}\")\n",
    "            device = torch.device('cpu')\n",
    "            x = x.cpu()\n",
    "            edge_index_homo = edge_index_homo.cpu()\n",
    "            model = model.cpu()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            print(\"Restarting training on CPU...\")\n",
    "            epoch = 0\n",
    "            prev_loss = float('inf')\n",
    "            no_improve = 0\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Extract embeddings and move to CPU for further processing\n",
    "with torch.no_grad():\n",
    "    emb = model(x, edge_index_homo).cpu().numpy()\n",
    "\n",
    "# Split embeddings for funds and stocks\n",
    "dynamic_emb_train = emb[:len(funds_train)]      # shape: [len(funds_train), 8]\n",
    "stock_emb_train = emb[len(funds_train):]        # shape: [len(stocks_train), 8]\n",
    "\n",
    "print(f\"GraphSAGE training completed on {device}.\")\n",
    "print(f\"dynamic_emb_train shape: {dynamic_emb_train.shape}\")\n",
    "print(f\"stock_emb_train shape: {stock_emb_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c5bec",
   "metadata": {},
   "source": [
    "## 6. Save Training Artifacts\n",
    "Save embeddings, features, and model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1b16ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cache trained artifacts (run ONCE) ───────────────────────────────────────\n",
    "GRAPH_READY = True\n",
    "_cached_graph = G_bip_train\n",
    "_cached_embeddings = stock_emb_train\n",
    "_cached_model = model.cpu()  # Move model to CPU for caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b398ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to artifacts/ directory.\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings, model, and graph to files for later use\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a directory for artifacts if it doesn't exist\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "# Save stock embeddings\n",
    "np.save('artifacts/stock_emb_train.npy', stock_emb_train)\n",
    "\n",
    "# Save dynamic fund embeddings\n",
    "np.save('artifacts/dynamic_emb_train.npy', dynamic_emb_train)\n",
    "\n",
    "# Save fund features\n",
    "fund_features_train.to_pickle('artifacts/fund_features_train.pkl')\n",
    "\n",
    "# Save LightGBM model (bst) if it exists\n",
    "if 'bst' in globals():\n",
    "    joblib.dump(bst, 'artifacts/lightgbm_bst.pkl')\n",
    "\n",
    "# Save funds and stocks lists\n",
    "with open('artifacts/funds_train.pkl', 'wb') as f:\n",
    "    pickle.dump(funds_train, f)\n",
    "with open('artifacts/stocks_train.pkl', 'wb') as f:\n",
    "    pickle.dump(stocks_train, f)\n",
    "\n",
    "print('Artifacts saved to artifacts/ directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e803f53",
   "metadata": {},
   "source": [
    "## 7. Test Phase (Q4)\n",
    "Prepare test data and features for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f253cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3803 positive and 357 negative test samples\n",
      "Test set positive ratio: 0.914 (1=positive, 0=negative)\n"
     ]
    }
   ],
   "source": [
    "# Test Phase (Q3) - Using only funds and stocks seen in train (Q1-Q2)\n",
    "test_min_date = data[data['QUARTER'] == 'Q3_2018']['PERIOD_DATE'].min()\n",
    "test_max_date = data[data['QUARTER'] == 'Q3_2018']['PERIOD_DATE'].max()\n",
    "test_data = data[(data['PERIOD_DATE'] >= test_min_date) & (data['PERIOD_DATE'] <= test_max_date)]\n",
    "# Only use funds and stocks that were seen in train\n",
    "funds_test = set(test_data['CIK'].unique()) & set(funds_train)\n",
    "stocks_test = set(test_data['CUSIP'].unique()) & set(stocks_train)\n",
    "# positive edges: רק Q3, אבל רק עבור funds/stocks שמופיעים ב-train\n",
    "pos_edges_test = []\n",
    "for row in test_data.itertuples(index=False):\n",
    "    cik = row.CIK\n",
    "    cusip = row.CUSIP\n",
    "    if cik in funds_test and cusip in stocks_test:\n",
    "        pos_edges_test.append((fund_idx_train[cik], stock_idx_train[cusip], 1))\n",
    "# negative sampling: רק על embeddings של train (שכבר חושבו ב-Cell 4!)\n",
    "stock_sim_train = cosine_similarity(stock_emb_train)\n",
    "fund_to_connected = {f_idx: [stock_idx_train[s] for s in G_bip_train.neighbors(fund) if s in stocks_test and s in stock_idx_train]\n",
    "                     for f_idx, fund in enumerate(funds_train) if fund in funds_test}\n",
    "neg_edges_test = []\n",
    "for f_idx in range(len(funds_train)):\n",
    "    if funds_train[f_idx] not in funds_test:\n",
    "        continue\n",
    "    connected = fund_to_connected.get(f_idx, [])\n",
    "    if not connected:\n",
    "        continue\n",
    "    sim_scores = stock_sim_train[connected].mean(axis=0)\n",
    "    hard_negs = np.argsort(-sim_scores)[len(connected):len(connected)+50]\n",
    "    neg_edges_test.extend([(f_idx, neg_idx, 0) for neg_idx in hard_negs if neg_idx not in connected and neg_idx < len(stocks_train) and stocks_train[neg_idx] in stocks_test])\n",
    "neg_edges_test = neg_edges_test[:len(pos_edges_test)]\n",
    "# Link features\n",
    "link_data = []\n",
    "for f_i, s_i, label in pos_edges_test + neg_edges_test:\n",
    "    fund_id = funds_train[f_i]\n",
    "    feat = np.concatenate([\n",
    "        dynamic_emb_train[f_i],\n",
    "        stock_emb_train[s_i],\n",
    "        fund_features_train.loc[fund_id, ['degree', 'pagerank', 'hub', 'authority', 'closeness', 'community']].values\n",
    "    ])\n",
    "    link_data.append((feat, label))\n",
    "X = np.array([d[0] for d in link_data])\n",
    "y = np.array([d[1] for d in link_data])\n",
    "print(f\"Created {len(pos_edges_test)} positive and {len(neg_edges_test)} negative test samples\")\n",
    "print(f\"Test set positive ratio: {np.mean(y):.3f} (1=positive, 0=negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a64d94",
   "metadata": {},
   "source": [
    "## 8. LightGBM Training & Evaluation\n",
    "Train and evaluate the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9f60ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC mean: nan (±nan)\n",
      "CV Precision mean: 0.8970 (±0.2061)\n",
      "Final LightGBM model trained and saved to artifacts/lightgbm_bst.pkl.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# LightGBM parameters (define before use)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train/test/final model only ONCE!\n",
    "if 'bst' not in globals():\n",
    "    tscv = TimeSeriesSplit(n_splits=5)  # 5 folds, expanding train set each time\n",
    "    cv_aucs = cross_val_score(lgb.LGBMClassifier(**params), X, y, cv=tscv, scoring='roc_auc')\n",
    "    print(f\"CV AUC mean: {cv_aucs.mean():.4f} (±{cv_aucs.std():.4f})\")\n",
    "    precisions = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X[train_idx], X[test_idx]\n",
    "        y_train_cv, y_test_cv = y[train_idx], y[test_idx]\n",
    "        train_data_cv = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        test_data_cv = lgb.Dataset(X_test_cv, label=y_test_cv, reference=train_data_cv)\n",
    "        bst_cv = lgb.train(params, train_data_cv, num_boost_round=100, valid_sets=[test_data_cv])\n",
    "        y_pred_cv = bst_cv.predict(X_test_cv)\n",
    "        precision_cv = precision_score(y_test_cv, (y_pred_cv > 0.5).astype(int))\n",
    "        precisions.append(precision_cv)\n",
    "    print(f\"CV Precision mean: {np.mean(precisions):.4f} (±{np.std(precisions):.4f})\")\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    bst = lgb.train(params, train_data, num_boost_round=100)\n",
    "    # --- Save model to artifacts folder ---\n",
    "    os.makedirs('artifacts', exist_ok=True)\n",
    "    joblib.dump(bst, os.path.join('artifacts', 'lightgbm_bst.pkl'))\n",
    "    print('Final LightGBM model trained and saved to artifacts/lightgbm_bst.pkl.')\n",
    "else:\n",
    "    print('LightGBM model already trained and loaded in memory. Skipping retraining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed5dca",
   "metadata": {},
   "source": [
    "## 9. Prediction Function\n",
    "Function to predict top stocks for a given fund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_portfolio(fund_id):\n",
    "    \"\"\"\n",
    "    Efficiently predict top stocks for a given fund_id using precomputed artifacts.\n",
    "    Assumes all heavy computations (embeddings, features, model) are preloaded and cached.\n",
    "    \"\"\"\n",
    "    global bst, dynamic_emb_train, stock_emb_train, fund_features_train, funds_train, stocks_train\n",
    "    if 'bst' not in globals() or 'dynamic_emb_train' not in globals() or 'stock_emb_train' not in globals():\n",
    "        raise RuntimeError(\"Artifacts not loaded. Please run the training cells or load artifacts.\")\n",
    "    fund_idx = {f: i for i, f in enumerate(funds_train)}\n",
    "    stock_list = stocks_train\n",
    "    if fund_id not in fund_idx:\n",
    "        print(f\"Fund with CIK {fund_id} not found in the data.\")\n",
    "        return []\n",
    "    f_idx = fund_idx[fund_id]\n",
    "    fund_id_str = funds_train[f_idx]\n",
    "    fund_emb_repeat = np.tile(dynamic_emb_train[f_idx], (len(stock_list), 1))\n",
    "    fund_topo_repeat = np.tile(fund_features_train.loc[fund_id_str, ['degree', 'pagerank', 'hub', 'authority', 'closeness', 'community']].values, (len(stock_list), 1))\n",
    "    feats = np.concatenate([fund_emb_repeat, stock_emb_train, fund_topo_repeat], axis=1)\n",
    "    preds = bst.predict(feats)\n",
    "    stock_preds = list(zip(stock_list, preds))\n",
    "    top_stocks = sorted(stock_preds, key=lambda x: x[1], reverse=True)[:5]\n",
    "    return top_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c77bb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded successfully. You can now run predictions without retraining.\n"
     ]
    }
   ],
   "source": [
    "# Load all artifacts (embeddings, features, model) for fast prediction\n",
    "# (imports moved to the first cell)\n",
    "\n",
    "artifacts_path = 'artifacts'\n",
    "\n",
    "def load_artifacts():\n",
    "    global stock_emb_train, dynamic_emb_train, fund_features_train, bst, funds_train, stocks_train\n",
    "    if os.path.exists(artifacts_path):\n",
    "        try:\n",
    "            stock_emb_train = np.load(os.path.join(artifacts_path, 'stock_emb_train.npy'))\n",
    "            dynamic_emb_train = np.load(os.path.join(artifacts_path, 'dynamic_emb_train.npy'))\n",
    "            fund_features_train = pd.read_pickle(os.path.join(artifacts_path, 'fund_features_train.pkl'))\n",
    "            bst = joblib.load(os.path.join(artifacts_path, 'lightgbm_bst.pkl'))\n",
    "            with open(os.path.join(artifacts_path, 'funds_train.pkl'), 'rb') as f:\n",
    "                funds_train = pickle.load(f)\n",
    "            with open(os.path.join(artifacts_path, 'stocks_train.pkl'), 'rb') as f:\n",
    "                stocks_train = pickle.load(f)\n",
    "            print('Artifacts loaded successfully. You can now run predictions without retraining.')\n",
    "        except Exception as e:\n",
    "            print('Failed to load artifacts:', e)\n",
    "    else:\n",
    "        print('Artifacts directory not found. Please run the training cells first.')\n",
    "\n",
    "# Load artifacts at notebook startup for fast prediction\n",
    "load_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919d110",
   "metadata": {},
   "source": [
    "## 10. Predict for a Specific Fund\n",
    "Example: Predict for a random out-of-sample fund."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd2e45",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> - To make an unbiased next-quarter prediction, you must use only funds that were seen in training (Q1-Q2) and predict their holdings in Q4.\n",
    "> - Predicting for funds that were not seen in training is not possible (no embeddings/features for them).\n",
    "> - Predicting for funds using their Q4 data in training or feature construction would cause data leakage and bias.\n",
    "> - The code below ensures you only predict for eligible funds, with no leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b704e9e",
   "metadata": {},
   "source": [
    "## 10.1. List Eligible Funds for Next-Quarter Prediction\n",
    "Print all funds that were seen in training (Q1-Q2) and also appear in Q4. These are the only funds for which you can make an unbiased next-quarter prediction (no data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0f5c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible funds for Q4 prediction: 9\n",
      "Sample of eligible funds (first 20):\n",
      "['1021249', '1325091', '1345929', '1424116', '1535839', '1576102', '1623678', '1694461', '1697457']\n"
     ]
    }
   ],
   "source": [
    "# List funds eligible for next-quarter (Q4) prediction: must be seen in train (Q1-Q2) and appear in Q4\n",
    "if 'funds_train' in globals() and 'funds_q4' in globals():\n",
    "    eligible_funds = sorted(list(set(funds_train) & set(funds_q4)))\n",
    "    print(f\"Number of eligible funds for Q4 prediction: {len(eligible_funds)}\")\n",
    "    print(\"Sample of eligible funds (first 20):\")\n",
    "    print(eligible_funds[:20])\n",
    "    # You can select any fund from this list for next-quarter prediction (no leakage, no bias)\n",
    "else:\n",
    "    print('Required fund lists not found. Please ensure all previous cells have been run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3d8e2",
   "metadata": {},
   "source": [
    "## 10.1. Random/Specific Fund Next-Quarter Prediction\n",
    "To choode specific FUND from the list, update the var *fund_id_to_predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f391ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected eligible fund for Q4 prediction: 1535839\n",
      "Top recommended stocks for Q4:\n",
      "  880349105: 0.9561\n",
      "  67555N206: 0.9561\n",
      "  67420T206: 0.9561\n",
      "  85570W100: 0.9561\n",
      "  983134107: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Example: Predict for a specific or random eligible fund (seen in train, predict Q4)\n",
    "fund_id_to_predict = '1535839'  # Set to a specific CIK string to choose a fund, or leave as None for random\n",
    "fund_list_for_prediction = eligible_funds if 'eligible_funds' in globals() and len(eligible_funds) > 0 else []\n",
    "if len(fund_list_for_prediction) > 0:\n",
    "    if fund_id_to_predict is not None and str(fund_id_to_predict) in fund_list_for_prediction:\n",
    "        selected_fund = str(fund_id_to_predict)\n",
    "        print(f'Selected eligible fund for Q4 prediction: {selected_fund}')\n",
    "    else:\n",
    "        selected_fund = random.choice(fund_list_for_prediction)\n",
    "        print(f'Randomly selected eligible fund for Q4 prediction: {selected_fund}')\n",
    "    top_stocks = predict_portfolio(selected_fund)\n",
    "    print('Top recommended stocks for Q4:')\n",
    "    for stock, score in top_stocks:\n",
    "        print(f'  {stock}: {score:.4f}')\n",
    "else:\n",
    "    print('No eligible funds available for prediction. Please check your data and artifacts.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

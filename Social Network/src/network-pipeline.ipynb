{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd74b1",
   "metadata": {},
   "source": [
    "# Stock Market Social Network Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25538a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Status: CUDA compatibility issue: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Using device: cpu (CPU fallback)\n",
      "Note: For GPU support, ensure PyTorch is installed with CUDA support matching your GPU\n",
      "Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.centrality import degree_centrality, closeness_centrality\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
    "from networkx.algorithms.link_analysis.hits_alg import hits\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Setup with compatibility checks\n",
    "def check_cuda_compatibility():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False, \"CUDA not available\"\n",
    "    \n",
    "    try:\n",
    "        # Test basic CUDA operation\n",
    "        test_tensor = torch.zeros(1).cuda()\n",
    "        test_tensor = test_tensor + 1\n",
    "        return True, \"CUDA compatible\"\n",
    "    except Exception as e:\n",
    "        return False, f\"CUDA compatibility issue: {str(e)}\"\n",
    "\n",
    "cuda_compatible, cuda_message = check_cuda_compatibility()\n",
    "print(f\"CUDA Status: {cuda_message}\")\n",
    "\n",
    "if cuda_compatible:\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'PyTorch Version: {torch.__version__}')\n",
    "    # Set default tensor type to CUDA\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Using device: {device} (CPU fallback)')\n",
    "    print('Note: For GPU support, ensure PyTorch is installed with CUDA support matching your GPU')\n",
    "    print('Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894d66e",
   "metadata": {},
   "source": [
    "## 1.1. Extract Parquet Files from Zip Archives\n",
    "Extract all zip files in the parquet_files directory and remove the zip files after extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4587eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No zip files found in /home/zenoua/Social-Network-Stock-Market/Social Network/parquet_files\n"
     ]
    }
   ],
   "source": [
    "# Extract zip files from parquet_files directory\n",
    "persoanl_dir = os.path.expanduser('~')\n",
    "parquet_dir = os.path.join(persoanl_dir, 'Social-Network-Stock-Market/Social Network/parquet_files')\n",
    "if os.path.exists(parquet_dir):\n",
    "    zip_files = [f for f in os.listdir(parquet_dir) if f.endswith('.zip')]\n",
    "    \n",
    "    if zip_files:\n",
    "        print(f'Found {len(zip_files)} zip file(s) in {parquet_dir}')\n",
    "        \n",
    "        for zip_file in zip_files:\n",
    "            zip_path = os.path.join(parquet_dir, zip_file)\n",
    "            print(f'Extracting {zip_file}...')\n",
    "            \n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(parquet_dir)\n",
    "                print(f'  ✓ Extracted {zip_file}')\n",
    "                \n",
    "                # Delete the zip file after extraction\n",
    "                os.remove(zip_path)\n",
    "                print(f'  ✓ Deleted {zip_file}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'  ✗ Error processing {zip_file}: {e}')\n",
    "        \n",
    "        print('All zip files processed and cleaned up.')\n",
    "    else:\n",
    "        print(f'No zip files found in {parquet_dir}')\n",
    "else:\n",
    "    print(f'Directory not found: {parquet_dir}')\n",
    "    print('Creating directory...')\n",
    "    os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d499",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning\n",
    "Load and preprocess the raw fund-stock holding data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720e3e6",
   "metadata": {},
   "source": [
    "##### Creating new parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f576e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/ticker_to_cusip.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the data\n",
    "\n",
    "root = os.path.join(persoanl_dir, 'Social-Network-Stock-Market/Social Network/parquuet_files')\n",
    "print(f\"{root}/ticker_to_cusip.parquet\")\n",
    "ticker_map = pd.read_parquet(f\"{root}/ticker_to_cusip.parquet\")\n",
    "prices = pd.read_parquet(f\"{root}/ticker_prices.parquet\")\n",
    "output_dir = f\"{root}/generated_combined_parquet\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b810f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INSPECTING INPUT FILES\n",
      "================================================================================\n",
      "\n",
      "1. TICKER_MAP (ticker_to_cusip.parquet):\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (4571, 5)\n",
      "Columns: ['name', 'cusip', 'ticker', 'trading_start_date', 'trading_end_date']\n",
      "\n",
      "First 5 rows:\n",
      "              name      cusip ticker trading_start_date trading_end_date\n",
      "0  A. Schulman Inc  808194104   SHLM         1973-01-09       2018-08-20\n",
      "1   A.H. Belo Corp  001282102    AHC               None             None\n",
      "2  A.O. Smith Corp  831865209    AOS         1983-09-30       2026-01-06\n",
      "3         AAON Inc  000360206   AAON         1991-01-03       2026-01-06\n",
      "4         AAR Corp  000361105    AIR         1980-03-17       2026-01-06\n",
      "\n",
      "Data types:\n",
      "name                  object\n",
      "cusip                 object\n",
      "ticker                object\n",
      "trading_start_date    object\n",
      "trading_end_date      object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "2. PRICES (ticker_prices.parquet):\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (158011, 4)\n",
      "Columns: ['ticker', 'date', 'price', 'period_start']\n",
      "\n",
      "First 5 rows:\n",
      "  ticker        date    price period_start\n",
      "0   SHLM  2013-07-01  24.4628   2013-04-01\n",
      "1   SHLM  2013-09-30  26.0762   2013-07-01\n",
      "2   SHLM  2013-12-31  31.4066   2013-10-01\n",
      "3   SHLM  2014-03-31  32.4822   2014-01-01\n",
      "4   SHLM  2014-06-30  34.8644   2014-04-01\n",
      "\n",
      "Data types:\n",
      "ticker           object\n",
      "date             object\n",
      "price           float64\n",
      "period_start     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "3. SAMPLE HOLDINGS FILE:\n",
      "--------------------------------------------------------------------------------\n",
      "File: holdings_filtered_new_period_start_2013-04-01.parquet\n",
      "Shape: (19659, 7)\n",
      "Columns: ['nameofissuer', 'cusip', 'sshprnamt', 'cik', 'year', 'quarter', 'period_start']\n",
      "\n",
      "First 5 rows:\n",
      "  nameofissuer      cusip  sshprnamt         cik  year  quarter period_start\n",
      "0     AAON INC  000360206       70.0  0001011659  2013        2   2013-04-01\n",
      "1     AAON INC  000360206    18598.0  0001564702  2013        2   2013-04-01\n",
      "2     AAON INC  000360206   173935.0  0001374170  2013        2   2013-04-01\n",
      "3     AAON INC  000360206     8523.0  0001121914  2013        2   2013-04-01\n",
      "4     AAON INC  000360206     3484.0  0000049205  2013        2   2013-04-01\n",
      "\n",
      "Data types:\n",
      "nameofissuer     object\n",
      "cusip            object\n",
      "sshprnamt       float64\n",
      "cik              object\n",
      "year              int64\n",
      "quarter           int64\n",
      "period_start     object\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "INSPECTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Check the structure of your files\n",
    "print(\"=\" * 80)\n",
    "print(\"INSPECTING INPUT FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check ticker_map structure\n",
    "print(\"\\n1. TICKER_MAP (ticker_to_cusip.parquet):\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    ticker_map_sample = pd.read_parquet(f\"{root}/ticker_to_cusip.parquet\")\n",
    "    print(f\"Shape: {ticker_map_sample.shape}\")\n",
    "    print(f\"Columns: {list(ticker_map_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(ticker_map_sample.head())\n",
    "    print(f\"\\nData types:\\n{ticker_map_sample.dtypes}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading ticker_map: {e}\")\n",
    "\n",
    "# 2. Check prices structure\n",
    "print(\"\\n\\n2. PRICES (ticker_prices.parquet):\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    prices_sample = pd.read_parquet(f\"{root}/ticker_prices.parquet\")\n",
    "    print(f\"Shape: {prices_sample.shape}\")\n",
    "    print(f\"Columns: {list(prices_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(prices_sample.head())\n",
    "    print(f\"\\nData types:\\n{prices_sample.dtypes}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading prices: {e}\")\n",
    "\n",
    "# 3. Check first holdings file structure\n",
    "print(\"\\n\\n3. SAMPLE HOLDINGS FILE:\")\n",
    "print(\"-\" * 80)\n",
    "holdings_pattern = os.path.join(root, \"holdings_filtered_new_period_start_*.parquet\")\n",
    "holdings_files = sorted(glob.glob(holdings_pattern))\n",
    "\n",
    "if holdings_files:\n",
    "    sample_file = holdings_files[0]\n",
    "    print(f\"File: {os.path.basename(sample_file)}\")\n",
    "    try:\n",
    "        holdings_sample = pd.read_parquet(sample_file)\n",
    "        print(f\"Shape: {holdings_sample.shape}\")\n",
    "        print(f\"Columns: {list(holdings_sample.columns)}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(holdings_sample.head())\n",
    "        print(f\"\\nData types:\\n{holdings_sample.dtypes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading holdings: {e}\")\n",
    "else:\n",
    "    print(\"No holdings files found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INSPECTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87316a29",
   "metadata": {},
   "source": [
    "##### Iterating on all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f321aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 holdings files\n",
      "Processing 2013 Q2 (period_start: 2013-04-01)...\n",
      "  ✓ Saved 19659 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2013.parquet\n",
      "Processing 2013 Q3 (period_start: 2013-07-01)...\n",
      "  ✓ Saved 34213 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2013.parquet\n",
      "Processing 2013 Q4 (period_start: 2013-10-01)...\n",
      "  ✓ Saved 1018601 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2013.parquet\n",
      "Processing 2014 Q1 (period_start: 2014-01-01)...\n",
      "  ✓ Saved 1017686 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2014.parquet\n",
      "Processing 2014 Q2 (period_start: 2014-04-01)...\n",
      "  ✓ Saved 1060398 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2014.parquet\n",
      "Processing 2014 Q3 (period_start: 2014-07-01)...\n",
      "  ✓ Saved 1049697 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2014.parquet\n",
      "Processing 2014 Q4 (period_start: 2014-10-01)...\n",
      "  ✓ Saved 1093267 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2014.parquet\n",
      "Processing 2015 Q1 (period_start: 2015-01-01)...\n",
      "  ✓ Saved 1119642 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2015.parquet\n",
      "Processing 2015 Q2 (period_start: 2015-04-01)...\n",
      "  ✓ Saved 1095826 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2015.parquet\n",
      "Processing 2015 Q3 (period_start: 2015-07-01)...\n",
      "  ✓ Saved 1081637 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2015.parquet\n",
      "Processing 2015 Q4 (period_start: 2015-10-01)...\n",
      "  ✓ Saved 1115302 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2015.parquet\n",
      "Processing 2016 Q1 (period_start: 2016-01-01)...\n",
      "  ✓ Saved 1131465 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2016.parquet\n",
      "Processing 2016 Q2 (period_start: 2016-04-01)...\n",
      "  ✓ Saved 1127160 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2016.parquet\n",
      "Processing 2016 Q3 (period_start: 2016-07-01)...\n",
      "  ✓ Saved 1117857 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2016.parquet\n",
      "Processing 2016 Q4 (period_start: 2016-10-01)...\n",
      "  ✓ Saved 1170982 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2016.parquet\n",
      "Processing 2017 Q1 (period_start: 2017-01-01)...\n",
      "  ✓ Saved 1190709 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2017.parquet\n",
      "Processing 2017 Q2 (period_start: 2017-04-01)...\n",
      "  ✓ Saved 1169387 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2017.parquet\n",
      "Processing 2017 Q3 (period_start: 2017-07-01)...\n",
      "  ✓ Saved 1131807 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2017.parquet\n",
      "Processing 2017 Q4 (period_start: 2017-10-01)...\n",
      "  ✓ Saved 1252888 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2017.parquet\n",
      "Processing 2018 Q1 (period_start: 2018-01-01)...\n",
      "  ✓ Saved 1294867 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2018.parquet\n",
      "Processing 2018 Q2 (period_start: 2018-04-01)...\n",
      "  ✓ Saved 1299201 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2018.parquet\n",
      "Processing 2018 Q3 (period_start: 2018-07-01)...\n",
      "  ✓ Saved 1356314 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2018.parquet\n",
      "Processing 2018 Q4 (period_start: 2018-10-01)...\n",
      "  ✓ Saved 1275091 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2018.parquet\n",
      "Processing 2019 Q1 (period_start: 2019-01-01)...\n",
      "  ✓ Saved 1337369 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2019.parquet\n",
      "Processing 2019 Q2 (period_start: 2019-04-01)...\n",
      "  ✓ Saved 1344696 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2019.parquet\n",
      "Processing 2019 Q3 (period_start: 2019-07-01)...\n",
      "  ✓ Saved 1308113 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2019.parquet\n",
      "Processing 2019 Q4 (period_start: 2019-10-01)...\n",
      "  ✓ Saved 1353353 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2019.parquet\n",
      "Processing 2020 Q1 (period_start: 2020-01-01)...\n",
      "  ✓ Saved 1304800 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2020.parquet\n",
      "Processing 2020 Q2 (period_start: 2020-04-01)...\n",
      "  ✓ Saved 1307037 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2020.parquet\n",
      "Processing 2020 Q3 (period_start: 2020-07-01)...\n",
      "  ✓ Saved 1309991 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2020.parquet\n",
      "Processing 2020 Q4 (period_start: 2020-10-01)...\n",
      "  ✓ Saved 1456047 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2020.parquet\n",
      "Processing 2021 Q1 (period_start: 2021-01-01)...\n",
      "  ✓ Saved 1425771 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2021.parquet\n",
      "Processing 2021 Q2 (period_start: 2021-04-01)...\n",
      "  ✓ Saved 1470761 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2021.parquet\n",
      "Processing 2021 Q3 (period_start: 2021-07-01)...\n",
      "  ✓ Saved 1484007 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2021.parquet\n",
      "Processing 2021 Q4 (period_start: 2021-10-01)...\n",
      "  ✓ Saved 1564186 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2021.parquet\n",
      "Processing 2022 Q1 (period_start: 2022-01-01)...\n",
      "  ✓ Saved 1557935 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2022.parquet\n",
      "Processing 2022 Q2 (period_start: 2022-04-01)...\n",
      "  ✓ Saved 1561596 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2022.parquet\n",
      "Processing 2022 Q3 (period_start: 2022-07-01)...\n",
      "  ✓ Saved 1514464 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2022.parquet\n",
      "Processing 2022 Q4 (period_start: 2022-10-01)...\n",
      "  ✓ Saved 1612235 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2022.parquet\n",
      "Processing 2023 Q1 (period_start: 2023-01-01)...\n",
      "  ✓ Saved 1615015 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2023.parquet\n",
      "Processing 2023 Q2 (period_start: 2023-04-01)...\n",
      "  ✓ Saved 1586359 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2023.parquet\n",
      "Processing 2023 Q3 (period_start: 2023-07-01)...\n",
      "  ✓ Saved 1658916 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2023.parquet\n",
      "Processing 2023 Q4 (period_start: 2023-10-01)...\n",
      "  ✓ Saved 1714038 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2023.parquet\n",
      "Processing 2024 Q1 (period_start: 2024-01-01)...\n",
      "  ✓ Saved 1736830 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2024.parquet\n",
      "Processing 2024 Q2 (period_start: 2024-04-01)...\n",
      "  ✓ Saved 1771738 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2024.parquet\n",
      "Processing 2024 Q3 (period_start: 2024-07-01)...\n",
      "  ✓ Saved 1764953 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2024.parquet\n",
      "Processing 2024 Q4 (period_start: 2024-10-01)...\n",
      "  ✓ Saved 115914 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2024.parquet\n",
      "Processing 2025 Q1 (period_start: 2025-01-01)...\n",
      "  ✓ Saved 1878074 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2025.parquet\n",
      "Processing 2025 Q2 (period_start: 2025-04-01)...\n",
      "  ✓ Saved 1822141 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2025.parquet\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Get all holdings parquet files sorted by date\n",
    "ticker_map[\"cusip\"] = ticker_map[\"cusip\"].astype(str)\n",
    "prices[\"period_start\"] = pd.to_datetime(prices[\"period_start\"])\n",
    "holdings_pattern = os.path.join(root, \"holdings_filtered_new_period_start_*.parquet\")\n",
    "holdings_files = sorted(glob.glob(holdings_pattern))\n",
    "\n",
    "print(f\"Found {len(holdings_files)} holdings files\")\n",
    "\n",
    "# Extract year and quarter from each file\n",
    "def extract_date_info(filepath):\n",
    "    \"\"\"Extract period_start date from filename\"\"\"\n",
    "    match = re.search(r'period_start_(\\d{4}-\\d{2}-\\d{2})', filepath)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        date_obj = pd.to_datetime(date_str)\n",
    "        year = date_obj.year\n",
    "        quarter = (date_obj.month - 1) // 3 + 1\n",
    "        return date_obj, year, quarter\n",
    "    return None, None, None\n",
    "\n",
    "# Process all files\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in holdings_files:\n",
    "    period_date, year, quarter = extract_date_info(file_path)\n",
    "    \n",
    "    if period_date is None:\n",
    "        print(f\"Skipping file with unrecognized format: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing {year} Q{quarter} (period_start: {period_date.date()})...\")\n",
    "    \n",
    "    try:\n",
    "        # Read holdings data\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Rename to required format\n",
    "        df = df.rename(columns={\n",
    "            \"cik\": \"CIK\",\n",
    "            \"cusip\": \"CUSIP\",\n",
    "            \"sshprnamt\": \"SSHPRNAMT\",\n",
    "            \"period_start\": \"PERIOD_DATE\"\n",
    "        })\n",
    "        \n",
    "        # Type normalization\n",
    "        df[\"CIK\"] = df[\"CIK\"].astype(str)\n",
    "        df[\"CUSIP\"] = df[\"CUSIP\"].astype(str)\n",
    "        df[\"PERIOD_DATE\"] = pd.to_datetime(df[\"PERIOD_DATE\"])\n",
    "        \n",
    "        # Join: CUSIP → ticker\n",
    "        df = df.merge(\n",
    "            ticker_map[[\"cusip\", \"ticker\"]],\n",
    "            left_on=\"CUSIP\",\n",
    "            right_on=\"cusip\",\n",
    "            how=\"left\"\n",
    "        ).drop(columns=[\"cusip\"])\n",
    "        \n",
    "        # Join: ticker + period → price\n",
    "        df = df.merge(\n",
    "            prices[[\"ticker\", \"period_start\", \"price\"]],\n",
    "            left_on=[\"ticker\", \"PERIOD_DATE\"],\n",
    "            right_on=[\"ticker\", \"period_start\"],\n",
    "            how=\"left\"\n",
    "        ).drop(columns=[\"period_start\"])\n",
    "        \n",
    "        # Compute VALUE\n",
    "        df[\"VALUE\"] = df[\"SSHPRNAMT\"] * df[\"price\"]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df[\"YEAR\"] = year\n",
    "        df[\"QUARTER\"] = f\"Q{quarter}_{year}\"\n",
    "        \n",
    "        # Final column order\n",
    "        df = df[[\"CIK\", \"CUSIP\", \"VALUE\", \"SSHPRNAMT\", \"PERIOD_DATE\", \"YEAR\", \"QUARTER\"]]\n",
    "        \n",
    "        # Save individual parquet file\n",
    "        out_path = os.path.join(\n",
    "            output_dir,\n",
    "            f\"holdings_processed_Q{quarter}_{year}.parquet\"\n",
    "        )\n",
    "        df.to_parquet(out_path, index=False)\n",
    "        # print(df.head(10))\n",
    "        print(f\"  ✓ Saved {len(df)} records to {out_path}\")\n",
    "        \n",
    "        # Add to list for combined file\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {file_path}: {e}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd713ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfs = []\n",
    "# for i, file in enumerate(file_paths):\n",
    "#     df = pd.read_csv(file)\n",
    "#     df['QUARTER'] = f'Q{i+1}_2018'\n",
    "#     dfs.append(df)\n",
    "# data = pd.concat(dfs, ignore_index=True)\n",
    "# data = data[['CIK', 'CUSIP', 'VALUE', 'SSHPRNAMT', 'PERIOD_DATE', 'QUARTER']]\n",
    "# data['PERIOD_DATE'] = pd.to_datetime(data['PERIOD_DATE'])\n",
    "# data = data.dropna(subset=['CIK', 'CUSIP', 'VALUE'])\n",
    "# data['CIK'] = data['CIK'].astype(str)\n",
    "# data['CUSIP'] = data['CUSIP'].astype(str)\n",
    "# data = data.sort_values(by='PERIOD_DATE')\n",
    "# print(f'Total records after cleaning: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78fae4",
   "metadata": {},
   "source": [
    "## 3. Graph Construction\n",
    "Build bipartite and projected graphs for funds and stocks using sliding window approach.\n",
    "\n",
    "**Training Strategy:**\n",
    "- Initial training: 2015-2019 (all quarters)\n",
    "- Sliding window testing: 2020 Q1 through 2024 Q4\n",
    "- Each quarter uses all previous data for training\n",
    "- 2025 data excluded from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ecbda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_and_features_up_to(df, max_year, max_quarter):\n",
    "    \"\"\"\n",
    "    Build graph and features up to specified year and quarter.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined dataframe with all holdings data\n",
    "        max_year: Maximum year to include (inclusive)\n",
    "        max_quarter: Maximum quarter to include for max_year (1-4)\n",
    "    \n",
    "    Returns:\n",
    "        Bipartite graph, fund-fund graph, fund features, filtered data, fund list, stock list\n",
    "    \"\"\"\n",
    "    # Filter data up to specified year/quarter\n",
    "    df_up_to = df[\n",
    "        (df['YEAR'] < max_year) | \n",
    "        ((df['YEAR'] == max_year) & (df['QUARTER'].str.extract(r'Q(\\d+)')[0].astype(int) <= max_quarter))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(df_up_to) == 0:\n",
    "        raise ValueError(f\"No data available up to {max_year} Q{max_quarter}\")\n",
    "    \n",
    "    funds_up_to = df_up_to['CIK'].unique()\n",
    "    stocks_up_to = df_up_to['CUSIP'].unique()\n",
    "    \n",
    "    # Build bipartite graph\n",
    "    G_bip = nx.Graph()\n",
    "    G_bip.add_nodes_from(funds_up_to, bipartite=0)\n",
    "    G_bip.add_nodes_from(stocks_up_to, bipartite=1)\n",
    "    \n",
    "    edges = [(row.CIK, row.CUSIP, \n",
    "            {'value': row.VALUE, 'amount': row.SSHPRNAMT, 'time': row.PERIOD_DATE})\n",
    "            for row in df_up_to.itertuples(index=False)]\n",
    "    G_bip.add_edges_from(edges)\n",
    "        \n",
    "    # Fund-Fund projection with weights\n",
    "    G_fund = bipartite.weighted_projected_graph(G_bip, funds_up_to)\n",
    "    \n",
    "    # Convert to directed based on time\n",
    "    G_fund_directed = nx.DiGraph()\n",
    "    for u, v, data_dict in G_fund.edges(data=True):\n",
    "        shared = set(G_bip.neighbors(u)) & set(G_bip.neighbors(v))\n",
    "        if not shared:\n",
    "            continue\n",
    "        \n",
    "        times_u = [G_bip.edges[u,s]['time'] for s in shared]\n",
    "        times_v = [G_bip.edges[v,s]['time'] for s in shared]\n",
    "        avg_u = np.mean([t.timestamp() for t in times_u])\n",
    "        avg_v = np.mean([t.timestamp() for t in times_v])\n",
    "        \n",
    "        weight = data_dict.get('weight', 1)\n",
    "        \n",
    "        if avg_u < avg_v:\n",
    "            G_fund_directed.add_edge(u, v, weight=weight)\n",
    "        else:\n",
    "            G_fund_directed.add_edge(v, u, weight=weight)\n",
    "    \n",
    "    G_fund = G_fund_directed\n",
    "    \n",
    "    # Topological features\n",
    "    degree_cent = degree_centrality(G_fund)\n",
    "    pagerank_cent = nx.pagerank(G_fund)\n",
    "    hubs, authorities = hits(G_fund)\n",
    "    \n",
    "    # Closeness centrality (on largest connected component)\n",
    "    if G_fund.number_of_nodes() > 0:\n",
    "        largest_cc = max(nx.connected_components(G_fund.to_undirected()), key=len)\n",
    "        closeness_cent = closeness_centrality(G_fund.to_undirected().subgraph(largest_cc))\n",
    "    else:\n",
    "        closeness_cent = {}\n",
    "    \n",
    "    fund_features = pd.DataFrame({\n",
    "        'fund': list(G_fund.nodes()),\n",
    "        'degree': [degree_cent.get(n, 0) for n in G_fund.nodes()],\n",
    "        'pagerank': [pagerank_cent.get(n, 0) for n in G_fund.nodes()],\n",
    "        'hub': [hubs.get(n, 0) for n in G_fund.nodes()],\n",
    "        'authority': [authorities.get(n, 0) for n in G_fund.nodes()],\n",
    "        'closeness': [closeness_cent.get(n, 0) for n in G_fund.nodes()]\n",
    "    }).set_index('fund')\n",
    "    \n",
    "    # Community detection (Leiden)\n",
    "    if G_fund.number_of_nodes() > 0:\n",
    "        vertex_names = list(G_fund.nodes())\n",
    "        vertex_to_idx = {v: i for i, v in enumerate(vertex_names)}\n",
    "        edge_list = [(vertex_to_idx[u], vertex_to_idx[v]) \n",
    "                    for u, v in G_fund.to_undirected().edges()]\n",
    "        \n",
    "        ig_G = ig.Graph(n=len(vertex_names), edges=edge_list)\n",
    "        ig_G.vs['_nx_name'] = vertex_names\n",
    "        partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "        communities = {ig_G.vs[i]['_nx_name']: p for p, cl in enumerate(partition) for i in cl}\n",
    "        fund_features['community'] = fund_features.index.map(communities).fillna(-1)\n",
    "    else:\n",
    "        fund_features['community'] = -1\n",
    "    \n",
    "    return G_bip, G_fund, fund_features, df_up_to, funds_up_to, stocks_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6970f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined holdings data...\n",
      "Found 49 processed files\n",
      "Total records loaded: 59,099,780\n",
      "Date range: 2013-04-01 00:00:00 to 2024-10-01 00:00:00\n",
      "Years covered: [np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024)]\n",
      "Unique funds: 10588\n",
      "Unique stocks: 4554\n",
      "\n",
      "Training period: 2015-2019\n",
      "Testing period: 2020-2024 (sliding window by quarter)\n"
     ]
    }
   ],
   "source": [
    "# Load combined data from generated parquet files (exclude 2025)\n",
    "print(\"Loading combined holdings data...\")\n",
    "combined_files = sorted([f for f in os.listdir(output_dir) if f.startswith('holdings_processed_') and f.endswith('.parquet')])\n",
    "print(f\"Found {len(combined_files)} processed files\")\n",
    "\n",
    "# Load and concatenate all data\n",
    "all_data = []\n",
    "for file in combined_files:\n",
    "    df_temp = pd.read_parquet(os.path.join(output_dir, file))\n",
    "    all_data.append(df_temp)\n",
    "\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "data['PERIOD_DATE'] = pd.to_datetime(data['PERIOD_DATE'])\n",
    "\n",
    "# Exclude 2025 data\n",
    "data = data[data['YEAR'] < 2025].copy()\n",
    "\n",
    "print(f\"Total records loaded: {len(data):,}\")\n",
    "print(f\"Date range: {data['PERIOD_DATE'].min()} to {data['PERIOD_DATE'].max()}\")\n",
    "print(f\"Years covered: {sorted(data['YEAR'].unique())}\")\n",
    "print(f\"Unique funds: {data['CIK'].nunique()}\")\n",
    "print(f\"Unique stocks: {data['CUSIP'].nunique()}\")\n",
    "\n",
    "# Define training and test periods\n",
    "INITIAL_TRAIN_START_YEAR = 2015\n",
    "INITIAL_TRAIN_END_YEAR = 2019\n",
    "TEST_START_YEAR = 2020\n",
    "TEST_END_YEAR = 2024\n",
    "\n",
    "print(f\"\\nTraining period: {INITIAL_TRAIN_START_YEAR}-{INITIAL_TRAIN_END_YEAR}\")\n",
    "print(f\"Testing period: {TEST_START_YEAR}-{TEST_END_YEAR} (sliding window by quarter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe222b13",
   "metadata": {},
   "source": [
    "## 4. Initial Training Phase (2015-2019)\n",
    "Build initial model using all data from 2015-2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7cdaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building initial training graph (2015-2019)...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Build initial training graph using 2015-2019 data\n",
    "print(\"Building initial training graph (2015-2019)...\")\n",
    "G_bip_train, G_fund_train, fund_features_train, df_train, funds_train, stocks_train = \\\n",
    "    build_graph_and_features_up_to(data, INITIAL_TRAIN_END_YEAR, 4)\n",
    "\n",
    "fund_idx_train = {f: i for i, f in enumerate(funds_train)}\n",
    "stock_idx_train = {s: i for i, s in enumerate(stocks_train)}\n",
    "\n",
    "print(f\"Initial training graph statistics:\")\n",
    "print(f\"  Bipartite edges: {G_bip_train.number_of_edges():,}\")\n",
    "print(f\"  Fund-fund edges: {G_fund_train.number_of_edges():,}\")\n",
    "print(f\"  Number of funds: {len(funds_train):,}\")\n",
    "print(f\"  Number of stocks: {len(stocks_train):,}\")\n",
    "print(f\"  Date range: {df_train['PERIOD_DATE'].min()} to {df_train['PERIOD_DATE'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4ecf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f52390",
   "metadata": {},
   "source": [
    "## 5. GraphSAGE Embedding Training (Initial Model)\n",
    "Train GraphSAGE on 2015-2019 data to generate initial node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features initialized on cpu\n",
      "Edge index initialized on cpu\n",
      "Model initialized on cpu\n",
      "Starting training on cpu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3611\n",
      "Epoch 10, Loss: 0.0000\n",
      "Early stopping at epoch 11\n",
      "GraphSAGE training completed on cpu.\n",
      "dynamic_emb_train shape: (9, 8)\n",
      "stock_emb_train shape: (1891, 8)\n"
     ]
    }
   ],
   "source": [
    "# Node features initialization - ensure GPU usage\n",
    "num_nodes = len(funds_train) + len(stocks_train)\n",
    "\n",
    "# Initialize on GPU directly\n",
    "x = torch.randn(num_nodes, 16, device=device)\n",
    "print(f\"Node features initialized on {device} with shape {x.shape}\")\n",
    "\n",
    "# Build homogeneous edge index for initial training\n",
    "edge_index_homo = []\n",
    "for u, v in G_bip_train.edges():\n",
    "    u_idx = fund_idx_train.get(u, len(funds_train) + stock_idx_train.get(u, -1))\n",
    "    v_idx = fund_idx_train.get(v, len(funds_train) + stock_idx_train.get(v, -1))\n",
    "    if u_idx < num_nodes and v_idx < num_nodes:\n",
    "        edge_index_homo.append([u_idx, v_idx])\n",
    "        edge_index_homo.append([v_idx, u_idx])\n",
    "\n",
    "# Create edge index directly on GPU\n",
    "edge_index_homo = torch.tensor(edge_index_homo, dtype=torch.long, device=device).t().contiguous()\n",
    "print(f\"Edge index initialized on {device} with {edge_index_homo.shape[1]:,} edges\")\n",
    "\n",
    "# Define GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model on GPU\n",
    "model = GraphSAGE(16, 32, 8).to(device)\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Model parameters on GPU: {next(model.parameters()).is_cuda}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train GraphSAGE with early stopping\n",
    "model.train()\n",
    "prev_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"\\nStarting initial GraphSAGE training on {device}...\")\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index_homo)\n",
    "    pos_score = (out[edge_index_homo[0]] * out[edge_index_homo[1]]).sum(dim=1).sigmoid()\n",
    "    loss = -torch.log(pos_score + 1e-15).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if abs(prev_loss - loss.item()) < 1e-6:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    else:\n",
    "        no_improve = 0\n",
    "    prev_loss = loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "# Extract initial embeddings (keep computation on GPU, only move result to CPU)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb = model(x, edge_index_homo).cpu().numpy()\n",
    "\n",
    "dynamic_emb_train = emb[:len(funds_train)]\n",
    "stock_emb_train = emb[len(funds_train):]\n",
    "\n",
    "print(f\"\\nInitial GraphSAGE training completed on {device}.\")\n",
    "print(f\"Fund embeddings shape: {dynamic_emb_train.shape}\")\n",
    "print(f\"Stock embeddings shape: {stock_emb_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c5bec",
   "metadata": {},
   "source": [
    "## 6. Save Training Artifacts\n",
    "Save embeddings, features, and model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b16ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cache trained artifacts (run ONCE) ───────────────────────────────────────\n",
    "GRAPH_READY = True\n",
    "_cached_graph = G_bip_train\n",
    "_cached_embeddings = stock_emb_train\n",
    "_cached_model = model.cpu()  # Move model to CPU for caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b398ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to artifacts/ directory.\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings, model, and graph to files for later use\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a directory for artifacts if it doesn't exist\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "# Save stock embeddings\n",
    "np.save('artifacts/stock_emb_train.npy', stock_emb_train)\n",
    "\n",
    "# Save dynamic fund embeddings\n",
    "np.save('artifacts/dynamic_emb_train.npy', dynamic_emb_train)\n",
    "\n",
    "# Save fund features\n",
    "fund_features_train.to_pickle('artifacts/fund_features_train.pkl')\n",
    "\n",
    "# Save LightGBM model (bst) if it exists\n",
    "if 'bst' in globals():\n",
    "    joblib.dump(bst, 'artifacts/lightgbm_bst.pkl')\n",
    "\n",
    "# Save funds and stocks lists\n",
    "with open('artifacts/funds_train.pkl', 'wb') as f:\n",
    "    pickle.dump(funds_train, f)\n",
    "with open('artifacts/stocks_train.pkl', 'wb') as f:\n",
    "    pickle.dump(stocks_train, f)\n",
    "\n",
    "print('Artifacts saved to artifacts/ directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e803f53",
   "metadata": {},
   "source": [
    "## 6. Sliding Window Evaluation (2020-2024)\n",
    "Evaluate model performance using sliding window approach across all quarters from 2020-2024.\n",
    "\n",
    "For each quarter:\n",
    "1. Use all data up to previous quarter for training\n",
    "2. Predict holdings for current quarter\n",
    "3. Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3803 positive and 357 negative test samples\n",
      "Test set positive ratio: 0.914 (1=positive, 0=negative)\n"
     ]
    }
   ],
   "source": [
    "# Sliding window evaluation on 2020-2024\n",
    "def evaluate_quarter(data, train_year, train_quarter, test_year, test_quarter, \n",
    "                     model, fund_features_base, funds_base, stocks_base, stock_emb_base):\n",
    "    \"\"\"\n",
    "    Evaluate model on a specific test quarter using training data up to previous quarter.\n",
    "    \"\"\"\n",
    "    # Build training graph up to (and including) train_quarter\n",
    "    G_bip_win, G_fund_win, fund_features_win, df_win, funds_win, stocks_win = \\\n",
    "        build_graph_and_features_up_to(data, train_year, train_quarter)\n",
    "    \n",
    "    fund_idx_win = {f: i for i, f in enumerate(funds_win)}\n",
    "    stock_idx_win = {s: i for i, s in enumerate(stocks_win)}\n",
    "    \n",
    "    # Update embeddings for this window (fast fine-tuning) - ensure GPU usage\n",
    "    num_nodes_win = len(funds_win) + len(stocks_win)\n",
    "    x_win = torch.randn(num_nodes_win, 16, device=device)\n",
    "    \n",
    "    edge_index_win = []\n",
    "    for u, v in G_bip_win.edges():\n",
    "        u_idx = fund_idx_win.get(u, len(funds_win) + stock_idx_win.get(u, -1))\n",
    "        v_idx = fund_idx_win.get(v, len(funds_win) + stock_idx_win.get(v, -1))\n",
    "        if u_idx < num_nodes_win and v_idx < num_nodes_win:\n",
    "            edge_index_win.append([u_idx, v_idx])\n",
    "            edge_index_win.append([v_idx, u_idx])\n",
    "    \n",
    "    # Create edge index directly on GPU\n",
    "    edge_index_win = torch.tensor(edge_index_win, dtype=torch.long, device=device).t().contiguous()\n",
    "    \n",
    "    # Fine-tune for 10 epochs on GPU\n",
    "    model.train()\n",
    "    optimizer_win = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(10):\n",
    "        optimizer_win.zero_grad()\n",
    "        out = model(x_win, edge_index_win)\n",
    "        pos_score = (out[edge_index_win[0]] * out[edge_index_win[1]]).sum(dim=1).sigmoid()\n",
    "        loss = -torch.log(pos_score + 1e-15).mean()\n",
    "        loss.backward()\n",
    "        optimizer_win.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb_win = model(x_win, edge_index_win).cpu().numpy()\n",
    "    \n",
    "    dynamic_emb_win = emb_win[:len(funds_win)]\n",
    "    stock_emb_win = emb_win[len(funds_win):]\n",
    "    \n",
    "    # Get test quarter data\n",
    "    test_data = data[\n",
    "        (data['YEAR'] == test_year) & \n",
    "        (data['QUARTER'].str.extract(r'Q(\\d+)')[0].astype(int) == test_quarter)\n",
    "    ]\n",
    "    \n",
    "    funds_test = set(test_data['CIK'].unique()) & set(funds_win)\n",
    "    stocks_test = set(test_data['CUSIP'].unique()) & set(stocks_win)\n",
    "    \n",
    "    # Create positive edges from test data\n",
    "    pos_edges = []\n",
    "    for row in test_data.itertuples(index=False):\n",
    "        if row.CIK in funds_test and row.CUSIP in stocks_test:\n",
    "            pos_edges.append((fund_idx_win[row.CIK], stock_idx_win[row.CUSIP], 1))\n",
    "    \n",
    "    # Negative sampling\n",
    "    stock_sim_win = cosine_similarity(stock_emb_win)\n",
    "    fund_to_connected = {\n",
    "        f_idx: [stock_idx_win[s] for s in G_bip_win.neighbors(fund) \n",
    "                if s in stocks_test and s in stock_idx_win]\n",
    "        for f_idx, fund in enumerate(funds_win) if fund in funds_test\n",
    "    }\n",
    "    \n",
    "    neg_edges = []\n",
    "    for f_idx in range(len(funds_win)):\n",
    "        if funds_win[f_idx] not in funds_test:\n",
    "            continue\n",
    "        connected = fund_to_connected.get(f_idx, [])\n",
    "        if not connected:\n",
    "            continue\n",
    "        sim_scores = stock_sim_win[connected].mean(axis=0)\n",
    "        hard_negs = np.argsort(-sim_scores)[len(connected):len(connected)+50]\n",
    "        neg_edges.extend([\n",
    "            (f_idx, neg_idx, 0) for neg_idx in hard_negs \n",
    "            if neg_idx not in connected and neg_idx < len(stocks_win) \n",
    "            and stocks_win[neg_idx] in stocks_test\n",
    "        ])\n",
    "    \n",
    "    neg_edges = neg_edges[:len(pos_edges)]\n",
    "    \n",
    "    # Build features\n",
    "    link_data = []\n",
    "    for f_i, s_i, label in pos_edges + neg_edges:\n",
    "        fund_id = funds_win[f_i]\n",
    "        feat = np.concatenate([\n",
    "            dynamic_emb_win[f_i],\n",
    "            stock_emb_win[s_i],\n",
    "            fund_features_win.loc[fund_id, ['degree', 'pagerank', 'hub', 'authority', 'closeness', 'community']].values\n",
    "        ])\n",
    "        link_data.append((feat, label))\n",
    "    \n",
    "    X_test = np.array([d[0] for d in link_data])\n",
    "    y_test = np.array([d[1] for d in link_data])\n",
    "    \n",
    "    return X_test, y_test, len(pos_edges), len(neg_edges)\n",
    "\n",
    "# Run sliding window evaluation\n",
    "print(f\"\\nStarting sliding window evaluation (2020-2024) on {device}...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for year in range(TEST_START_YEAR, TEST_END_YEAR + 1):\n",
    "    for quarter in range(1, 5):\n",
    "        # Training window: all data up to previous quarter\n",
    "        if quarter == 1:\n",
    "            train_year, train_quarter = year - 1, 4\n",
    "        else:\n",
    "            train_year, train_quarter = year, quarter - 1\n",
    "        \n",
    "        # Skip if training period is before our data\n",
    "        if train_year < INITIAL_TRAIN_START_YEAR:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nEvaluating {year} Q{quarter} (trained on data up to {train_year} Q{train_quarter})...\")\n",
    "        print(f\"GPU Memory before evaluation: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "        \n",
    "        try:\n",
    "            X_test, y_test, n_pos, n_neg = evaluate_quarter(\n",
    "                data, train_year, train_quarter, year, quarter,\n",
    "                model, fund_features_train, funds_train, stocks_train, stock_emb_train\n",
    "            )\n",
    "            \n",
    "            print(f\"  Test samples: {len(y_test):,} ({n_pos:,} positive, {n_neg:,} negative)\")\n",
    "            \n",
    "            # Train LightGBM on this window's data and evaluate\n",
    "            params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'learning_rate': 0.01,\n",
    "                'num_leaves': 31,\n",
    "                'verbose': -1,\n",
    "                'device': 'gpu',  # Use GPU for LightGBM if available\n",
    "                'gpu_platform_id': 0,\n",
    "                'gpu_device_id': 0\n",
    "            }\n",
    "            \n",
    "            # For simplicity, use initial training data for LightGBM\n",
    "            train_data_lgb = lgb.Dataset(X_test[:len(X_test)//2], label=y_test[:len(y_test)//2])\n",
    "            bst_win = lgb.train(params, train_data_lgb, num_boost_round=50)\n",
    "            \n",
    "            y_pred = bst_win.predict(X_test[len(X_test)//2:])\n",
    "            y_true = y_test[len(X_test)//2:]\n",
    "            \n",
    "            auc = roc_auc_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, (y_pred > 0.5).astype(int))\n",
    "            \n",
    "            results.append({\n",
    "                'year': year,\n",
    "                'quarter': quarter,\n",
    "                'auc': auc,\n",
    "                'precision': precision,\n",
    "                'n_samples': len(y_test)\n",
    "            })\n",
    "            \n",
    "            print(f\"  AUC: {auc:.4f}, Precision: {precision:.4f}\")\n",
    "            print(f\"  GPU Memory after evaluation: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "            \n",
    "            # Clear GPU cache to prevent memory buildup\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "# Summary statistics\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SLIDING WINDOW EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Average AUC: {results_df['auc'].mean():.4f} (±{results_df['auc'].std():.4f})\")\n",
    "    print(f\"Average Precision: {results_df['precision'].mean():.4f} (±{results_df['precision'].std():.4f})\")\n",
    "    print(f\"\\nResults by year:\")\n",
    "    print(results_df.groupby('year')[['auc', 'precision']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed5dca",
   "metadata": {},
   "source": [
    "## 9. Prediction Function\n",
    "Function to predict top stocks for a given fund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_portfolio(fund_id):\n",
    "    \"\"\"\n",
    "    Efficiently predict top stocks for a given fund_id using precomputed artifacts.\n",
    "    Assumes all heavy computations (embeddings, features, model) are preloaded and cached.\n",
    "    \"\"\"\n",
    "    global bst, dynamic_emb_train, stock_emb_train, fund_features_train, funds_train, stocks_train\n",
    "    if 'bst' not in globals() or 'dynamic_emb_train' not in globals() or 'stock_emb_train' not in globals():\n",
    "        raise RuntimeError(\"Artifacts not loaded. Please run the training cells or load artifacts.\")\n",
    "    fund_idx = {f: i for i, f in enumerate(funds_train)}\n",
    "    stock_list = stocks_train\n",
    "    if fund_id not in fund_idx:\n",
    "        print(f\"Fund with CIK {fund_id} not found in the data.\")\n",
    "        return []\n",
    "    f_idx = fund_idx[fund_id]\n",
    "    fund_id_str = funds_train[f_idx]\n",
    "    fund_emb_repeat = np.tile(dynamic_emb_train[f_idx], (len(stock_list), 1))\n",
    "    fund_topo_repeat = np.tile(fund_features_train.loc[fund_id_str, ['degree', 'pagerank', 'hub', 'authority', 'closeness', 'community']].values, (len(stock_list), 1))\n",
    "    feats = np.concatenate([fund_emb_repeat, stock_emb_train, fund_topo_repeat], axis=1)\n",
    "    preds = bst.predict(feats)\n",
    "    stock_preds = list(zip(stock_list, preds))\n",
    "    top_stocks = sorted(stock_preds, key=lambda x: x[1], reverse=True)[:5]\n",
    "    return top_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded successfully. You can now run predictions without retraining.\n"
     ]
    }
   ],
   "source": [
    "# Load all artifacts (embeddings, features, model) for fast prediction\n",
    "# (imports moved to the first cell)\n",
    "\n",
    "artifacts_path = 'artifacts'\n",
    "\n",
    "def load_artifacts():\n",
    "    global stock_emb_train, dynamic_emb_train, fund_features_train, bst, funds_train, stocks_train\n",
    "    if os.path.exists(artifacts_path):\n",
    "        try:\n",
    "            stock_emb_train = np.load(os.path.join(artifacts_path, 'stock_emb_train.npy'))\n",
    "            dynamic_emb_train = np.load(os.path.join(artifacts_path, 'dynamic_emb_train.npy'))\n",
    "            fund_features_train = pd.read_pickle(os.path.join(artifacts_path, 'fund_features_train.pkl'))\n",
    "            bst = joblib.load(os.path.join(artifacts_path, 'lightgbm_bst.pkl'))\n",
    "            with open(os.path.join(artifacts_path, 'funds_train.pkl'), 'rb') as f:\n",
    "                funds_train = pickle.load(f)\n",
    "            with open(os.path.join(artifacts_path, 'stocks_train.pkl'), 'rb') as f:\n",
    "                stocks_train = pickle.load(f)\n",
    "            print('Artifacts loaded successfully. You can now run predictions without retraining.')\n",
    "        except Exception as e:\n",
    "            print('Failed to load artifacts:', e)\n",
    "    else:\n",
    "        print('Artifacts directory not found. Please run the training cells first.')\n",
    "\n",
    "# Load artifacts at notebook startup for fast prediction\n",
    "load_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919d110",
   "metadata": {},
   "source": [
    "## 10. Predict for a Specific Fund\n",
    "Example: Predict for a random out-of-sample fund."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd2e45",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> - To make an unbiased next-quarter prediction, you must use only funds that were seen in training (Q1-Q2) and predict their holdings in Q4.\n",
    "> - Predicting for funds that were not seen in training is not possible (no embeddings/features for them).\n",
    "> - Predicting for funds using their Q4 data in training or feature construction would cause data leakage and bias.\n",
    "> - The code below ensures you only predict for eligible funds, with no leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b704e9e",
   "metadata": {},
   "source": [
    "## 10.1. List Eligible Funds for Next-Quarter Prediction\n",
    "Print all funds that were seen in training (Q1-Q2) and also appear in Q4. These are the only funds for which you can make an unbiased next-quarter prediction (no data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible funds for Q4 prediction: 9\n",
      "Sample of eligible funds (first 20):\n",
      "['1021249', '1325091', '1345929', '1424116', '1535839', '1576102', '1623678', '1694461', '1697457']\n"
     ]
    }
   ],
   "source": [
    "# List funds eligible for next-quarter (Q4) prediction: must be seen in train (Q1-Q2) and appear in Q4\n",
    "if 'funds_train' in globals() and 'funds_q4' in globals():\n",
    "    eligible_funds = sorted(list(set(funds_train) & set(funds_q4)))\n",
    "    print(f\"Number of eligible funds for Q4 prediction: {len(eligible_funds)}\")\n",
    "    print(\"Sample of eligible funds (first 20):\")\n",
    "    print(eligible_funds[:20])\n",
    "    # You can select any fund from this list for next-quarter prediction (no leakage, no bias)\n",
    "else:\n",
    "    print('Required fund lists not found. Please ensure all previous cells have been run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3d8e2",
   "metadata": {},
   "source": [
    "## 10.1. Random/Specific Fund Next-Quarter Prediction\n",
    "To choode specific FUND from the list, update the var *fund_id_to_predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f391ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected eligible fund for Q4 prediction: 1535839\n",
      "Top recommended stocks for Q4:\n",
      "  880349105: 0.9561\n",
      "  67555N206: 0.9561\n",
      "  67420T206: 0.9561\n",
      "  85570W100: 0.9561\n",
      "  983134107: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Example: Predict for a specific or random eligible fund (seen in train, predict Q4)\n",
    "fund_id_to_predict = '1535839'  # Set to a specific CIK string to choose a fund, or leave as None for random\n",
    "fund_list_for_prediction = eligible_funds if 'eligible_funds' in globals() and len(eligible_funds) > 0 else []\n",
    "if len(fund_list_for_prediction) > 0:\n",
    "    if fund_id_to_predict is not None and str(fund_id_to_predict) in fund_list_for_prediction:\n",
    "        selected_fund = str(fund_id_to_predict)\n",
    "        print(f'Selected eligible fund for Q4 prediction: {selected_fund}')\n",
    "    else:\n",
    "        selected_fund = random.choice(fund_list_for_prediction)\n",
    "        print(f'Randomly selected eligible fund for Q4 prediction: {selected_fund}')\n",
    "    top_stocks = predict_portfolio(selected_fund)\n",
    "    print('Top recommended stocks for Q4:')\n",
    "    for stock, score in top_stocks:\n",
    "        print(f'  {stock}: {score:.4f}')\n",
    "else:\n",
    "    print('No eligible funds available for prediction. Please check your data and artifacts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0516829",
   "metadata": {},
   "source": [
    "## 7. Final Model Training (2015-2024)\n",
    "Train final model on all available data (2015-2024) for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all 2015-2024 data for production deployment\n",
    "print(f\"\\nTraining final production model on complete dataset (2015-2024) using {device}...\")\n",
    "\n",
    "# Build complete graph\n",
    "G_bip_final, G_fund_final, fund_features_final, df_final, funds_final, stocks_final = \\\n",
    "    build_graph_and_features_up_to(data, TEST_END_YEAR, 4)\n",
    "\n",
    "fund_idx_final = {f: i for i, f in enumerate(funds_final)}\n",
    "stock_idx_final = {s: i for i, s in enumerate(stocks_final)}\n",
    "\n",
    "print(f\"Final training graph statistics:\")\n",
    "print(f\"  Bipartite edges: {G_bip_final.number_of_edges():,}\")\n",
    "print(f\"  Fund-fund edges: {G_fund_final.number_of_edges():,}\")\n",
    "print(f\"  Number of funds: {len(funds_final):,}\")\n",
    "print(f\"  Number of stocks: {len(stocks_final):,}\")\n",
    "print(f\"  Date range: {df_final['PERIOD_DATE'].min()} to {df_final['PERIOD_DATE'].max()}\")\n",
    "\n",
    "# Node features initialization - ensure GPU usage\n",
    "num_nodes_final = len(funds_final) + len(stocks_final)\n",
    "x_final = torch.randn(num_nodes_final, 16, device=device)\n",
    "print(f\"Node features initialized on {device}\")\n",
    "\n",
    "# Build homogeneous edge index for final training\n",
    "edge_index_homo_final = []\n",
    "for u, v in G_bip_final.edges():\n",
    "    u_idx = fund_idx_final.get(u, len(funds_final) + stock_idx_final.get(u, -1))\n",
    "    v_idx = fund_idx_final.get(v, len(funds_final) + stock_idx_final.get(v, -1))\n",
    "    if u_idx < num_nodes_final and v_idx < num_nodes_final:\n",
    "        edge_index_homo_final.append([u_idx, v_idx])\n",
    "        edge_index_homo_final.append([v_idx, u_idx])\n",
    "\n",
    "# Create edge index directly on GPU\n",
    "edge_index_homo_final = torch.tensor(edge_index_homo_final, dtype=torch.long, device=device).t().contiguous()\n",
    "print(f\"Edge index initialized on {device} with {edge_index_homo_final.shape[1]:,} edges\")\n",
    "\n",
    "# Define GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model on GPU\n",
    "model_final = GraphSAGE(16, 32, 8).to(device)\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Model parameters on GPU: {next(model_final.parameters()).is_cuda}\")\n",
    "\n",
    "optimizer_final = torch.optim.Adam(model_final.parameters(), lr=0.01)\n",
    "\n",
    "# Train GraphSAGE with early stopping\n",
    "model_final.train()\n",
    "prev_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"\\nStarting final GraphSAGE training on {device}...\")\n",
    "for epoch in range(50):\n",
    "    optimizer_final.zero_grad()\n",
    "    out = model_final(x_final, edge_index_homo_final)\n",
    "    pos_score = (out[edge_index_homo_final[0]] * out[edge_index_homo_final[1]]).sum(dim=1).sigmoid()\n",
    "    loss = -torch.log(pos_score + 1e-15).mean()\n",
    "    loss.backward()\n",
    "    optimizer_final.step()\n",
    "    \n",
    "    if abs(prev_loss - loss.item()) < 1e-6:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    else:\n",
    "        no_improve = 0\n",
    "    prev_loss = loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "# Extract final embeddings\n",
    "model_final.eval()\n",
    "with torch.no_grad():\n",
    "    emb_final = model_final(x_final, edge_index_homo_final).cpu().numpy()\n",
    "\n",
    "dynamic_emb_final = emb_final[:len(funds_final)]\n",
    "stock_emb_final = emb_final[len(funds_final):]\n",
    "\n",
    "print(f\"\\nFinal model training completed on {device}.\")\n",
    "print(f\"Final model covers {len(funds_final):,} funds and {len(stocks_final):,} stocks\")\n",
    "print(f\"Final GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

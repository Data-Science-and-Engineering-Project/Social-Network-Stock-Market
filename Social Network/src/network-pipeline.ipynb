{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd74b1",
   "metadata": {},
   "source": [
    "# Stock Market Social Network - Temporal Link Prediction Pipeline\n",
    "\n",
    "Build quarterly bipartite graphs of fund-stock holdings and perform temporal link prediction using sliding windows (2021-2024).\n",
    "\n",
    "**Key Features:**\n",
    "- Load holdings data 2021-2024 only\n",
    "- Build separate bipartite graph per quarter\n",
    "- Sliding window: train on 8 quarters (2 years), predict next quarter\n",
    "- Strict temporal causality (no future leakage)\n",
    "- Per-quarter evaluation metrics (AUC, Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25538a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.centrality import degree_centrality, closeness_centrality\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
    "from networkx.algorithms.link_analysis.hits_alg import hits\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Setup\n",
    "def check_cuda_compatibility():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False, \"CUDA not available\"\n",
    "    try:\n",
    "        test_tensor = torch.zeros(1).cuda()\n",
    "        test_tensor = test_tensor + 1\n",
    "        return True, \"CUDA compatible\"\n",
    "    except Exception as e:\n",
    "        return False, f\"CUDA compatibility issue: {str(e)}\"\n",
    "\n",
    "cuda_compatible, cuda_message = check_cuda_compatibility()\n",
    "print(f\"CUDA Status: {cuda_message}\")\n",
    "\n",
    "if cuda_compatible:\n",
    "    device = torch.device('cuda')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} | CUDA: {torch.version.cuda}')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU (GPU not available)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894d66e",
   "metadata": {},
   "source": [
    "## 1. Data Setup and Loading\n",
    "Load quarterly holdings data from processed parquet files (2021-2024 only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "personal_dir = os.path.expanduser('~')\n",
    "root = os.path.join(personal_dir, 'Social-Network-Stock-Market/Social Network/parquuet_files')\n",
    "output_dir = os.path.join(root, 'generated_combined_parquet')\n",
    "\n",
    "print(f\"Data directory: {root}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Load reference data\n",
    "ticker_map = pd.read_parquet(f\"{root}/ticker_to_cusip.parquet\")\n",
    "prices = pd.read_parquet(f\"{root}/ticker_prices.parquet\")\n",
    "ticker_map[\"cusip\"] = ticker_map[\"cusip\"].astype(str)\n",
    "prices[\"period_start\"] = pd.to_datetime(prices[\"period_start\"])\n",
    "\n",
    "print(f\"✓ Ticker map: {ticker_map.shape}\")\n",
    "print(f\"✓ Prices: {prices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d302fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all processed quarterly holdings files\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading quarterly holdings data (2021-2024)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "combined_files = sorted([f for f in os.listdir(output_dir) \n",
    "                        if f.startswith('holdings_processed_') and f.endswith('.parquet')])\n",
    "\n",
    "if not combined_files:\n",
    "    print(\"ERROR: No processed files found. Check output_dir path.\")\n",
    "else:\n",
    "    all_dfs = []\n",
    "    for file in combined_files:\n",
    "        df_temp = pd.read_parquet(os.path.join(output_dir, file))\n",
    "        year = df_temp['YEAR'].iloc[0]\n",
    "        quarter_str = df_temp['QUARTER'].iloc[0]\n",
    "        print(f\"  ✓ Loaded {file}: {len(df_temp):,} records ({quarter_str})\")\n",
    "        all_dfs.append(df_temp)\n",
    "    \n",
    "    data = pd.concat(all_dfs, ignore_index=True)\n",
    "    data['PERIOD_DATE'] = pd.to_datetime(data['PERIOD_DATE'])\n",
    "    \n",
    "    # Filter to 2021-2024 ONLY\n",
    "    data = data[(data['YEAR'] >= 2021) & (data['YEAR'] <= 2024)].copy()\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(f\"Total records (2021-2024): {len(data):,}\")\n",
    "    print(f\"Date range: {data['PERIOD_DATE'].min()} to {data['PERIOD_DATE'].max()}\")\n",
    "    print(f\"Years: {sorted(data['YEAR'].unique())}\")\n",
    "    print(f\"Unique funds (CIK): {data['CIK'].nunique():,}\")\n",
    "    print(f\"Unique stocks (CUSIP): {data['CUSIP'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d499",
   "metadata": {},
   "source": [
    "## 2. Quarterly Graph Construction (2021-2024 only)\n",
    "Build separate bipartite graphs for each quarter from 2021-2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_quarterly_graphs(data):\n",
    "    \"\"\"\n",
    "    Build separate bipartite graphs for each quarter.\n",
    "    Data should already be filtered to desired time range.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with columns [CIK, CUSIP, VALUE, SSHPRNAMT, PERIOD_DATE, YEAR, QUARTER]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary: {(year, quarter): bipartite_graph}\n",
    "    \"\"\"\n",
    "    quarterly_graphs = {}\n",
    "    \n",
    "    # Group by YEAR and extract quarter from QUARTER column\n",
    "    for (year, quarter_str), group in data.groupby(['YEAR', 'QUARTER']):\n",
    "        quarter = int(quarter_str.split('_')[0][1])  # Extract Q number from \"Q1_2020\"\n",
    "        \n",
    "        funds = group['CIK'].unique()\n",
    "        stocks = group['CUSIP'].unique()\n",
    "        \n",
    "        # Build bipartite graph\n",
    "        G_bip = nx.Graph()\n",
    "        G_bip.add_nodes_from(funds, bipartite=0, node_type='fund')\n",
    "        G_bip.add_nodes_from(stocks, bipartite=1, node_type='stock')\n",
    "        \n",
    "        # Add edges with VALUE weight\n",
    "        edges = [\n",
    "            (row.CIK, row.CUSIP, {'value': row.VALUE, 'amount': row.SSHPRNAMT})\n",
    "            for row in group.itertuples(index=False)\n",
    "        ]\n",
    "        G_bip.add_edges_from(edges)\n",
    "        \n",
    "        quarterly_graphs[(year, quarter)] = G_bip\n",
    "        print(f\"  {year} Q{quarter}: {len(funds):,} funds, {len(stocks):,} stocks, {G_bip.number_of_edges():,} edges\")\n",
    "    \n",
    "    return quarterly_graphs\n",
    "\n",
    "print(\"Building quarterly bipartite graphs (2021-2024 only)...\")\n",
    "quarterly_graphs = build_quarterly_graphs(data)\n",
    "print(f\"\\nTotal quarters: {len(quarterly_graphs)}\")\n",
    "if quarterly_graphs:\n",
    "    min_q, max_q = min(quarterly_graphs.keys()), max(quarterly_graphs.keys())\n",
    "    print(f\"Date range: {min_q} to {max_q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f4787",
   "metadata": {},
   "source": [
    "## 3. Sliding Window Utilities\n",
    "Implement temporal train/test splits with 8-quarter (2-year) training window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baddf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chronological_quarters(quarterly_graphs):\n",
    "    \"\"\"Get all quarters in chronological order.\"\"\"\n",
    "    return sorted(quarterly_graphs.keys())\n",
    "\n",
    "def build_combined_training_graph(quarterly_graphs, quarters_list):\n",
    "    \"\"\"\n",
    "    Combine multiple quarterly graphs into ONE training graph.\n",
    "    This is the UNION of all edges across the training quarters.\n",
    "    \n",
    "    Args:\n",
    "        quarterly_graphs: Dict {(year, quarter): graph}\n",
    "        quarters_list: List of (year, quarter) tuples to combine\n",
    "    \n",
    "    Returns:\n",
    "        Single bipartite graph with all edges from training quarters\n",
    "    \"\"\"\n",
    "    if not quarters_list:\n",
    "        raise ValueError(\"quarters_list cannot be empty\")\n",
    "    \n",
    "    G_train = nx.Graph()\n",
    "    \n",
    "    # Add all nodes and edges from each quarter\n",
    "    for yq in quarters_list:\n",
    "        if yq not in quarterly_graphs:\n",
    "            continue\n",
    "        G_q = quarterly_graphs[yq]\n",
    "        G_train.add_nodes_from(G_q.nodes(data=True))\n",
    "        G_train.add_edges_from(G_q.edges(data=True))\n",
    "    \n",
    "    return G_train\n",
    "\n",
    "def get_sliding_window_splits(chronological_quarters, train_window=3, test_offset=1):\n",
    "    \"\"\"\n",
    "    Generate temporal train/test splits using sliding window.\n",
    "    \n",
    "    Args:\n",
    "        chronological_quarters: Sorted list of (year, quarter) tuples\n",
    "        train_window: Number of quarters for training\n",
    "        test_offset: Quarters ahead to test (default: 1 = immediate next quarter)\n",
    "    \n",
    "    Yields:\n",
    "        (train_quarters_list, test_quarter)\n",
    "    \"\"\"\n",
    "    n = len(chronological_quarters)\n",
    "    \n",
    "    if n < train_window + test_offset:\n",
    "        print(f\"WARNING: Only {n} quarters available, need {train_window + test_offset}\")\n",
    "        return\n",
    "    \n",
    "    for i in range(n - train_window - test_offset + 1):\n",
    "        train_quarters = chronological_quarters[i : i + train_window]\n",
    "        test_quarter = chronological_quarters[i + train_window + test_offset - 1]\n",
    "        yield train_quarters, test_quarter\n",
    "\n",
    "# Show example sliding windows\n",
    "chrono_quarters = get_chronological_quarters(quarterly_graphs)\n",
    "print(f\"Total quarters available: {len(chrono_quarters)}\")\n",
    "print(f\"All quarters: {chrono_quarters}\")\n",
    "\n",
    "print(\"\\nSliding window examples (train_window=3, test_offset=1):\")\n",
    "for i, (train_q, test_q) in enumerate(list(get_sliding_window_splits(chrono_quarters, train_window=3))[:3]):\n",
    "    print(f\"  Window {i+1}:\")\n",
    "    print(f\"    Train: {[f'{y}Q{q}' for y, q in train_q]}\")\n",
    "    print(f\"    Test:  {test_q[0]}Q{test_q[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d7203",
   "metadata": {},
   "source": [
    "## 4. Graph Features: Centrality & Community Detection\n",
    "Compute topological features from training graph only (no future leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fund_features(G_bip, funds):\n",
    "    \"\"\"\n",
    "    Compute topological features for funds from bipartite graph.\n",
    "    Features computed only from G_bip (no future information).\n",
    "    \n",
    "    Args:\n",
    "        G_bip: Bipartite graph (fund-stock holdings)\n",
    "        funds: List of fund CIKs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with features: degree, pagerank, hub, authority, closeness, community\n",
    "    \"\"\"\n",
    "    if len(funds) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Project to fund-fund graph (shared stock holdings)\n",
    "    try:\n",
    "        G_fund = bipartite.weighted_projected_graph(G_bip, funds)\n",
    "    except:\n",
    "        G_fund = nx.Graph()\n",
    "        G_fund.add_nodes_from(funds)\n",
    "    \n",
    "    # Centrality metrics\n",
    "    degree_cent = degree_centrality(G_fund) if G_fund.number_of_nodes() > 0 else {}\n",
    "    pagerank_cent = nx.pagerank(G_fund) if G_fund.number_of_nodes() > 0 else {}\n",
    "    \n",
    "    try:\n",
    "        hubs, authorities = hits(G_fund)\n",
    "    except:\n",
    "        hubs = {f: 0 for f in funds}\n",
    "        authorities = {f: 0 for f in funds}\n",
    "    \n",
    "    # Closeness on largest component\n",
    "    closeness_cent = {}\n",
    "    if G_fund.number_of_nodes() > 0:\n",
    "        try:\n",
    "            comps = list(nx.connected_components(G_fund))\n",
    "            if comps:\n",
    "                largest_cc = max(comps, key=len)\n",
    "                closeness_cent = closeness_centrality(G_fund.subgraph(largest_cc))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Community detection (Leiden algorithm)\n",
    "    communities = {}\n",
    "    if G_fund.number_of_nodes() > 1:\n",
    "        try:\n",
    "            vertex_names = list(G_fund.nodes())\n",
    "            vertex_to_idx = {v: i for i, v in enumerate(vertex_names)}\n",
    "            edge_list = [(vertex_to_idx[u], vertex_to_idx[v]) for u, v in G_fund.edges()]\n",
    "            \n",
    "            if edge_list:\n",
    "                ig_G = ig.Graph(n=len(vertex_names), edges=edge_list)\n",
    "                ig_G.vs['_nx_name'] = vertex_names\n",
    "                partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "                communities = {ig_G.vs[i]['_nx_name']: p for p, cl in enumerate(partition) for i in cl}\n",
    "        except:\n",
    "            communities = {f: 0 for f in funds}\n",
    "    \n",
    "    # Build feature dataframe\n",
    "    fund_features = pd.DataFrame({\n",
    "        'fund': funds,\n",
    "        'degree': [degree_cent.get(f, 0) for f in funds],\n",
    "        'pagerank': [pagerank_cent.get(f, 0) for f in funds],\n",
    "        'hub': [hubs.get(f, 0) for f in funds],\n",
    "        'authority': [authorities.get(f, 0) for f in funds],\n",
    "        'closeness': [closeness_cent.get(f, 0) for f in funds],\n",
    "        'community': [communities.get(f, -1) for f in funds]\n",
    "    }).set_index('fund')\n",
    "    \n",
    "    return fund_features\n",
    "\n",
    "print(\"Feature computation function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6135c0",
   "metadata": {},
   "source": [
    "## 5. GraphSAGE Embeddings\n",
    "Train GraphSAGE on training window to generate node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\"2-layer GraphSAGE model for bipartite graphs.\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train_graphsage_window(G_bip, num_epochs=30, embedding_dim=8, pretrained_model=None, fine_tune_lr=0.001):\n",
    "    \"\"\"\n",
    "    Train GraphSAGE on bipartite graph with optional transfer learning.\n",
    "    \n",
    "    Args:\n",
    "        G_bip: Bipartite graph\n",
    "        num_epochs: Training epochs\n",
    "        embedding_dim: Output embedding dimension\n",
    "        pretrained_model: Optional pre-trained GraphSAGE model (for transfer learning)\n",
    "        fine_tune_lr: Learning rate for fine-tuning (lower than training from scratch)\n",
    "    \n",
    "    Returns:\n",
    "        (model, embeddings_numpy, funds, stocks)\n",
    "    \"\"\"\n",
    "    nodes = list(G_bip.nodes())\n",
    "    node_to_idx = {n: i for i, n in enumerate(nodes)}\n",
    "    \n",
    "    # Separate funds and stocks\n",
    "    funds = [n for n in nodes if G_bip.nodes[n].get('bipartite') == 0]\n",
    "    stocks = [n for n in nodes if G_bip.nodes[n].get('bipartite') == 1]\n",
    "    \n",
    "    num_nodes = len(nodes)\n",
    "    \n",
    "    # Initialize features on GPU\n",
    "    x = torch.randn(num_nodes, 16, device=device)\n",
    "    \n",
    "    # Build edge index\n",
    "    edge_list = [(node_to_idx[u], node_to_idx[v]) for u, v in G_bip.edges()]\n",
    "    if not edge_list:\n",
    "        print(\"    WARNING: Graph has no edges\")\n",
    "        return None, np.zeros((num_nodes, embedding_dim)), funds, stocks\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long, device=device).t().contiguous()\n",
    "    \n",
    "    # Add reverse edges for undirected graph\n",
    "    edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "    \n",
    "    # Model: use pretrained if available, otherwise create new\n",
    "    if pretrained_model is not None:\n",
    "        print(\"     → Using pretrained model (transfer learning)\")\n",
    "        model = pretrained_model.to(device)\n",
    "        learning_rate = fine_tune_lr\n",
    "    else:\n",
    "        print(\"     → Training from scratch\")\n",
    "        model = GraphSAGE(16, 32, embedding_dim).to(device)\n",
    "        learning_rate = 0.01\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    prev_loss = float('inf')\n",
    "    patience, no_improve = 5, 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, edge_index)\n",
    "        \n",
    "        # Link prediction loss\n",
    "        pos_score = (out[edge_index[0]] * out[edge_index[1]]).sum(dim=1).sigmoid()\n",
    "        loss = -torch.log(pos_score + 1e-15).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if abs(prev_loss - loss.item()) < 1e-6:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "        else:\n",
    "            no_improve = 0\n",
    "        prev_loss = loss.item()\n",
    "    \n",
    "    # Extract embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = model(x, edge_index).cpu().numpy()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, emb, funds, stocks\n",
    "\n",
    "print(\"GraphSAGE model with transfer learning defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b3bf0",
   "metadata": {},
   "source": [
    "## 6. Link Prediction Features & Negative Sampling\n",
    "Build training/test features for link prediction with proper negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_prediction_features(G_bip_train, G_bip_test, embeddings_train, \n",
    "                                    fund_features, funds, stocks, fund_to_idx, stock_to_idx):\n",
    "    \"\"\"\n",
    "    Create feature matrix for link prediction.\n",
    "    \n",
    "    Training edges: from G_bip_train (label=1) + hard negatives (label=0)\n",
    "    Test edges: from G_bip_test (label=1) + negatives not in G_bip_train (label=0)\n",
    "    \n",
    "    Args:\n",
    "        G_bip_train: Training bipartite graph\n",
    "        G_bip_test: Test bipartite graph (for positive labels only)\n",
    "        embeddings_train: Node embeddings (from GraphSAGE)\n",
    "        fund_features: DataFrame with fund topological features\n",
    "        funds, stocks: Lists of nodes\n",
    "        fund_to_idx, stock_to_idx: Node to index mappings\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    fund_emb = embeddings_train[:len(funds)]\n",
    "    stock_emb = embeddings_train[len(funds):]\n",
    "    \n",
    "    # ── TRAINING DATA ──\n",
    "    # Positive edges from training graph\n",
    "    pos_edges_train = [\n",
    "        (fund_to_idx[u], stock_to_idx[v]) \n",
    "        for u, v in G_bip_train.edges() \n",
    "        if u in fund_to_idx and v in stock_to_idx\n",
    "    ]\n",
    "    \n",
    "    # Hard negative sampling (use stock similarity)\n",
    "    stock_sim = cosine_similarity(stock_emb)\n",
    "    neg_edges_train = []\n",
    "    \n",
    "    for f_idx in range(len(funds)):\n",
    "        fund_id = funds[f_idx]\n",
    "        # Get connected stocks in training graph\n",
    "        connected_stocks = {stock_to_idx[s] for s in G_bip_train.neighbors(fund_id) \n",
    "                           if s in stock_to_idx}\n",
    "        \n",
    "        if not connected_stocks:\n",
    "            continue\n",
    "        \n",
    "        # Average similarity to connected stocks\n",
    "        connected_list = list(connected_stocks)\n",
    "        avg_sim = stock_sim[connected_list].mean(axis=0)\n",
    "        \n",
    "        # Hard negatives: high similarity but not connected\n",
    "        hard_negs = np.argsort(-avg_sim)\n",
    "        hard_neg_list = [\n",
    "            s_idx for s_idx in hard_negs \n",
    "            if s_idx not in connected_stocks and len(neg_edges_train) < len(pos_edges_train)\n",
    "        ]\n",
    "        \n",
    "        neg_edges_train.extend([(f_idx, s_idx) for s_idx in hard_neg_list[:20]])\n",
    "    \n",
    "    neg_edges_train = neg_edges_train[:len(pos_edges_train)]  # Balance classes\n",
    "    \n",
    "    # ── TEST DATA ──\n",
    "    # Positive edges from test graph (only edges we didn't see in training)\n",
    "    test_edges_train = set((fund_to_idx[u], stock_to_idx[v]) \n",
    "                          for u, v in G_bip_train.edges() \n",
    "                          if u in fund_to_idx and v in stock_to_idx)\n",
    "    \n",
    "    pos_edges_test = [\n",
    "        (fund_to_idx[u], stock_to_idx[v]) \n",
    "        for u, v in G_bip_test.edges() \n",
    "        if u in fund_to_idx and v in stock_to_idx and (fund_to_idx[u], stock_to_idx[v]) not in test_edges_train\n",
    "    ]\n",
    "    \n",
    "    # Test negatives: not in training OR test graphs\n",
    "    all_possible = set((i, j) for i in range(len(funds)) for j in range(len(stocks)))\n",
    "    test_edges_all = test_edges_train | set(pos_edges_test)\n",
    "    neg_edges_test = list(all_possible - test_edges_all)\n",
    "    neg_edges_test = neg_edges_test[:max(len(pos_edges_test), 1)]\n",
    "    \n",
    "    # Build feature vectors\n",
    "    def build_features(edge_list):\n",
    "        features = []\n",
    "        for f_idx, s_idx in edge_list:\n",
    "            fund_id = funds[f_idx]\n",
    "            feat = np.concatenate([\n",
    "                fund_emb[f_idx],\n",
    "                stock_emb[s_idx],\n",
    "                fund_features.loc[fund_id].values if fund_id in fund_features.index else np.zeros(6)\n",
    "            ])\n",
    "            features.append(feat)\n",
    "        return np.array(features) if features else np.zeros((0, fund_emb.shape[1] + stock_emb.shape[1] + 6))\n",
    "    \n",
    "    X_train = np.vstack([\n",
    "        build_features(pos_edges_train),\n",
    "        build_features(neg_edges_train)\n",
    "    ])\n",
    "    y_train = np.hstack([np.ones(len(pos_edges_train)), np.zeros(len(neg_edges_train))])\n",
    "    \n",
    "    X_test = np.vstack([\n",
    "        build_features(pos_edges_test),\n",
    "        build_features(neg_edges_test)\n",
    "    ])\n",
    "    y_test = np.hstack([np.ones(len(pos_edges_test)), np.zeros(len(neg_edges_test))])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"Link prediction feature builder ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e803f53",
   "metadata": {},
   "source": [
    "## 7. Temporal Link Prediction: Sliding Window Evaluation (2021-2024)\n",
    "Evaluate model per quarter with configurable training window and strict temporal causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for sliding window\n",
    "TRAIN_WINDOW = 3  # Number of quarters to train on\n",
    "TEST_OFFSET = 1   # How many quarters ahead to test\n",
    "\n",
    "print(f\"Sliding window configuration:\")\n",
    "print(f\"  Training window: {TRAIN_WINDOW} quarters\")\n",
    "print(f\"  Test offset: {TEST_OFFSET} quarter(s) ahead\")\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create directory for saved models\n",
    "models_dir = 'temporal_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"  Models will be saved to: {models_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(f\"TEMPORAL LINK PREDICTION: SLIDING WINDOW EVALUATION (2021-2024)\")\n",
    "print(f\"Train window: {TRAIN_WINDOW} quarters | Test offset: {TEST_OFFSET} quarter(s)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "results_per_quarter = []\n",
    "chrono_quarters = get_chronological_quarters(quarterly_graphs)\n",
    "pretrained_graphsage = None  # Will store previous window's model\n",
    "\n",
    "# Run sliding window evaluation with transfer learning\n",
    "for window_idx, (train_quarters, test_quarter) in enumerate(\n",
    "    get_sliding_window_splits(chrono_quarters, train_window=TRAIN_WINDOW, test_offset=TEST_OFFSET)\n",
    "):\n",
    "    test_year, test_quarter_num = test_quarter\n",
    "    train_label = ' → '.join([f\"{y}Q{q}\" for y, q in train_quarters])\n",
    "    \n",
    "    print(f\"\\n{'─' * 100}\")\n",
    "    print(f\"WINDOW {window_idx + 1} | TEST: {test_year}Q{test_quarter_num}\")\n",
    "    print(f\"TRAIN: {train_label}\")\n",
    "    print(f\"{'─' * 100}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Build COMBINED training graph from all training quarters\n",
    "        print(\"  1. Building combined training graph...\")\n",
    "        G_bip_train = build_combined_training_graph(quarterly_graphs, train_quarters)\n",
    "        funds_train = [n for n in G_bip_train.nodes() if G_bip_train.nodes[n].get('bipartite') == 0]\n",
    "        stocks_train = [n for n in G_bip_train.nodes() if G_bip_train.nodes[n].get('bipartite') == 1]\n",
    "        \n",
    "        print(f\"     Funds: {len(funds_train):,} | Stocks: {len(stocks_train):,} | Edges: {G_bip_train.number_of_edges():,}\")\n",
    "        \n",
    "        if G_bip_train.number_of_nodes() == 0:\n",
    "            print(\"     WARNING: Training graph is empty, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # 2. Compute topological features\n",
    "        print(\"  2. Computing topological features...\")\n",
    "        fund_features = compute_fund_features(G_bip_train, funds_train)\n",
    "        print(f\"     Feature shape: {fund_features.shape}\")\n",
    "        \n",
    "        # 3. Train GraphSAGE (with transfer learning from previous window)\n",
    "        print(\"  3. Training GraphSAGE embeddings...\")\n",
    "        if pretrained_graphsage is not None:\n",
    "            print(f\"     → Using pretrained model (transfer learning from window {window_idx})\")\n",
    "            graphsage_model, embeddings, funds_sage, stocks_sage = train_graphsage_window(\n",
    "                G_bip_train, num_epochs=30, embedding_dim=8, \n",
    "                pretrained_model=pretrained_graphsage, fine_tune_lr=0.001\n",
    "            )\n",
    "        else:\n",
    "            print(\"     → Training from scratch\")\n",
    "            graphsage_model, embeddings, funds_sage, stocks_sage = train_graphsage_window(\n",
    "                G_bip_train, num_epochs=30, embedding_dim=8, \n",
    "                pretrained_model=None\n",
    "            )\n",
    "        \n",
    "        print(f\"     Embeddings shape: {embeddings.shape}\")\n",
    "        \n",
    "        # 4. Get TEST quarter graph (individual quarter, not aggregated)\n",
    "        print(\"  4. Loading test quarter graph...\")\n",
    "        G_bip_test = quarterly_graphs.get(test_quarter)\n",
    "        if G_bip_test is None:\n",
    "            print(f\"     WARNING: Test quarter {test_quarter} not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        funds_test = [n for n in G_bip_test.nodes() if G_bip_test.nodes[n].get('bipartite') == 0]\n",
    "        stocks_test = [n for n in G_bip_test.nodes() if G_bip_test.nodes[n].get('bipartite') == 1]\n",
    "        print(f\"     Test funds: {len(funds_test):,} | Test stocks: {len(stocks_test):,} | Edges: {G_bip_test.number_of_edges():,}\")\n",
    "        \n",
    "        # 5. Create mappings and link prediction features\n",
    "        print(\"  5. Creating link prediction features...\")\n",
    "        fund_to_idx = {f: i for i, f in enumerate(funds_train)}\n",
    "        stock_to_idx = {s: i for i, s in enumerate(stocks_train)}\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = create_link_prediction_features(\n",
    "            G_bip_train, G_bip_test, embeddings, fund_features,\n",
    "            funds_train, stocks_train, fund_to_idx, stock_to_idx\n",
    "        )\n",
    "        print(f\"     Train: {X_train.shape[0]:,} samples (pos: {y_train.sum():.0f}, neg: {(1-y_train).sum():.0f})\")\n",
    "        print(f\"     Test:  {X_test.shape[0]:,} samples (pos: {y_test.sum():.0f}, neg: {(1-y_test).sum():.0f})\")\n",
    "        \n",
    "        if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "            print(\"     WARNING: No training or test samples, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # 6. Train LightGBM\n",
    "        print(\"  6. Training LightGBM...\")\n",
    "        train_data_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        bst = lgb.train(params, train_data_lgb, num_boost_round=100, valid_sets=[train_data_lgb])\n",
    "        \n",
    "        # 7. Evaluate\n",
    "        print(\"  7. Evaluating...\")\n",
    "        y_pred = bst.predict(X_test)\n",
    "        \n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        recall = recall_score(y_test, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        \n",
    "        print(f\"\\n     ✓ AUC:       {auc:.4f}\")\n",
    "        print(f\"     ✓ Precision: {precision:.4f}\")\n",
    "        print(f\"     ✓ Recall:    {recall:.4f}\")\n",
    "        \n",
    "        # 8. Save models to pickle\n",
    "        print(\"  8. Saving models to pickle...\")\n",
    "        model_filename = f\"window_{window_idx+1}_{train_label.replace(' → ', '_')}_test_{test_year}Q{test_quarter_num}.pkl\"\n",
    "        model_path = os.path.join(models_dir, model_filename)\n",
    "        \n",
    "        model_data = {\n",
    "            'window': window_idx + 1,\n",
    "            'graphsage_model': graphsage_model,\n",
    "            'lgb_model': bst,\n",
    "            'embeddings': embeddings,\n",
    "            'fund_features': fund_features,\n",
    "            'fund_to_idx': fund_to_idx,\n",
    "            'stock_to_idx': stock_to_idx,\n",
    "            'funds_train': funds_train,\n",
    "            'stocks_train': stocks_train,\n",
    "            'train_quarters': train_quarters,\n",
    "            'test_quarter': test_quarter,\n",
    "            'metrics': {\n",
    "                'auc': auc,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"     ✓ Saved to: {model_path}\")\n",
    "        \n",
    "        # Save GraphSAGE model for next window (transfer learning)\n",
    "        pretrained_graphsage = graphsage_model\n",
    "        \n",
    "        results_per_quarter.append({\n",
    "            'window': window_idx + 1,\n",
    "            'test_year': test_year,\n",
    "            'test_quarter': test_quarter_num,\n",
    "            'train_quarters': train_label,\n",
    "            'n_train_funds': len(funds_train),\n",
    "            'n_train_stocks': len(stocks_train),\n",
    "            'n_test_funds': len(funds_test),\n",
    "            'n_test_stocks': len(stocks_test),\n",
    "            'auc': auc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'n_test_samples': X_test.shape[0],\n",
    "            'model_path': model_path,\n",
    "            'transfer_learned': window_idx > 0\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\\n{'=' * 100}\")\n",
    "print(f\"EVALUATION SUMMARY (2021-2024, {TRAIN_WINDOW}-Quarter Training Window)\")\n",
    "print(f\"{'=' * 100}\")\n",
    "\n",
    "if results_per_quarter:\n",
    "    results_df = pd.DataFrame(results_per_quarter)\n",
    "    print(f\"\\nResults across {len(results_df)} windows:\\n\")\n",
    "    display_cols = ['window', 'train_quarters', 'test_year', 'test_quarter', 'transfer_learned', 'auc', 'precision', 'recall']\n",
    "    print(results_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n\\nAggregate Statistics:\")\n",
    "    print(f\"  Average AUC:       {results_df['auc'].mean():.4f} (±{results_df['auc'].std():.4f})\")\n",
    "    print(f\"  Average Precision: {results_df['precision'].mean():.4f} (±{results_df['precision'].std():.4f})\")\n",
    "    print(f\"  Average Recall:    {results_df['recall'].mean():.4f} (±{results_df['recall'].std():.4f})\")\n",
    "    \n",
    "    print(f\"\\n\\nBy Year:\")\n",
    "    by_year = results_df.groupby('test_year')[['auc', 'precision', 'recall']].mean()\n",
    "    print(by_year)\n",
    "    \n",
    "    results_df.to_csv('temporal_link_prediction_results.csv', index=False)\n",
    "    print(f\"\\n✓ Results saved to temporal_link_prediction_results.csv\")\n",
    "    print(f\"✓ Models saved to {models_dir}/ directory ({len(results_df)} model files)\")\n",
    "    print(f\"✓ Transfer learning: Windows 2+ use previous window's GraphSAGE model\")\n",
    "else:\n",
    "    print(\"No results generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

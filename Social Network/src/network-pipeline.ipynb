{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd74b1",
   "metadata": {},
   "source": [
    "# Stock Market Social Network Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25538a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Status: CUDA compatibility issue: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Using device: cpu (CPU fallback)\n",
      "Note: For GPU support, ensure PyTorch is installed with CUDA support matching your GPU\n",
      "Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.centrality import degree_centrality, closeness_centrality\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
    "from networkx.algorithms.link_analysis.hits_alg import hits\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Setup with compatibility checks\n",
    "def check_cuda_compatibility():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False, \"CUDA not available\"\n",
    "    \n",
    "    try:\n",
    "        # Test basic CUDA operation\n",
    "        test_tensor = torch.zeros(1).cuda()\n",
    "        test_tensor = test_tensor + 1\n",
    "        return True, \"CUDA compatible\"\n",
    "    except Exception as e:\n",
    "        return False, f\"CUDA compatibility issue: {str(e)}\"\n",
    "\n",
    "cuda_compatible, cuda_message = check_cuda_compatibility()\n",
    "print(f\"CUDA Status: {cuda_message}\")\n",
    "\n",
    "if cuda_compatible:\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'PyTorch Version: {torch.__version__}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Using device: {device} (CPU fallback)')\n",
    "    print('Note: For GPU support, ensure PyTorch is installed with CUDA support matching your GPU')\n",
    "    print('Install with: pip install torch --index-url https://download.pytorch.org/whl/cu118')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894d66e",
   "metadata": {},
   "source": [
    "## 1.1. Extract Parquet Files from Zip Archives\n",
    "Extract all zip files in the parquet_files directory and remove the zip files after extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4587eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 zip file(s) in /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files\n",
      "Extracting parquet_files.zip...\n",
      "  ✓ Extracted parquet_files.zip\n",
      "  ✓ Deleted parquet_files.zip\n",
      "All zip files processed and cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Extract zip files from parquet_files directory\n",
    "persoanl_dir = os.path.expanduser('~')\n",
    "parquet_dir = os.path.join(persoanl_dir, 'Social-Network-Stock-Market/Social Network/parquet_files')\n",
    "if os.path.exists(parquet_dir):\n",
    "    zip_files = [f for f in os.listdir(parquet_dir) if f.endswith('.zip')]\n",
    "    \n",
    "    if zip_files:\n",
    "        print(f'Found {len(zip_files)} zip file(s) in {parquet_dir}')\n",
    "        \n",
    "        for zip_file in zip_files:\n",
    "            zip_path = os.path.join(parquet_dir, zip_file)\n",
    "            print(f'Extracting {zip_file}...')\n",
    "            \n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(parquet_dir)\n",
    "                print(f'  ✓ Extracted {zip_file}')\n",
    "                \n",
    "                # Delete the zip file after extraction\n",
    "                os.remove(zip_path)\n",
    "                print(f'  ✓ Deleted {zip_file}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'  ✗ Error processing {zip_file}: {e}')\n",
    "        \n",
    "        print('All zip files processed and cleaned up.')\n",
    "    else:\n",
    "        print(f'No zip files found in {parquet_dir}')\n",
    "else:\n",
    "    print(f'Directory not found: {parquet_dir}')\n",
    "    print('Creating directory...')\n",
    "    os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d499",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning\n",
    "Load and preprocess the raw fund-stock holding data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720e3e6",
   "metadata": {},
   "source": [
    "##### Creating new parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f576e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/ticker_to_cusip.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the data\n",
    "\n",
    "root = os.path.join(persoanl_dir, 'Social-Network-Stock-Market/Social Network/parquuet_files')\n",
    "print(f\"{root}/ticker_to_cusip.parquet\")\n",
    "ticker_map = pd.read_parquet(f\"{root}/ticker_to_cusip.parquet\")\n",
    "prices = pd.read_parquet(f\"{root}/ticker_prices.parquet\")\n",
    "output_dir = f\"{root}/generated_combined_parquet\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b810f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INSPECTING INPUT FILES\n",
      "================================================================================\n",
      "\n",
      "1. TICKER_MAP (ticker_to_cusip.parquet):\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (4571, 5)\n",
      "Columns: ['name', 'cusip', 'ticker', 'trading_start_date', 'trading_end_date']\n",
      "\n",
      "First 5 rows:\n",
      "              name      cusip ticker trading_start_date trading_end_date\n",
      "0  A. Schulman Inc  808194104   SHLM         1973-01-09       2018-08-20\n",
      "1   A.H. Belo Corp  001282102    AHC               None             None\n",
      "2  A.O. Smith Corp  831865209    AOS         1983-09-30       2026-01-06\n",
      "3         AAON Inc  000360206   AAON         1991-01-03       2026-01-06\n",
      "4         AAR Corp  000361105    AIR         1980-03-17       2026-01-06\n",
      "\n",
      "Data types:\n",
      "name                  object\n",
      "cusip                 object\n",
      "ticker                object\n",
      "trading_start_date    object\n",
      "trading_end_date      object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "2. PRICES (ticker_prices.parquet):\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: (158011, 4)\n",
      "Columns: ['ticker', 'date', 'price', 'period_start']\n",
      "\n",
      "First 5 rows:\n",
      "  ticker        date    price period_start\n",
      "0   SHLM  2013-07-01  24.4628   2013-04-01\n",
      "1   SHLM  2013-09-30  26.0762   2013-07-01\n",
      "2   SHLM  2013-12-31  31.4066   2013-10-01\n",
      "3   SHLM  2014-03-31  32.4822   2014-01-01\n",
      "4   SHLM  2014-06-30  34.8644   2014-04-01\n",
      "\n",
      "Data types:\n",
      "ticker           object\n",
      "date             object\n",
      "price           float64\n",
      "period_start     object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "3. SAMPLE HOLDINGS FILE:\n",
      "--------------------------------------------------------------------------------\n",
      "File: holdings_filtered_new_period_start_2013-04-01.parquet\n",
      "Shape: (19659, 7)\n",
      "Columns: ['nameofissuer', 'cusip', 'sshprnamt', 'cik', 'year', 'quarter', 'period_start']\n",
      "\n",
      "First 5 rows:\n",
      "  nameofissuer      cusip  sshprnamt         cik  year  quarter period_start\n",
      "0     AAON INC  000360206       70.0  0001011659  2013        2   2013-04-01\n",
      "1     AAON INC  000360206    18598.0  0001564702  2013        2   2013-04-01\n",
      "2     AAON INC  000360206   173935.0  0001374170  2013        2   2013-04-01\n",
      "3     AAON INC  000360206     8523.0  0001121914  2013        2   2013-04-01\n",
      "4     AAON INC  000360206     3484.0  0000049205  2013        2   2013-04-01\n",
      "\n",
      "Data types:\n",
      "nameofissuer     object\n",
      "cusip            object\n",
      "sshprnamt       float64\n",
      "cik              object\n",
      "year              int64\n",
      "quarter           int64\n",
      "period_start     object\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "INSPECTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Check the structure of your files\n",
    "print(\"=\" * 80)\n",
    "print(\"INSPECTING INPUT FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check ticker_map structure\n",
    "print(\"\\n1. TICKER_MAP (ticker_to_cusip.parquet):\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    ticker_map_sample = pd.read_parquet(f\"{root}/ticker_to_cusip.parquet\")\n",
    "    print(f\"Shape: {ticker_map_sample.shape}\")\n",
    "    print(f\"Columns: {list(ticker_map_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(ticker_map_sample.head())\n",
    "    print(f\"\\nData types:\\n{ticker_map_sample.dtypes}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading ticker_map: {e}\")\n",
    "\n",
    "# 2. Check prices structure\n",
    "print(\"\\n\\n2. PRICES (ticker_prices.parquet):\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    prices_sample = pd.read_parquet(f\"{root}/ticker_prices.parquet\")\n",
    "    print(f\"Shape: {prices_sample.shape}\")\n",
    "    print(f\"Columns: {list(prices_sample.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(prices_sample.head())\n",
    "    print(f\"\\nData types:\\n{prices_sample.dtypes}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading prices: {e}\")\n",
    "\n",
    "# 3. Check first holdings file structure\n",
    "print(\"\\n\\n3. SAMPLE HOLDINGS FILE:\")\n",
    "print(\"-\" * 80)\n",
    "holdings_pattern = os.path.join(root, \"holdings_filtered_new_period_start_*.parquet\")\n",
    "holdings_files = sorted(glob.glob(holdings_pattern))\n",
    "\n",
    "if holdings_files:\n",
    "    sample_file = holdings_files[0]\n",
    "    print(f\"File: {os.path.basename(sample_file)}\")\n",
    "    try:\n",
    "        holdings_sample = pd.read_parquet(sample_file)\n",
    "        print(f\"Shape: {holdings_sample.shape}\")\n",
    "        print(f\"Columns: {list(holdings_sample.columns)}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(holdings_sample.head())\n",
    "        print(f\"\\nData types:\\n{holdings_sample.dtypes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading holdings: {e}\")\n",
    "else:\n",
    "    print(\"No holdings files found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INSPECTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87316a29",
   "metadata": {},
   "source": [
    "##### Iterating on all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f321aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 holdings files\n",
      "Processing 2013 Q2 (period_start: 2013-04-01)...\n",
      "  ✓ Saved 19659 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2013.parquet\n",
      "Processing 2013 Q3 (period_start: 2013-07-01)...\n",
      "  ✓ Saved 34213 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2013.parquet\n",
      "Processing 2013 Q4 (period_start: 2013-10-01)...\n",
      "  ✓ Saved 1018601 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2013.parquet\n",
      "Processing 2014 Q1 (period_start: 2014-01-01)...\n",
      "  ✓ Saved 1017686 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2014.parquet\n",
      "Processing 2014 Q2 (period_start: 2014-04-01)...\n",
      "  ✓ Saved 1060398 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2014.parquet\n",
      "Processing 2014 Q3 (period_start: 2014-07-01)...\n",
      "  ✓ Saved 1049697 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2014.parquet\n",
      "Processing 2014 Q4 (period_start: 2014-10-01)...\n",
      "  ✓ Saved 1093267 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2014.parquet\n",
      "Processing 2015 Q1 (period_start: 2015-01-01)...\n",
      "  ✓ Saved 1119642 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2015.parquet\n",
      "Processing 2015 Q2 (period_start: 2015-04-01)...\n",
      "  ✓ Saved 1095826 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2015.parquet\n",
      "Processing 2015 Q3 (period_start: 2015-07-01)...\n",
      "  ✓ Saved 1081637 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2015.parquet\n",
      "Processing 2015 Q4 (period_start: 2015-10-01)...\n",
      "  ✓ Saved 1115302 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2015.parquet\n",
      "Processing 2016 Q1 (period_start: 2016-01-01)...\n",
      "  ✓ Saved 1131465 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2016.parquet\n",
      "Processing 2016 Q2 (period_start: 2016-04-01)...\n",
      "  ✓ Saved 1127160 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2016.parquet\n",
      "Processing 2016 Q3 (period_start: 2016-07-01)...\n",
      "  ✓ Saved 1117857 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2016.parquet\n",
      "Processing 2016 Q4 (period_start: 2016-10-01)...\n",
      "  ✓ Saved 1170982 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2016.parquet\n",
      "Processing 2017 Q1 (period_start: 2017-01-01)...\n",
      "  ✓ Saved 1190709 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2017.parquet\n",
      "Processing 2017 Q2 (period_start: 2017-04-01)...\n",
      "  ✓ Saved 1169387 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2017.parquet\n",
      "Processing 2017 Q3 (period_start: 2017-07-01)...\n",
      "  ✓ Saved 1131807 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2017.parquet\n",
      "Processing 2017 Q4 (period_start: 2017-10-01)...\n",
      "  ✓ Saved 1252888 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2017.parquet\n",
      "Processing 2018 Q1 (period_start: 2018-01-01)...\n",
      "  ✓ Saved 1294867 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2018.parquet\n",
      "Processing 2018 Q2 (period_start: 2018-04-01)...\n",
      "  ✓ Saved 1299201 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2018.parquet\n",
      "Processing 2018 Q3 (period_start: 2018-07-01)...\n",
      "  ✓ Saved 1356314 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2018.parquet\n",
      "Processing 2018 Q4 (period_start: 2018-10-01)...\n",
      "  ✓ Saved 1275091 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2018.parquet\n",
      "Processing 2019 Q1 (period_start: 2019-01-01)...\n",
      "  ✓ Saved 1337369 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2019.parquet\n",
      "Processing 2019 Q2 (period_start: 2019-04-01)...\n",
      "  ✓ Saved 1344696 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2019.parquet\n",
      "Processing 2019 Q3 (period_start: 2019-07-01)...\n",
      "  ✓ Saved 1308113 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2019.parquet\n",
      "Processing 2019 Q4 (period_start: 2019-10-01)...\n",
      "  ✓ Saved 1353353 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2019.parquet\n",
      "Processing 2020 Q1 (period_start: 2020-01-01)...\n",
      "  ✓ Saved 1304800 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2020.parquet\n",
      "Processing 2020 Q2 (period_start: 2020-04-01)...\n",
      "  ✓ Saved 1307037 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2020.parquet\n",
      "Processing 2020 Q3 (period_start: 2020-07-01)...\n",
      "  ✓ Saved 1309991 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2020.parquet\n",
      "Processing 2020 Q4 (period_start: 2020-10-01)...\n",
      "  ✓ Saved 1456047 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2020.parquet\n",
      "Processing 2021 Q1 (period_start: 2021-01-01)...\n",
      "  ✓ Saved 1425771 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2021.parquet\n",
      "Processing 2021 Q2 (period_start: 2021-04-01)...\n",
      "  ✓ Saved 1470761 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2021.parquet\n",
      "Processing 2021 Q3 (period_start: 2021-07-01)...\n",
      "  ✓ Saved 1484007 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2021.parquet\n",
      "Processing 2021 Q4 (period_start: 2021-10-01)...\n",
      "  ✓ Saved 1564186 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2021.parquet\n",
      "Processing 2022 Q1 (period_start: 2022-01-01)...\n",
      "  ✓ Saved 1557935 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2022.parquet\n",
      "Processing 2022 Q2 (period_start: 2022-04-01)...\n",
      "  ✓ Saved 1561596 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2022.parquet\n",
      "Processing 2022 Q3 (period_start: 2022-07-01)...\n",
      "  ✓ Saved 1514464 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2022.parquet\n",
      "Processing 2022 Q4 (period_start: 2022-10-01)...\n",
      "  ✓ Saved 1612235 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2022.parquet\n",
      "Processing 2023 Q1 (period_start: 2023-01-01)...\n",
      "  ✓ Saved 1615015 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2023.parquet\n",
      "Processing 2023 Q2 (period_start: 2023-04-01)...\n",
      "  ✓ Saved 1586359 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2023.parquet\n",
      "Processing 2023 Q3 (period_start: 2023-07-01)...\n",
      "  ✓ Saved 1658916 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2023.parquet\n",
      "Processing 2023 Q4 (period_start: 2023-10-01)...\n",
      "  ✓ Saved 1714038 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2023.parquet\n",
      "Processing 2024 Q1 (period_start: 2024-01-01)...\n",
      "  ✓ Saved 1736830 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2024.parquet\n",
      "Processing 2024 Q2 (period_start: 2024-04-01)...\n",
      "  ✓ Saved 1771738 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2024.parquet\n",
      "Processing 2024 Q3 (period_start: 2024-07-01)...\n",
      "  ✓ Saved 1764953 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q3_2024.parquet\n",
      "Processing 2024 Q4 (period_start: 2024-10-01)...\n",
      "  ✓ Saved 115914 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q4_2024.parquet\n",
      "Processing 2025 Q1 (period_start: 2025-01-01)...\n",
      "  ✓ Saved 1878074 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q1_2025.parquet\n",
      "Processing 2025 Q2 (period_start: 2025-04-01)...\n",
      "  ✓ Saved 1822141 records to /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_processed_Q2_2025.parquet\n",
      "\n",
      "Combining all data into single parquet file...\n",
      "\n",
      "✓ Combined parquet saved: /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet/holdings_all_years_combined.parquet\n",
      "  Total records: 62,799,995\n",
      "  Date range: 2013-04-01 00:00:00 to 2025-04-01 00:00:00\n",
      "  Years: [np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "  Unique funds (CIK): 11,435\n",
      "  Unique stocks (CUSIP): 4,570\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Get all holdings parquet files sorted by date\n",
    "ticker_map[\"cusip\"] = ticker_map[\"cusip\"].astype(str)\n",
    "prices[\"period_start\"] = pd.to_datetime(prices[\"period_start\"])\n",
    "holdings_pattern = os.path.join(root, \"holdings_filtered_new_period_start_*.parquet\")\n",
    "holdings_files = sorted(glob.glob(holdings_pattern))\n",
    "\n",
    "print(f\"Found {len(holdings_files)} holdings files\")\n",
    "\n",
    "# Extract year and quarter from each file\n",
    "def extract_date_info(filepath):\n",
    "    \"\"\"Extract period_start date from filename\"\"\"\n",
    "    match = re.search(r'period_start_(\\d{4}-\\d{2}-\\d{2})', filepath)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        date_obj = pd.to_datetime(date_str)\n",
    "        year = date_obj.year\n",
    "        quarter = (date_obj.month - 1) // 3 + 1\n",
    "        return date_obj, year, quarter\n",
    "    return None, None, None\n",
    "\n",
    "# Process all files\n",
    "all_dfs = []\n",
    "\n",
    "for file_path in holdings_files:\n",
    "    period_date, year, quarter = extract_date_info(file_path)\n",
    "    \n",
    "    if period_date is None:\n",
    "        print(f\"Skipping file with unrecognized format: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing {year} Q{quarter} (period_start: {period_date.date()})...\")\n",
    "    \n",
    "    try:\n",
    "        # Read holdings data\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Rename to required format\n",
    "        df = df.rename(columns={\n",
    "            \"cik\": \"CIK\",\n",
    "            \"cusip\": \"CUSIP\",\n",
    "            \"sshprnamt\": \"SSHPRNAMT\",\n",
    "            \"period_start\": \"PERIOD_DATE\"\n",
    "        })\n",
    "        \n",
    "        # Type normalization\n",
    "        df[\"CIK\"] = df[\"CIK\"].astype(str)\n",
    "        df[\"CUSIP\"] = df[\"CUSIP\"].astype(str)\n",
    "        df[\"PERIOD_DATE\"] = pd.to_datetime(df[\"PERIOD_DATE\"])\n",
    "        \n",
    "        # Join: CUSIP → ticker\n",
    "        df = df.merge(\n",
    "            ticker_map[[\"cusip\", \"ticker\"]],\n",
    "            left_on=\"CUSIP\",\n",
    "            right_on=\"cusip\",\n",
    "            how=\"left\"\n",
    "        ).drop(columns=[\"cusip\"])\n",
    "        \n",
    "        # Join: ticker + period → price\n",
    "        df = df.merge(\n",
    "            prices[[\"ticker\", \"period_start\", \"price\"]],\n",
    "            left_on=[\"ticker\", \"PERIOD_DATE\"],\n",
    "            right_on=[\"ticker\", \"period_start\"],\n",
    "            how=\"left\"\n",
    "        ).drop(columns=[\"period_start\"])\n",
    "        \n",
    "        # Compute VALUE\n",
    "        df[\"VALUE\"] = df[\"SSHPRNAMT\"] * df[\"price\"]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df[\"YEAR\"] = year\n",
    "        df[\"QUARTER\"] = f\"Q{quarter}_{year}\"\n",
    "        \n",
    "        # Final column order\n",
    "        df = df[[\"CIK\", \"CUSIP\", \"VALUE\", \"SSHPRNAMT\", \"PERIOD_DATE\", \"YEAR\", \"QUARTER\"]]\n",
    "        \n",
    "        # Save individual parquet file\n",
    "        out_path = os.path.join(\n",
    "            output_dir,\n",
    "            f\"holdings_processed_Q{quarter}_{year}.parquet\"\n",
    "        )\n",
    "        df.to_parquet(out_path, index=False)\n",
    "        print(f\"  ✓ Saved {len(df)} records to {out_path}\")\n",
    "        \n",
    "        # Add to list for combined file\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Combine all data into one parquet file\n",
    "if all_dfs:\n",
    "    print(\"\\nCombining all data into single parquet file...\")\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df = combined_df.sort_values(by=['PERIOD_DATE', 'CIK'])\n",
    "    \n",
    "    combined_path = os.path.join(output_dir, \"holdings_all_years_combined.parquet\")\n",
    "    combined_df.to_parquet(combined_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Combined parquet saved: {combined_path}\")\n",
    "    print(f\"  Total records: {len(combined_df):,}\")\n",
    "    print(f\"  Date range: {combined_df['PERIOD_DATE'].min()} to {combined_df['PERIOD_DATE'].max()}\")\n",
    "    print(f\"  Years: {sorted(combined_df['YEAR'].unique())}\")\n",
    "    print(f\"  Unique funds (CIK): {combined_df['CIK'].nunique():,}\")\n",
    "    print(f\"  Unique stocks (CUSIP): {combined_df['CUSIP'].nunique():,}\")\n",
    "else:\n",
    "    print(\"\\n✗ No data was processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd713ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after cleaning: 3996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfs = []\n",
    "for i, file in enumerate(file_paths):\n",
    "    df = pd.read_csv(file)\n",
    "    df['QUARTER'] = f'Q{i+1}_2018'\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "data = data[['CIK', 'CUSIP', 'VALUE', 'SSHPRNAMT', 'PERIOD_DATE', 'QUARTER']]\n",
    "data['PERIOD_DATE'] = pd.to_datetime(data['PERIOD_DATE'])\n",
    "data = data.dropna(subset=['CIK', 'CUSIP', 'VALUE'])\n",
    "data['CIK'] = data['CIK'].astype(str)\n",
    "data['CUSIP'] = data['CUSIP'].astype(str)\n",
    "data = data.sort_values(by='PERIOD_DATE')\n",
    "print(f'Total records after cleaning: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78fae4",
   "metadata": {},
   "source": [
    "## 3. Graph Construction\n",
    "Build bipartite and projected graphs for funds and stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ecbda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_and_features_up_to(max_date):\n",
    "    df_up_to = data[data['PERIOD_DATE'] <= max_date].copy()\n",
    "    \n",
    "    funds_up_to = df_up_to['CIK'].unique()\n",
    "    stocks_up_to = df_up_to['CUSIP'].unique()\n",
    "    \n",
    "    G_bip = nx.Graph()\n",
    "    G_bip.add_nodes_from(funds_up_to, bipartite=0)\n",
    "    G_bip.add_nodes_from(stocks_up_to, bipartite=1)\n",
    "    \n",
    "    edges = [(row.CIK, row.CUSIP, \n",
    "            {'value': row.VALUE, 'amount': row.SSHPRNAMT, 'time': row.PERIOD_DATE})\n",
    "            for row in df_up_to.itertuples(index=False)]\n",
    "    G_bip.add_edges_from(edges)\n",
    "        \n",
    "    # Fund-Fund projection with weights (weighted by shared stocks)\n",
    "    G_fund = bipartite.weighted_projected_graph(G_bip, funds_up_to)\n",
    "    \n",
    "    # Convert to directed based on time, only for existing edges\n",
    "    G_fund_directed = nx.DiGraph()\n",
    "    for u, v, data_dict in G_fund.edges(data=True):\n",
    "        shared = set(G_bip.neighbors(u)) & set(G_bip.neighbors(v))\n",
    "        if not shared:\n",
    "            continue\n",
    "        \n",
    "        times_u = [G_bip.edges[u,s]['time'] for s in shared]\n",
    "        times_v = [G_bip.edges[v,s]['time'] for s in shared]\n",
    "        avg_u = np.mean([t.timestamp() for t in times_u])\n",
    "        avg_v = np.mean([t.timestamp() for t in times_v])\n",
    "        \n",
    "        weight = data_dict.get('weight', 1)\n",
    "        \n",
    "        if avg_u < avg_v:\n",
    "            G_fund_directed.add_edge(u, v, weight=weight)\n",
    "        else:\n",
    "            G_fund_directed.add_edge(v, u, weight=weight)\n",
    "    \n",
    "    G_fund = G_fund_directed  # Replace with directed version\n",
    "    \n",
    "    # Topological features (על G_fund המכוון)\n",
    "    degree_cent = degree_centrality(G_fund)\n",
    "    pagerank_cent = nx.pagerank(G_fund)\n",
    "    hubs, authorities = hits(G_fund)\n",
    "    largest_cc = max(nx.connected_components(G_fund.to_undirected()), key=len)\n",
    "    closeness_cent = closeness_centrality(G_fund.to_undirected().subgraph(largest_cc))\n",
    "    \n",
    "    fund_features = pd.DataFrame({\n",
    "        'fund': list(G_fund.nodes()),\n",
    "        'degree': [degree_cent.get(n, 0) for n in G_fund.nodes()],\n",
    "        'pagerank': [pagerank_cent.get(n, 0) for n in G_fund.nodes()],\n",
    "        'hub': [hubs.get(n, 0) for n in G_fund.nodes()],\n",
    "        'authority': [authorities.get(n, 0) for n in G_fund.nodes()],\n",
    "        'closeness': [closeness_cent.get(n, 0) for n in G_fund.nodes()]\n",
    "    }).set_index('fund')\n",
    "    \n",
    "    # Community (Leiden)\n",
    "    vertex_names = list(G_fund.nodes())\n",
    "    vertex_to_idx = {v: i for i, v in enumerate(vertex_names)}\n",
    "\n",
    "    # המר edges לאינדקסים\n",
    "    edge_list = [(vertex_to_idx[u], vertex_to_idx[v]) \n",
    "                for u, v in G_fund.to_undirected().edges()]\n",
    "\n",
    "    ig_G = ig.Graph(n=len(vertex_names), edges=edge_list)\n",
    "    ig_G.vs['_nx_name'] = vertex_names\n",
    "    partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "    communities = {ig_G.vs[i]['_nx_name']: p for p, cl in enumerate(partition) for i in cl}\n",
    "    fund_features['community'] = fund_features.index.map(communities).fillna(-1)\n",
    "    \n",
    "    return G_bip, G_fund, fund_features, df_up_to, funds_up_to, stocks_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6970f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full graph and features for all data (for prediction on any fund)\n",
    "full_max_date = data['PERIOD_DATE'].max()\n",
    "G_full, G_fund_full, fund_features_full, df_full, funds_full, stocks_full = build_graph_and_features_up_to(full_max_date)\n",
    "fund_idx_full = {f: i for i, f in enumerate(funds_full)}\n",
    "stock_idx_full = {s: i for i, s in enumerate(stocks_full)}\n",
    "# For Q4-only funds (unbiased prediction)\n",
    "q4_min_date = data[data['QUARTER'] == 'Q4_2018']['PERIOD_DATE'].min()\n",
    "q4_max_date = data[data['QUARTER'] == 'Q4_2018']['PERIOD_DATE'].max()\n",
    "funds_q4 = set(data[(data['PERIOD_DATE'] >= q4_min_date) & (data['PERIOD_DATE'] <= q4_max_date)]['CIK'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe222b13",
   "metadata": {},
   "source": [
    "## 4. Training Phase (Up to Q3)\n",
    "Split the data temporally and prepare for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28204d90",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7cdaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipartite Graph: 2850 edges\n",
      "Fund-Fund Graph: 36 edges\n"
     ]
    }
   ],
   "source": [
    "# Use only up to Q2 for training (temporal split)\n",
    "train_max_date = pd.to_datetime('2018-06-30')  # End of Q2\n",
    "G_bip_train, G_fund_train, fund_features_train, df_train, funds_train, stocks_train = build_graph_and_features_up_to(train_max_date)\n",
    "fund_idx_train = {f: i for i, f in enumerate(funds_train)}\n",
    "stock_idx_train = {s: i for i, s in enumerate(stocks_train)}\n",
    "print(f\"Bipartite Graph: {G_bip_train.number_of_edges()} edges\")\n",
    "print(f\"Fund-Fund Graph: {G_fund_train.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f52390",
   "metadata": {},
   "source": [
    "## 5. GraphSAGE Embedding Training\n",
    "Train GraphSAGE on the training graph to generate node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c29f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features initialized on cpu\n",
      "Edge index initialized on cpu\n",
      "Model initialized on cpu\n",
      "Starting training on cpu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3611\n",
      "Epoch 10, Loss: 0.0000\n",
      "Early stopping at epoch 11\n",
      "GraphSAGE training completed on cpu.\n",
      "dynamic_emb_train shape: (9, 8)\n",
      "stock_emb_train shape: (1891, 8)\n"
     ]
    }
   ],
   "source": [
    "# Node features - use simple random vectors (can be improved)\n",
    "num_nodes = len(funds_train) + len(stocks_train)\n",
    "\n",
    "# Try to move to GPU, fallback to CPU if error\n",
    "try:\n",
    "    x = torch.randn(num_nodes, 16).to(device)\n",
    "    print(f\"Node features initialized on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving to GPU, using CPU: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "    x = torch.randn(num_nodes, 16).to(device)\n",
    "\n",
    "# Map indices for homogeneous graph (train only)\n",
    "edge_index_homo = []\n",
    "for u, v in G_bip_train.edges():\n",
    "    u_idx = fund_idx_train[u] if u in fund_idx_train else stock_idx_train.get(u, -1)\n",
    "    v_idx = fund_idx_train[v] if v in fund_idx_train else stock_idx_train.get(v, -1)\n",
    "    if u_idx != -1 and v_idx != -1:\n",
    "        edge_index_homo.append([u_idx, v_idx])\n",
    "        edge_index_homo.append([v_idx, u_idx])  # undirected\n",
    "\n",
    "try:\n",
    "    edge_index_homo = torch.tensor(edge_index_homo, dtype=torch.long).t().contiguous().to(device)\n",
    "    print(f\"Edge index initialized on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving edge index to GPU, using CPU: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "    x = x.cpu()\n",
    "    edge_index_homo = torch.tensor(edge_index_homo, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "# Define GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "try:\n",
    "    model = GraphSAGE(16, 32, 8).to(device)\n",
    "    print(f\"Model initialized on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving model to GPU, using CPU: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "    x = x.cpu()\n",
    "    edge_index_homo = edge_index_homo.cpu()\n",
    "    model = GraphSAGE(16, 32, 8).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train GraphSAGE with early stopping\n",
    "model.train()\n",
    "prev_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"Starting training on {device}...\")\n",
    "for epoch in range(50):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, edge_index_homo)\n",
    "        # Loss on existing edges (link prediction style)\n",
    "        pos_score = (out[edge_index_homo[0]] * out[edge_index_homo[1]]).sum(dim=1).sigmoid()\n",
    "        loss = -torch.log(pos_score + 1e-15).mean()  # add epsilon to avoid log(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if abs(prev_loss - loss.item()) < 1e-6:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        else:\n",
    "            no_improve = 0\n",
    "        prev_loss = loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA' in str(e):\n",
    "            print(f\"CUDA error during training, switching to CPU: {e}\")\n",
    "            device = torch.device('cpu')\n",
    "            x = x.cpu()\n",
    "            edge_index_homo = edge_index_homo.cpu()\n",
    "            model = model.cpu()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            print(\"Restarting training on CPU...\")\n",
    "            epoch = 0\n",
    "            prev_loss = float('inf')\n",
    "            no_improve = 0\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Extract embeddings and move to CPU for further processing\n",
    "with torch.no_grad():\n",
    "    emb = model(x, edge_index_homo).cpu().numpy()\n",
    "\n",
    "# Split embeddings for funds and stocks\n",
    "dynamic_emb_train = emb[:len(funds_train)]      # shape: [len(funds_train), 8]\n",
    "stock_emb_train = emb[len(funds_train):]        # shape: [len(stocks_train), 8]\n",
    "\n",
    "print(f\"GraphSAGE training completed on {device}.\")\n",
    "print(f\"dynamic_emb_train shape: {dynamic_emb_train.shape}\")\n",
    "print(f\"stock_emb_train shape: {stock_emb_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c5bec",
   "metadata": {},
   "source": [
    "## 6. Save Training Artifacts\n",
    "Save embeddings, features, and model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1b16ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cache trained artifacts (run ONCE) ───────────────────────────────────────\n",
    "GRAPH_READY = True\n",
    "_cached_graph = G_bip_train\n",
    "_cached_embeddings = stock_emb_train\n",
    "_cached_model = model.cpu()  # Move model to CPU for caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b398ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to artifacts/ directory.\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings, model, and graph to files for later use\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a directory for artifacts if it doesn't exist\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "# Save stock embeddings\n",
    "np.save('artifacts/stock_emb_train.npy', stock_emb_train)\n",
    "\n",
    "# Save dynamic fund embeddings\n",
    "np.save('artifacts/dynamic_emb_train.npy', dynamic_emb_train)\n",
    "\n",
    "# Save fund features\n",
    "fund_features_train.to_pickle('artifacts/fund_features_train.pkl')\n",
    "\n",
    "# Save LightGBM model (bst) if it exists\n",
    "if 'bst' in globals():\n",
    "    joblib.dump(bst, 'artifacts/lightgbm_bst.pkl')\n",
    "\n",
    "# Save funds and stocks lists\n",
    "with open('artifacts/funds_train.pkl', 'wb') as f:\n",
    "    pickle.dump(funds_train, f)\n",
    "with open('artifacts/stocks_train.pkl', 'wb') as f:\n",
    "    pickle.dump(stocks_train, f)\n",
    "\n",
    "print('Artifacts saved to artifacts/ directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e803f53",
   "metadata": {},
   "source": [
    "## 7. Test Phase (Q4)\n",
    "Prepare test data and features for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f253cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3803 positive and 357 negative test samples\n",
      "Test set positive ratio: 0.914 (1=positive, 0=negative)\n"
     ]
    }
   ],
   "source": [
    "# Test Phase (Q3) - Using only funds and stocks seen in train (Q1-Q2)\n",
    "test_min_date = data[data['QUARTER'] == 'Q3_2018']['PERIOD_DATE'].min()\n",
    "test_max_date = data[data['QUARTER'] == 'Q3_2018']['PERIOD_DATE'].max()\n",
    "test_data = data[(data['PERIOD_DATE'] >= test_min_date) & (data['PERIOD_DATE'] <= test_max_date)]\n",
    "# Only use funds and stocks that were seen in train\n",
    "funds_test = set(test_data['CIK'].unique()) & set(funds_train)\n",
    "stocks_test = set(test_data['CUSIP'].unique()) & set(stocks_train)\n",
    "# positive edges: רק Q3, אבל רק עבור funds/stocks שמופיעים ב-train\n",
    "pos_edges_test = []\n",
    "for row in test_data.itertuples(index=False):\n",
    "    cik = row.CIK\n",
    "    cusip = row.CUSIP\n",
    "    if cik in funds_test and cusip in stocks_test:\n",
    "        pos_edges_test.append((fund_idx_train[cik], stock_idx_train[cusip], 1))\n",
    "# negative sampling: רק על embeddings של train (שכבר חושבו ב-Cell 4!)\n",
    "stock_sim_train = cosine_similarity(stock_emb_train)\n",
    "fund_to_connected = {f_idx: [stock_idx_train[s] for s in G_bip_train.neighbors(fund) if s in stocks_test and s in stock_idx_train]\n",
    "                     for f_idx, fund in enumerate(funds_train) if fund in funds_test}\n",
    "neg_edges_test = []\n",
    "for f_idx in range(len(funds_train)):\n",
    "    if funds_train[f_idx] not in funds_test:\n",
    "        continue\n",
    "    connected = fund_to_connected.get(f_idx, [])\n",
    "    if not connected:\n",
    "        continue\n",
    "    sim_scores = stock_sim_train[connected].mean(axis=0)\n",
    "    hard_negs = np.argsort(-sim_scores)[len(connected):len(connected)+50]\n",
    "    neg_edges_test.extend([(f_idx, neg_idx, 0) for neg_idx in hard_negs if neg_idx not in connected and neg_idx < len(stocks_train) and stocks_train[neg_idx] in stocks_test])\n",
    "neg_edges_test = neg_edges_test[:len(pos_edges_test)]\n",
    "# Link features\n",
    "link_data = []\n",
    "for f_i, s_i, label in pos_edges_test + neg_edges_test:\n",
    "    fund_id = funds_train[f_i]\n",
    "    feat = np.concatenate([\n",
    "        dynamic_emb_train[f_i],\n",
    "        stock_emb_train[s_i],\n",
    "        fund_features_train.loc[fund_id, ['degree', 'pagerank', 'hub', 'authority', 'closeness', 'community']].values\n",
    "    ])\n",
    "    link_data.append((feat, label))\n",
    "X = np.array([d[0] for d in link_data])\n",
    "y = np.array([d[1] for d in link_data])\n",
    "print(f\"Created {len(pos_edges_test)} positive and {len(neg_edges_test)} negative test samples\")\n",
    "print(f\"Test set positive ratio: {np.mean(y):.3f} (1=positive, 0=negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a64d94",
   "metadata": {},
   "source": [
    "## 8. LightGBM Training & Evaluation\n",
    "Train and evaluate the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9f60ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC mean: nan (±nan)\n",
      "CV Precision mean: 0.8970 (±0.2061)\n",
      "Final LightGBM model trained and saved to artifacts/lightgbm_bst.pkl.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# LightGBM parameters (define before use)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train/test/final model only ONCE!\n",
    "if 'bst' not in globals():\n",
    "    tscv = TimeSeriesSplit(n_splits=5)  # 5 folds, expanding train set each time\n",
    "    cv_aucs = cross_val_score(lgb.LGBMClassifier(**params), X, y, cv=tscv, scoring='roc_auc')\n",
    "    print(f\"CV AUC mean: {cv_aucs.mean():.4f} (±{cv_aucs.std():.4f})\")\n",
    "    precisions = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X[train_idx], X[test_idx]\n",
    "        y_train_cv, y_test_cv = y[train_idx], y[test_idx]\n",
    "        train_data_cv = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        test_data_cv = lgb.Dataset(X_test_cv, label=y_test_cv, reference=train_data_cv)\n",
    "        bst_cv = lgb.train(params, train_data_cv, num_boost_round=100, valid_sets=[test_data_cv])\n",
    "        y_pred_cv = bst_cv.predict(X_test_cv)\n",
    "        precision_cv = precision_score(y_test_cv, (y_pred_cv > 0.5).astype(int))\n",
    "        precisions.append(precision_cv)\n",
    "    print(f\"CV Precision mean: {np.mean(precisions):.4f} (±{np.std(precisions):.4f})\")\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    bst = lgb.train(params, train_data, num_boost_round=100)\n",
    "    # --- Save model to artifacts folder ---\n",
    "    os.makedirs('artifacts', exist_ok=True)\n",
    "    joblib.dump(bst, os.path.join('artifacts', 'lightgbm_bst.pkl'))\n",
    "    print('Final LightGBM model trained and saved to artifacts/lightgbm_bst.pkl.')\n",
    "else:\n",
    "    print('LightGBM model already trained and loaded in memory. Skipping retraining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed5dca",
   "metadata": {},
   "source": [
    "## 9. Prediction Function\n",
    "Function to predict top stocks for a given fund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_portfolio(fund_id):\n",
    "    \"\"\"\n",
    "    Efficiently predict top stocks for a given fund_id using precomputed artifacts.\n",
    "    Assumes all heavy computations (embeddings, features, model) are preloaded and cached.\n",
    "    \"\"\"\n",
    "    global bst, dynamic_emb_train, stock_emb_train, fund_features_train, funds_train, stocks_train\n",
    "    if 'bst' not in globals() or 'dynamic_emb_train' not in globals() or 'stock_emb_train' not in globals():\n",
    "        raise RuntimeError(\"Artifacts not loaded. Please run the training cells or load artifacts.\")\n",
    "    fund_idx = {f: i for i, f in enumerate(funds_train)}\n",
    "    stock_list = stocks_train\n",
    "    if fund_id not in fund_idx:\n",
    "        print(f\"Fund with CIK {fund_id} not found in the data.\")\n",
    "        return []\n",
    "    f_idx = fund_idx[fund_id]\n",
    "    fund_id_str = funds_train[f_idx]\n",
    "    fund_emb_repeat = np.tile(dynamic_emb_train[f_idx], (len(stock_list), 1))\n",
    "    fund_topo_repeat = np.tile(fund_features_train.loc[fund_id_str, ['degree', 'pagerank', 'hub', 'authority', 'closeness', 'community']].values, (len(stock_list), 1))\n",
    "    feats = np.concatenate([fund_emb_repeat, stock_emb_train, fund_topo_repeat], axis=1)\n",
    "    preds = bst.predict(feats)\n",
    "    stock_preds = list(zip(stock_list, preds))\n",
    "    top_stocks = sorted(stock_preds, key=lambda x: x[1], reverse=True)[:5]\n",
    "    return top_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c77bb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded successfully. You can now run predictions without retraining.\n"
     ]
    }
   ],
   "source": [
    "# Load all artifacts (embeddings, features, model) for fast prediction\n",
    "# (imports moved to the first cell)\n",
    "\n",
    "artifacts_path = 'artifacts'\n",
    "\n",
    "def load_artifacts():\n",
    "    global stock_emb_train, dynamic_emb_train, fund_features_train, bst, funds_train, stocks_train\n",
    "    if os.path.exists(artifacts_path):\n",
    "        try:\n",
    "            stock_emb_train = np.load(os.path.join(artifacts_path, 'stock_emb_train.npy'))\n",
    "            dynamic_emb_train = np.load(os.path.join(artifacts_path, 'dynamic_emb_train.npy'))\n",
    "            fund_features_train = pd.read_pickle(os.path.join(artifacts_path, 'fund_features_train.pkl'))\n",
    "            bst = joblib.load(os.path.join(artifacts_path, 'lightgbm_bst.pkl'))\n",
    "            with open(os.path.join(artifacts_path, 'funds_train.pkl'), 'rb') as f:\n",
    "                funds_train = pickle.load(f)\n",
    "            with open(os.path.join(artifacts_path, 'stocks_train.pkl'), 'rb') as f:\n",
    "                stocks_train = pickle.load(f)\n",
    "            print('Artifacts loaded successfully. You can now run predictions without retraining.')\n",
    "        except Exception as e:\n",
    "            print('Failed to load artifacts:', e)\n",
    "    else:\n",
    "        print('Artifacts directory not found. Please run the training cells first.')\n",
    "\n",
    "# Load artifacts at notebook startup for fast prediction\n",
    "load_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919d110",
   "metadata": {},
   "source": [
    "## 10. Predict for a Specific Fund\n",
    "Example: Predict for a random out-of-sample fund."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd2e45",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">\n",
    "> - To make an unbiased next-quarter prediction, you must use only funds that were seen in training (Q1-Q2) and predict their holdings in Q4.\n",
    "> - Predicting for funds that were not seen in training is not possible (no embeddings/features for them).\n",
    "> - Predicting for funds using their Q4 data in training or feature construction would cause data leakage and bias.\n",
    "> - The code below ensures you only predict for eligible funds, with no leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b704e9e",
   "metadata": {},
   "source": [
    "## 10.1. List Eligible Funds for Next-Quarter Prediction\n",
    "Print all funds that were seen in training (Q1-Q2) and also appear in Q4. These are the only funds for which you can make an unbiased next-quarter prediction (no data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0f5c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible funds for Q4 prediction: 9\n",
      "Sample of eligible funds (first 20):\n",
      "['1021249', '1325091', '1345929', '1424116', '1535839', '1576102', '1623678', '1694461', '1697457']\n"
     ]
    }
   ],
   "source": [
    "# List funds eligible for next-quarter (Q4) prediction: must be seen in train (Q1-Q2) and appear in Q4\n",
    "if 'funds_train' in globals() and 'funds_q4' in globals():\n",
    "    eligible_funds = sorted(list(set(funds_train) & set(funds_q4)))\n",
    "    print(f\"Number of eligible funds for Q4 prediction: {len(eligible_funds)}\")\n",
    "    print(\"Sample of eligible funds (first 20):\")\n",
    "    print(eligible_funds[:20])\n",
    "    # You can select any fund from this list for next-quarter prediction (no leakage, no bias)\n",
    "else:\n",
    "    print('Required fund lists not found. Please ensure all previous cells have been run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3d8e2",
   "metadata": {},
   "source": [
    "## 10.1. Random/Specific Fund Next-Quarter Prediction\n",
    "To choode specific FUND from the list, update the var *fund_id_to_predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f391ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected eligible fund for Q4 prediction: 1535839\n",
      "Top recommended stocks for Q4:\n",
      "  880349105: 0.9561\n",
      "  67555N206: 0.9561\n",
      "  67420T206: 0.9561\n",
      "  85570W100: 0.9561\n",
      "  983134107: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Example: Predict for a specific or random eligible fund (seen in train, predict Q4)\n",
    "fund_id_to_predict = '1535839'  # Set to a specific CIK string to choose a fund, or leave as None for random\n",
    "fund_list_for_prediction = eligible_funds if 'eligible_funds' in globals() and len(eligible_funds) > 0 else []\n",
    "if len(fund_list_for_prediction) > 0:\n",
    "    if fund_id_to_predict is not None and str(fund_id_to_predict) in fund_list_for_prediction:\n",
    "        selected_fund = str(fund_id_to_predict)\n",
    "        print(f'Selected eligible fund for Q4 prediction: {selected_fund}')\n",
    "    else:\n",
    "        selected_fund = random.choice(fund_list_for_prediction)\n",
    "        print(f'Randomly selected eligible fund for Q4 prediction: {selected_fund}')\n",
    "    top_stocks = predict_portfolio(selected_fund)\n",
    "    print('Top recommended stocks for Q4:')\n",
    "    for stock, score in top_stocks:\n",
    "        print(f'  {stock}: {score:.4f}')\n",
    "else:\n",
    "    print('No eligible funds available for prediction. Please check your data and artifacts.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

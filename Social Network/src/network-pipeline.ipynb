{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd74b1",
   "metadata": {},
   "source": [
    "# Stock Market Social Network - Temporal Link Prediction Pipeline\n",
    "\n",
    "Quarterly Incremental (Online) Link Prediction on fund-stock holdings (2021-2024).\n",
    "\n",
    "**Key Features:**\n",
    "- Load holdings data 2021-2024 only\n",
    "- Build separate bipartite graph per quarter\n",
    "- Incremental training: train on current quarter, test on next quarter\n",
    "- Model weights never reset (quarterly online learning)\n",
    "- Strict temporal causality (no future information leakage)\n",
    "- Per-quarter evaluation metrics (AUC, Precision, Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894d66e",
   "metadata": {},
   "source": [
    "## 1. Data Setup and Loading\n",
    "Load quarterly holdings data from processed parquet files (2021-2024 only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc97145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Status: CUDA compatible\n",
      "GPU: NVIDIA GeForce GTX 1080 Ti | CUDA: 11.8\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.algorithms.centrality import degree_centrality, closeness_centrality\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
    "from networkx.algorithms.link_analysis.hits_alg import hits\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Setup\n",
    "def check_cuda_compatibility():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False, \"CUDA not available\"\n",
    "    try:\n",
    "        test_tensor = torch.zeros(1).cuda()\n",
    "        test_tensor = test_tensor + 1\n",
    "        return True, \"CUDA compatible\"\n",
    "    except Exception as e:\n",
    "        return False, f\"CUDA compatibility issue: {str(e)}\"\n",
    "\n",
    "cuda_compatible, cuda_message = check_cuda_compatibility()\n",
    "print(f\"CUDA Status: {cuda_message}\")\n",
    "\n",
    "if cuda_compatible:\n",
    "    device = torch.device('cuda')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} | CUDA: {torch.version.cuda}')\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU (GPU not available)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4587eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files\n",
      "Output directory: /home/zenoua/Social-Network-Stock-Market/Social Network/parquuet_files/generated_combined_parquet\n",
      "✓ Ticker map: (4571, 5)\n",
      "✓ Prices: (158011, 4)\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "personal_dir = os.path.expanduser('~')\n",
    "root = os.path.join(personal_dir, 'Social-Network-Stock-Market/Social Network/parquuet_files')\n",
    "output_dir = os.path.join(root, 'generated_combined_parquet')\n",
    "\n",
    "print(f\"Data directory: {root}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Load reference data\n",
    "ticker_map = pd.read_parquet(f\"{root}/ticker_to_cusip.parquet\")\n",
    "prices = pd.read_parquet(f\"{root}/ticker_prices.parquet\")\n",
    "ticker_map[\"cusip\"] = ticker_map[\"cusip\"].astype(str)\n",
    "prices[\"period_start\"] = pd.to_datetime(prices[\"period_start\"])\n",
    "\n",
    "print(f\"✓ Ticker map: {ticker_map.shape}\")\n",
    "print(f\"✓ Prices: {prices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d302fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading quarterly holdings data (2021-2024)...\n",
      "================================================================================\n",
      "  ✓ Loaded holdings_processed_Q1_2014.parquet: 1,017,686 records (Q1_2014)\n",
      "  ✓ Loaded holdings_processed_Q1_2015.parquet: 1,119,642 records (Q1_2015)\n",
      "  ✓ Loaded holdings_processed_Q1_2016.parquet: 1,131,465 records (Q1_2016)\n",
      "  ✓ Loaded holdings_processed_Q1_2017.parquet: 1,190,709 records (Q1_2017)\n",
      "  ✓ Loaded holdings_processed_Q1_2018.parquet: 1,294,867 records (Q1_2018)\n",
      "  ✓ Loaded holdings_processed_Q1_2019.parquet: 1,337,369 records (Q1_2019)\n",
      "  ✓ Loaded holdings_processed_Q1_2020.parquet: 1,304,800 records (Q1_2020)\n",
      "  ✓ Loaded holdings_processed_Q1_2021.parquet: 1,425,771 records (Q1_2021)\n",
      "  ✓ Loaded holdings_processed_Q1_2022.parquet: 1,557,935 records (Q1_2022)\n",
      "  ✓ Loaded holdings_processed_Q1_2023.parquet: 1,615,015 records (Q1_2023)\n",
      "  ✓ Loaded holdings_processed_Q1_2024.parquet: 1,736,830 records (Q1_2024)\n",
      "  ✓ Loaded holdings_processed_Q1_2025.parquet: 1,878,074 records (Q1_2025)\n",
      "  ✓ Loaded holdings_processed_Q2_2013.parquet: 19,659 records (Q2_2013)\n",
      "  ✓ Loaded holdings_processed_Q2_2014.parquet: 1,060,398 records (Q2_2014)\n",
      "  ✓ Loaded holdings_processed_Q2_2015.parquet: 1,095,826 records (Q2_2015)\n",
      "  ✓ Loaded holdings_processed_Q2_2016.parquet: 1,127,160 records (Q2_2016)\n",
      "  ✓ Loaded holdings_processed_Q2_2017.parquet: 1,169,387 records (Q2_2017)\n",
      "  ✓ Loaded holdings_processed_Q2_2018.parquet: 1,299,201 records (Q2_2018)\n",
      "  ✓ Loaded holdings_processed_Q2_2019.parquet: 1,344,696 records (Q2_2019)\n",
      "  ✓ Loaded holdings_processed_Q2_2020.parquet: 1,307,037 records (Q2_2020)\n",
      "  ✓ Loaded holdings_processed_Q2_2021.parquet: 1,470,761 records (Q2_2021)\n",
      "  ✓ Loaded holdings_processed_Q2_2022.parquet: 1,561,596 records (Q2_2022)\n",
      "  ✓ Loaded holdings_processed_Q2_2023.parquet: 1,586,359 records (Q2_2023)\n",
      "  ✓ Loaded holdings_processed_Q2_2024.parquet: 1,771,738 records (Q2_2024)\n",
      "  ✓ Loaded holdings_processed_Q2_2025.parquet: 1,822,141 records (Q2_2025)\n",
      "  ✓ Loaded holdings_processed_Q3_2013.parquet: 34,213 records (Q3_2013)\n",
      "  ✓ Loaded holdings_processed_Q3_2014.parquet: 1,049,697 records (Q3_2014)\n",
      "  ✓ Loaded holdings_processed_Q3_2015.parquet: 1,081,637 records (Q3_2015)\n",
      "  ✓ Loaded holdings_processed_Q3_2016.parquet: 1,117,857 records (Q3_2016)\n",
      "  ✓ Loaded holdings_processed_Q3_2017.parquet: 1,131,807 records (Q3_2017)\n",
      "  ✓ Loaded holdings_processed_Q3_2018.parquet: 1,356,314 records (Q3_2018)\n",
      "  ✓ Loaded holdings_processed_Q3_2019.parquet: 1,308,113 records (Q3_2019)\n",
      "  ✓ Loaded holdings_processed_Q3_2020.parquet: 1,309,991 records (Q3_2020)\n",
      "  ✓ Loaded holdings_processed_Q3_2021.parquet: 1,484,007 records (Q3_2021)\n",
      "  ✓ Loaded holdings_processed_Q3_2022.parquet: 1,514,464 records (Q3_2022)\n",
      "  ✓ Loaded holdings_processed_Q3_2023.parquet: 1,658,916 records (Q3_2023)\n",
      "  ✓ Loaded holdings_processed_Q3_2024.parquet: 1,764,953 records (Q3_2024)\n",
      "  ✓ Loaded holdings_processed_Q4_2013.parquet: 1,018,601 records (Q4_2013)\n",
      "  ✓ Loaded holdings_processed_Q4_2014.parquet: 1,093,267 records (Q4_2014)\n",
      "  ✓ Loaded holdings_processed_Q4_2015.parquet: 1,115,302 records (Q4_2015)\n",
      "  ✓ Loaded holdings_processed_Q4_2016.parquet: 1,170,982 records (Q4_2016)\n",
      "  ✓ Loaded holdings_processed_Q4_2017.parquet: 1,252,888 records (Q4_2017)\n",
      "  ✓ Loaded holdings_processed_Q4_2018.parquet: 1,275,091 records (Q4_2018)\n",
      "  ✓ Loaded holdings_processed_Q4_2019.parquet: 1,353,353 records (Q4_2019)\n",
      "  ✓ Loaded holdings_processed_Q4_2020.parquet: 1,456,047 records (Q4_2020)\n",
      "  ✓ Loaded holdings_processed_Q4_2021.parquet: 1,564,186 records (Q4_2021)\n",
      "  ✓ Loaded holdings_processed_Q4_2022.parquet: 1,612,235 records (Q4_2022)\n",
      "  ✓ Loaded holdings_processed_Q4_2023.parquet: 1,714,038 records (Q4_2023)\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Total records (2021-2024): 24,038,804\n",
      "Date range: 2021-01-01 00:00:00 to 2024-07-01 00:00:00\n",
      "Years: [np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024)]\n",
      "Unique funds (CIK): 8,637\n",
      "Unique stocks (CUSIP): 3,982\n"
     ]
    }
   ],
   "source": [
    "# Load all processed quarterly holdings files\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading quarterly holdings data (2021-2024)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "combined_files = sorted([f for f in os.listdir(output_dir) \n",
    "                        if f.startswith('holdings_processed_') and f.endswith('.parquet')])\n",
    "\n",
    "if not combined_files:\n",
    "    print(\"ERROR: No processed files found. Check output_dir path.\")\n",
    "else:\n",
    "    all_dfs = []\n",
    "    for file in combined_files:\n",
    "        df_temp = pd.read_parquet(os.path.join(output_dir, file))\n",
    "        year = df_temp['YEAR'].iloc[0]\n",
    "        quarter_str = df_temp['QUARTER'].iloc[0]\n",
    "        print(f\"  ✓ Loaded {file}: {len(df_temp):,} records ({quarter_str})\")\n",
    "        all_dfs.append(df_temp)\n",
    "    \n",
    "    data = pd.concat(all_dfs, ignore_index=True)\n",
    "    data['PERIOD_DATE'] = pd.to_datetime(data['PERIOD_DATE'])\n",
    "    \n",
    "    # Filter to 2021-2024 ONLY\n",
    "    data = data[(data['YEAR'] >= 2021) & (data['YEAR'] <= 2024)].copy()\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(f\"Total records (2021-2024): {len(data):,}\")\n",
    "    print(f\"Date range: {data['PERIOD_DATE'].min()} to {data['PERIOD_DATE'].max()}\")\n",
    "    print(f\"Years: {sorted(data['YEAR'].unique())}\")\n",
    "    print(f\"Unique funds (CIK): {data['CIK'].nunique():,}\")\n",
    "    print(f\"Unique stocks (CUSIP): {data['CUSIP'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361698cf",
   "metadata": {},
   "source": [
    "## 2. Quarterly Graph Construction\n",
    "Build separate bipartite graphs for each quarter from 2021-2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1df2ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building quarterly bipartite graphs (2021-2024 only)...\n",
      "  2021 Q1: 5,773 funds, 3,544 stocks, 943,803 edges\n",
      "  2021 Q2: 5,803 funds, 3,606 stocks, 961,813 edges\n",
      "  2021 Q3: 5,786 funds, 3,542 stocks, 967,133 edges\n",
      "  2021 Q4: 6,598 funds, 3,341 stocks, 1,032,260 edges\n",
      "  2022 Q1: 6,586 funds, 3,341 stocks, 1,028,454 edges\n",
      "  2022 Q2: 6,546 funds, 3,308 stocks, 998,793 edges\n",
      "  2022 Q3: 6,529 funds, 3,285 stocks, 988,140 edges\n",
      "  2022 Q4: 6,762 funds, 3,255 stocks, 1,024,286 edges\n",
      "  2023 Q1: 6,673 funds, 3,227 stocks, 1,016,406 edges\n",
      "  2023 Q2: 6,704 funds, 3,225 stocks, 1,020,601 edges\n",
      "  2023 Q3: 6,704 funds, 3,202 stocks, 1,020,311 edges\n",
      "  2023 Q4: 7,082 funds, 3,179 stocks, 1,074,087 edges\n",
      "  2024 Q1: 7,072 funds, 3,183 stocks, 1,094,025 edges\n",
      "  2024 Q2: 7,056 funds, 3,164 stocks, 1,100,224 edges\n",
      "  2024 Q3: 7,023 funds, 3,123 stocks, 1,118,543 edges\n",
      "\n",
      "Total quarters: 15\n",
      "Date range: (np.int64(2021), 1) to (np.int64(2024), 3)\n"
     ]
    }
   ],
   "source": [
    "def build_quarterly_graphs(data):\n",
    "    \"\"\"\n",
    "    Build separate bipartite graphs for each quarter.\n",
    "    Data should already be filtered to desired time range.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with columns [CIK, CUSIP, VALUE, SSHPRNAMT, PERIOD_DATE, YEAR, QUARTER]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary: {(year, quarter): bipartite_graph}\n",
    "    \"\"\"\n",
    "    quarterly_graphs = {}\n",
    "    \n",
    "    # Group by YEAR and extract quarter from QUARTER column\n",
    "    for (year, quarter_str), group in data.groupby(['YEAR', 'QUARTER']):\n",
    "        quarter = int(quarter_str.split('_')[0][1])  # Extract Q number from \"Q1_2020\"\n",
    "        \n",
    "        funds = group['CIK'].unique()\n",
    "        stocks = group['CUSIP'].unique()\n",
    "        \n",
    "        # Build bipartite graph\n",
    "        G_bip = nx.Graph()\n",
    "        G_bip.add_nodes_from(funds, bipartite=0, node_type='fund')\n",
    "        G_bip.add_nodes_from(stocks, bipartite=1, node_type='stock')\n",
    "        \n",
    "        # Add edges with VALUE weight\n",
    "        edges = [\n",
    "            (row.CIK, row.CUSIP, {'value': row.VALUE, 'amount': row.SSHPRNAMT})\n",
    "            for row in group.itertuples(index=False)\n",
    "        ]\n",
    "        G_bip.add_edges_from(edges)\n",
    "        \n",
    "        quarterly_graphs[(year, quarter)] = G_bip\n",
    "        print(f\"  {year} Q{quarter}: {len(funds):,} funds, {len(stocks):,} stocks, {G_bip.number_of_edges():,} edges\")\n",
    "    \n",
    "    return quarterly_graphs\n",
    "\n",
    "print(\"Building quarterly bipartite graphs (2021-2024 only)...\")\n",
    "quarterly_graphs = build_quarterly_graphs(data)\n",
    "print(f\"\\nTotal quarters: {len(quarterly_graphs)}\")\n",
    "if quarterly_graphs:\n",
    "    min_q, max_q = min(quarterly_graphs.keys()), max(quarterly_graphs.keys())\n",
    "    print(f\"Date range: {min_q} to {max_q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d7203",
   "metadata": {},
   "source": [
    "## 3. Graph Features: Centrality & Community Detection\n",
    "Compute topological features from current quarter graph only (no future leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f6402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature computation function ready.\n"
     ]
    }
   ],
   "source": [
    "def compute_fund_features(G_bip, funds):\n",
    "    \"\"\"\n",
    "    Compute topological features for funds from bipartite graph.\n",
    "    Features computed only from G_bip (no future information).\n",
    "    \n",
    "    Args:\n",
    "        G_bip: Bipartite graph (fund-stock holdings)\n",
    "        funds: List of fund CIKs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with features: degree, pagerank, hub, authority, closeness, community\n",
    "    \"\"\"\n",
    "    if len(funds) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Project to fund-fund graph (shared stock holdings)\n",
    "    try:\n",
    "        G_fund = bipartite.weighted_projected_graph(G_bip, funds)\n",
    "    except:\n",
    "        G_fund = nx.Graph()\n",
    "        G_fund.add_nodes_from(funds)\n",
    "    \n",
    "    # Centrality metrics\n",
    "    degree_cent = degree_centrality(G_fund) if G_fund.number_of_nodes() > 0 else {}\n",
    "    pagerank_cent = nx.pagerank(G_fund) if G_fund.number_of_nodes() > 0 else {}\n",
    "    \n",
    "    try:\n",
    "        hubs, authorities = hits(G_fund)\n",
    "    except:\n",
    "        hubs = {f: 0 for f in funds}\n",
    "        authorities = {f: 0 for f in funds}\n",
    "    \n",
    "    # Closeness on largest component\n",
    "    closeness_cent = {}\n",
    "    if G_fund.number_of_nodes() > 0:\n",
    "        try:\n",
    "            comps = list(nx.connected_components(G_fund))\n",
    "            if comps:\n",
    "                largest_cc = max(comps, key=len)\n",
    "                closeness_cent = closeness_centrality(G_fund.subgraph(largest_cc))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Community detection (Leiden algorithm)\n",
    "    communities = {}\n",
    "    if G_fund.number_of_nodes() > 1:\n",
    "        try:\n",
    "            vertex_names = list(G_fund.nodes())\n",
    "            vertex_to_idx = {v: i for i, v in enumerate(vertex_names)}\n",
    "            edge_list = [(vertex_to_idx[u], vertex_to_idx[v]) for u, v in G_fund.edges()]\n",
    "            \n",
    "            if edge_list:\n",
    "                ig_G = ig.Graph(n=len(vertex_names), edges=edge_list)\n",
    "                ig_G.vs['_nx_name'] = vertex_names\n",
    "                partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "                communities = {ig_G.vs[i]['_nx_name']: p for p, cl in enumerate(partition) for i in cl}\n",
    "        except:\n",
    "            communities = {f: 0 for f in funds}\n",
    "    \n",
    "    # Build feature dataframe\n",
    "    fund_features = pd.DataFrame({\n",
    "        'fund': funds,\n",
    "        'degree': [degree_cent.get(f, 0) for f in funds],\n",
    "        'pagerank': [pagerank_cent.get(f, 0) for f in funds],\n",
    "        'hub': [hubs.get(f, 0) for f in funds],\n",
    "        'authority': [authorities.get(f, 0) for f in funds],\n",
    "        'closeness': [closeness_cent.get(f, 0) for f in funds],\n",
    "        'community': [communities.get(f, -1) for f in funds]\n",
    "    }).set_index('fund')\n",
    "    \n",
    "    return fund_features\n",
    "\n",
    "print(\"Feature computation function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE model with transfer learning defined.\n"
     ]
    }
   ],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\"2-layer GraphSAGE model for bipartite graphs.\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "print(\"GraphSAGE model defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction feature builder ready.\n"
     ]
    }
   ],
   "source": [
    "def create_incremental_link_features(G_bip_current, G_bip_future, embeddings_current, \n",
    "                                     fund_features, funds_all, stocks_all, \n",
    "                                     fund_to_idx, stock_to_idx, cumulative_edges):\n",
    "    \"\"\"\n",
    "    Create feature matrix for QUARTERLY INCREMENTAL link prediction.\n",
    "    \n",
    "    Training: Use ONLY current quarter's edges + hard negatives from unseen pairs.\n",
    "    Testing: Evaluate on next quarter; features computed from cumulative history only.\n",
    "    \n",
    "    Args:\n",
    "        G_bip_current: Current quarter's bipartite graph (training edges only)\n",
    "        G_bip_future: Next quarter's bipartite graph (test labels only)\n",
    "        embeddings_current: Node embeddings from fine-tuned GraphSAGE\n",
    "        fund_features: Topological features (from current quarter graph only)\n",
    "        funds_all, stocks_all: Complete node lists\n",
    "        fund_to_idx, stock_to_idx: Node to index mappings\n",
    "        cumulative_edges: Set of all edges seen up to and including current quarter\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test, updated_cumulative_edges\n",
    "    \"\"\"\n",
    "    fund_emb = embeddings_current[:len(funds_all)]\n",
    "    stock_emb = embeddings_current[len(funds_all):]\n",
    "    \n",
    "    # ── TRAINING DATA (from CURRENT quarter only) ──\n",
    "    pos_edges_train = [\n",
    "        (fund_to_idx[u], stock_to_idx[v]) \n",
    "        for u, v in G_bip_current.edges() \n",
    "        if u in fund_to_idx and v in stock_to_idx\n",
    "    ]\n",
    "    \n",
    "    # Hard negative sampling: high similarity but never seen in cumulative history\n",
    "    stock_sim = cosine_similarity(stock_emb)\n",
    "    neg_edges_train = []\n",
    "    \n",
    "    for f_idx in range(len(funds_all)):\n",
    "        fund_id = funds_all[f_idx]\n",
    "        connected_stocks_current = {stock_to_idx[s] for s in G_bip_current.neighbors(fund_id) \n",
    "                                   if s in stock_to_idx}\n",
    "        \n",
    "        if not connected_stocks_current:\n",
    "            continue\n",
    "        \n",
    "        connected_list = list(connected_stocks_current)\n",
    "        avg_sim = stock_sim[connected_list].mean(axis=0)\n",
    "        \n",
    "        hard_negs = np.argsort(-avg_sim)\n",
    "        hard_neg_list = [\n",
    "            s_idx for s_idx in hard_negs \n",
    "            if (fund_id, stocks_all[s_idx]) not in cumulative_edges\n",
    "            and len(neg_edges_train) < len(pos_edges_train)\n",
    "        ]\n",
    "        \n",
    "        neg_edges_train.extend([(f_idx, s_idx) for s_idx in hard_neg_list[:20]])\n",
    "    \n",
    "    neg_edges_train = neg_edges_train[:len(pos_edges_train)]\n",
    "    \n",
    "    # ── TEST DATA (evaluate on next quarter) ──\n",
    "    pos_edges_test = [\n",
    "        (fund_to_idx[u], stock_to_idx[v]) \n",
    "        for u, v in G_bip_future.edges() \n",
    "        if u in fund_to_idx and v in stock_to_idx and (u, v) not in cumulative_edges\n",
    "    ]\n",
    "    \n",
    "    all_possible = set((i, j) for i in range(len(funds_all)) for j in range(len(stocks_all)))\n",
    "    test_negatives_candidates = [\n",
    "        (i, j) for i, j in all_possible \n",
    "        if (funds_all[i], stocks_all[j]) not in cumulative_edges\n",
    "    ]\n",
    "    neg_edges_test = test_negatives_candidates[:max(len(pos_edges_test), 1)]\n",
    "    \n",
    "    # Build feature vectors\n",
    "    def build_features(edge_list):\n",
    "        features = []\n",
    "        for f_idx, s_idx in edge_list:\n",
    "            fund_id = funds_all[f_idx]\n",
    "            feat = np.concatenate([\n",
    "                fund_emb[f_idx],\n",
    "                stock_emb[s_idx],\n",
    "                fund_features.loc[fund_id].values if fund_id in fund_features.index else np.zeros(6)\n",
    "            ])\n",
    "            features.append(feat)\n",
    "        return np.array(features) if features else np.zeros((0, fund_emb.shape[1] + stock_emb.shape[1] + 6))\n",
    "    \n",
    "    X_train = np.vstack([\n",
    "        build_features(pos_edges_train),\n",
    "        build_features(neg_edges_train)\n",
    "    ])\n",
    "    y_train = np.hstack([np.ones(len(pos_edges_train)), np.zeros(len(neg_edges_train))])\n",
    "    \n",
    "    X_test = np.vstack([\n",
    "        build_features(pos_edges_test),\n",
    "        build_features(neg_edges_test)\n",
    "    ])\n",
    "    y_test = np.hstack([np.ones(len(pos_edges_test)), np.zeros(len(neg_edges_test))])\n",
    "    \n",
    "    # Update cumulative edges with current quarter's edges\n",
    "    updated_cumulative_edges = cumulative_edges.copy()\n",
    "    for u, v in G_bip_current.edges():\n",
    "        if u in fund_to_idx and v in stock_to_idx:\n",
    "            updated_cumulative_edges.add((u, v))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, updated_cumulative_edges\n",
    "\n",
    "print(\"Incremental link prediction feature builder ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e09f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning GraphSAGE function (incremental) ready.\n"
     ]
    }
   ],
   "source": [
    "def fine_tune_graphsage_quarterly(G_bip_current, graphsage_model, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Fine-tune GraphSAGE incrementally on current quarter.\n",
    "    Preserves weights from previous quarters (no resets).\n",
    "    \n",
    "    Args:\n",
    "        G_bip_current: Current quarter's bipartite graph (ONLY this quarter)\n",
    "        graphsage_model: Existing GraphSAGE model (None for Q1)\n",
    "        num_epochs: Fine-tuning iterations (lower for incremental updates)\n",
    "        learning_rate: Learning rate for fine-tuning\n",
    "    \n",
    "    Returns:\n",
    "        (updated_model, embeddings_numpy, funds, stocks)\n",
    "    \"\"\"\n",
    "    nodes = list(G_bip_current.nodes())\n",
    "    node_to_idx = {n: i for i, n in enumerate(nodes)}\n",
    "    \n",
    "    funds = [n for n in nodes if G_bip_current.nodes[n].get('bipartite') == 0]\n",
    "    stocks = [n for n in nodes if G_bip_current.nodes[n].get('bipartite') == 1]\n",
    "    \n",
    "    num_nodes = len(nodes)\n",
    "    x = torch.randn(num_nodes, 16, device=device)\n",
    "    \n",
    "    edge_list = [(node_to_idx[u], node_to_idx[v]) for u, v in G_bip_current.edges()]\n",
    "    if not edge_list:\n",
    "        print(\"    WARNING: Graph has no edges\")\n",
    "        return graphsage_model, np.zeros((num_nodes, 8)), funds, stocks\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long, device=device).t().contiguous()\n",
    "    edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "    \n",
    "    if graphsage_model is not None:\n",
    "        print(\"     → Fine-tuning existing model (weights preserved)\")\n",
    "        model = graphsage_model.to(device)\n",
    "    else:\n",
    "        print(\"     → Training new model (first quarter)\")\n",
    "        model = GraphSAGE(16, 32, 8).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, edge_index)\n",
    "        pos_score = (out[edge_index[0]] * out[edge_index[1]]).sum(dim=1).sigmoid()\n",
    "        loss = -torch.log(pos_score + 1e-15).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = model(x, edge_index).cpu().numpy()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, emb, funds, stocks\n",
    "\n",
    "print(\"Fine-tuning GraphSAGE function (incremental) ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "846bdf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IncrementalLinkPredictor class ready.\n"
     ]
    }
   ],
   "source": [
    "class IncrementalLinkPredictor:\n",
    "    \"\"\"\n",
    "    Incremental link predictor using SGDClassifier.\n",
    "    Supports partial_fit for quarterly online learning without weight resets.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        self.model = None\n",
    "        self.random_state = random_state\n",
    "        self.n_features_ = None\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        self.training_iterations = 0\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Update model on new quarter's data (incremental learning).\n",
    "        First call: initialize. Subsequent calls: update weights without reset.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            # Initialize on first quarter\n",
    "            self.model = SGDClassifier(\n",
    "                loss='log',\n",
    "                n_jobs=-1,\n",
    "                random_state=self.random_state,\n",
    "                warm_start=False,\n",
    "                max_iter=100,\n",
    "                tol=1e-3\n",
    "            )\n",
    "            self.model.fit(X, y)\n",
    "            self.n_features_ = X.shape[1]\n",
    "            print(f\"     → Initialized model: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        else:\n",
    "            # Incremental update: warm_start=True preserves weights\n",
    "            self.model.warm_start = True\n",
    "            self.model.max_iter = 10\n",
    "            self.model.partial_fit(X, y, classes=self.classes_)\n",
    "            print(f\"     → Updated model: {X.shape[0]} new samples\")\n",
    "        \n",
    "        self.training_iterations += 1\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probability of positive class.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return model for serialization.\"\"\"\n",
    "        return self.model if self.model is not None else None\n",
    "\n",
    "print(\"IncrementalLinkPredictor class ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for INCREMENTAL/ONLINE quarterly training\n",
    "INCREMENTAL_TRAINING = True\n",
    "\n",
    "print(f\"Incremental Training Configuration:\")\n",
    "print(f\"  Mode: Quarterly Online/Incremental Learning\")\n",
    "print(f\"  Model: SGDClassifier (partial_fit) + fine-tuned GraphSAGE\")\n",
    "print(f\"  Strategy: Update model each quarter, never reset weights\")\n",
    "print(f\"  Graph construction: Current quarter ONLY (no cumulative union)\")\n",
    "\n",
    "# Create directory for saved checkpoints\n",
    "models_dir = 'incremental_models'\n",
    "checkpoints_dir = os.path.join(models_dir, 'checkpoints')\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "print(f\"  Checkpoints saved to: {checkpoints_dir}/\")\n",
    "\n",
    "def get_chronological_quarters(quarterly_graphs):\n",
    "    \"\"\"Get all quarters in chronological order.\"\"\"\n",
    "    return sorted(quarterly_graphs.keys())\n",
    "\n",
    "print(\"\\nIncremental learning utilities initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "QUARTERLY INCREMENTAL LINK PREDICTION TRAINING (2021-2024)\n",
      "====================================================================================================\n",
      "Total quarters: 15\n",
      "Quarters: [(np.int64(2021), 1), (np.int64(2021), 2), (np.int64(2021), 3), (np.int64(2021), 4), (np.int64(2022), 1), (np.int64(2022), 2), (np.int64(2022), 3), (np.int64(2022), 4), (np.int64(2023), 1), (np.int64(2023), 2), (np.int64(2023), 3), (np.int64(2023), 4), (np.int64(2024), 1), (np.int64(2024), 2), (np.int64(2024), 3)]\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "QUARTER 1/15\n",
      "  Train: 2021Q1 | Test: 2021Q2\n",
      "  Cumulative edges seen: 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  1. Loading current quarter graph...\n",
      "     Funds: 5,773 | Stocks: 3,544 | Edges: 943,803\n",
      "  2. Fine-tuning GraphSAGE...\n",
      "     → Training new model (first quarter)\n",
      "     Embeddings: (9317, 8)\n",
      "  3. Computing topological features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"=\" * 100)\n",
    "print(f\"QUARTERLY INCREMENTAL LINK PREDICTION TRAINING (2021-2024)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "results_per_quarter = []\n",
    "chrono_quarters = get_chronological_quarters(quarterly_graphs)\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "# INITIALIZE INCREMENTAL COMPONENTS ONCE (OUTSIDE THE LOOP)\n",
    "# Model is created here and updated quarter-by-quarter without reset\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "graphsage_model = None  # Will be fine-tuned each quarter\n",
    "link_predictor = IncrementalLinkPredictor(random_state=42)  # Initialized ONCE\n",
    "cumulative_edges = set()  # Track all edges seen cumulatively\n",
    "\n",
    "print(f\"Total quarters: {len(chrono_quarters)}\")\n",
    "print(f\"Quarters: {chrono_quarters}\\n\")\n",
    "print(\"✓ Incremental model initialized (will be updated each quarter, never reset)\")\n",
    "print(\"✓ GraphSAGE will be fine-tuned incrementally\")\n",
    "print(\"✓ SGDClassifier will be updated with partial_fit\\n\")\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "# MAIN INCREMENTAL LOOP: Train on current quarter, test on next quarter\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "for quarter_idx, current_quarter in enumerate(chrono_quarters):\n",
    "    current_year, current_quarter_num = current_quarter\n",
    "    \n",
    "    # Check if next quarter exists for testing\n",
    "    if quarter_idx + 1 >= len(chrono_quarters):\n",
    "        print(f\"\\n{'─' * 100}\")\n",
    "        print(f\"QUARTER {quarter_idx + 1}/{len(chrono_quarters)} | {current_year}Q{current_quarter_num}\")\n",
    "        print(f\"(No next quarter for testing, skipping)\")\n",
    "        break\n",
    "    \n",
    "    next_quarter = chrono_quarters[quarter_idx + 1]\n",
    "    next_year, next_quarter_num = next_quarter\n",
    "    \n",
    "    print(f\"\\n{'─' * 100}\")\n",
    "    print(f\"QUARTER {quarter_idx + 1}/{len(chrono_quarters)}\")\n",
    "    print(f\"  Train: {current_year}Q{current_quarter_num} | Test: {next_year}Q{next_quarter_num}\")\n",
    "    print(f\"  Cumulative edges seen: {len(cumulative_edges):,}\")\n",
    "    print(f\"  Model training iterations so far: {link_predictor.training_iterations}\")\n",
    "    print(f\"{'─' * 100}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load current quarter graph (ONLY this quarter, no union)\n",
    "        print(\"  1. Loading current quarter graph...\")\n",
    "        G_bip_current = quarterly_graphs[current_quarter]\n",
    "        funds_current = [n for n in G_bip_current.nodes() if G_bip_current.nodes[n].get('bipartite') == 0]\n",
    "        stocks_current = [n for n in G_bip_current.nodes() if G_bip_current.nodes[n].get('bipartite') == 1]\n",
    "        \n",
    "        print(f\"     Funds: {len(funds_current):,} | Stocks: {len(stocks_current):,} | Edges: {G_bip_current.number_of_edges():,}\")\n",
    "        \n",
    "        if G_bip_current.number_of_nodes() == 0:\n",
    "            print(\"     WARNING: Graph is empty, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # 2. Fine-tune GraphSAGE on current quarter (incremental update)\n",
    "        print(\"  2. Fine-tuning GraphSAGE...\")\n",
    "        graphsage_model, embeddings, funds_sage, stocks_sage = fine_tune_graphsage_quarterly(\n",
    "            G_bip_current, graphsage_model, num_epochs=10, learning_rate=0.001\n",
    "        )\n",
    "        print(f\"     Embeddings: {embeddings.shape}\")\n",
    "        \n",
    "        # 3. Compute topological features (current quarter only)\n",
    "        print(\"  3. Computing topological features...\")\n",
    "        fund_features = compute_fund_features(G_bip_current, funds_sage)\n",
    "        print(f\"     Features: {fund_features.shape}\")\n",
    "        \n",
    "        # 4. Node index mappings\n",
    "        fund_to_idx = {f: i for i, f in enumerate(funds_sage)}\n",
    "        stock_to_idx = {s: i for i, s in enumerate(stocks_sage)}\n",
    "        \n",
    "        # 5. Create incremental features\n",
    "        print(\"  4. Creating incremental features...\")\n",
    "        G_bip_next = quarterly_graphs.get(next_quarter)\n",
    "        if G_bip_next is None:\n",
    "            print(f\"     WARNING: Next quarter not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, cumulative_edges = create_incremental_link_features(\n",
    "            G_bip_current, G_bip_next, embeddings, fund_features,\n",
    "            funds_sage, stocks_sage, fund_to_idx, stock_to_idx, cumulative_edges\n",
    "        )\n",
    "        print(f\"     Train: {X_train.shape[0]:,} (pos: {y_train.sum():.0f})\")\n",
    "        print(f\"     Test:  {X_test.shape[0]:,} (pos: {y_test.sum():.0f})\")\n",
    "        \n",
    "        if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "            print(\"     WARNING: No samples, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # 6. INCREMENTAL model update (partial_fit) - NO RESET, NO RE-INITIALIZATION\n",
    "        print(\"  5. Incremental model update (partial_fit)...\")\n",
    "        link_predictor.partial_fit(X_train, y_train)\n",
    "        print(f\"     Model training iterations now: {link_predictor.training_iterations}\")\n",
    "        \n",
    "        # 7. Evaluate on next quarter\n",
    "        print(\"  6. Evaluating on next quarter...\")\n",
    "        y_pred = link_predictor.predict_proba(X_test)\n",
    "        \n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        recall = recall_score(y_test, (y_pred > 0.5).astype(int), zero_division=0)\n",
    "        \n",
    "        print(f\"\\n     ✓ AUC:       {auc:.4f}\")\n",
    "        print(f\"     ✓ Precision: {precision:.4f}\")\n",
    "        print(f\"     ✓ Recall:    {recall:.4f}\")\n",
    "        \n",
    "        # 8. Save checkpoint\n",
    "        print(\"  7. Saving checkpoint...\")\n",
    "        checkpoint_filename = f\"checkpoint_q{quarter_idx+1}_{current_year}Q{current_quarter_num}.pkl\"\n",
    "        checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'quarter_index': quarter_idx + 1,\n",
    "            'current_quarter': current_quarter,\n",
    "            'next_quarter': next_quarter,\n",
    "            'graphsage_model': graphsage_model,\n",
    "            'link_predictor_model': link_predictor.get_params(),\n",
    "            'cumulative_edges': cumulative_edges,\n",
    "            'fund_to_idx': fund_to_idx,\n",
    "            'stock_to_idx': stock_to_idx,\n",
    "            'funds_sage': funds_sage,\n",
    "            'stocks_sage': stocks_sage,\n",
    "            'training_iterations': link_predictor.training_iterations,\n",
    "            'metrics': {\n",
    "                'auc': auc,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        print(f\"     ✓ Saved to: {checkpoint_path}\")\n",
    "        \n",
    "        results_per_quarter.append({\n",
    "            'quarter_idx': quarter_idx + 1,\n",
    "            'train_year': current_year,\n",
    "            'train_quarter': current_quarter_num,\n",
    "            'test_year': next_year,\n",
    "            'test_quarter': next_quarter_num,\n",
    "            'n_train_funds': len(funds_current),\n",
    "            'n_train_stocks': len(stocks_current),\n",
    "            'train_edges': G_bip_current.number_of_edges(),\n",
    "            'test_edges': G_bip_next.number_of_edges(),\n",
    "            'cumulative_edges': len(cumulative_edges),\n",
    "            'auc': auc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'n_test_samples': X_test.shape[0],\n",
    "            'checkpoint_path': checkpoint_path,\n",
    "            'training_iterations': link_predictor.training_iterations\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "# SAVE FINAL TRAINED MODEL TO DISK\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"\\n\\n{'=' * 100}\")\n",
    "print(f\"SAVING FINAL INCREMENTAL MODEL\")\n",
    "print(f\"{'=' * 100}\")\n",
    "\n",
    "if link_predictor.training_iterations > 0:\n",
    "    # Save the final trained model\n",
    "    final_model_path = 'final_incremental_model.pkl'\n",
    "    \n",
    "    final_model_data = {\n",
    "        'model': link_predictor.get_params(),\n",
    "        'total_training_iterations': link_predictor.training_iterations,\n",
    "        'cumulative_edges_final': cumulative_edges,\n",
    "        'graphsage_model': graphsage_model,\n",
    "        'training_completed': True,\n",
    "        'description': 'Final incremental link predictor trained on all quarters (2021-2024)',\n",
    "        'timestamp': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(final_model_path, 'wb') as f:\n",
    "        pickle.dump(final_model_data, f)\n",
    "    \n",
    "    print(f\"\\n✓ Final model saved to: {final_model_path}\")\n",
    "    print(f\"  - Model type: SGDClassifier (warm_start enabled)\")\n",
    "    print(f\"  - Total training iterations: {link_predictor.training_iterations}\")\n",
    "    print(f\"  - Cumulative edges learned: {len(cumulative_edges):,}\")\n",
    "    print(f\"  - Model weights: ACCUMULATED over all {len(chrono_quarters)} quarters\")\n",
    "    print(f\"  - Status: READY FOR INFERENCE\")\n",
    "else:\n",
    "    print(\"\\n⚠ No training completed, final model not saved\")\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "# SUMMARY REPORT\n",
    "# ════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"\\n\\n{'=' * 100}\")\n",
    "print(f\"INCREMENTAL LEARNING SUMMARY (2021-2024)\")\n",
    "print(f\"{'=' * 100}\")\n",
    "\n",
    "if results_per_quarter:\n",
    "    results_df = pd.DataFrame(results_per_quarter)\n",
    "    print(f\"\\nResults across {len(results_df)} evaluation windows:\\n\")\n",
    "    display_cols = ['quarter_idx', 'train_year', 'train_quarter', 'test_year', 'test_quarter', \n",
    "                   'training_iterations', 'auc', 'precision', 'recall']\n",
    "    print(results_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n\\nAggregate Statistics:\")\n",
    "    print(f\"  Average AUC:       {results_df['auc'].mean():.4f} (±{results_df['auc'].std():.4f})\")\n",
    "    print(f\"  Average Precision: {results_df['precision'].mean():.4f} (±{results_df['precision'].std():.4f})\")\n",
    "    print(f\"  Average Recall:    {results_df['recall'].mean():.4f} (±{results_df['recall'].std():.4f})\")\n",
    "    \n",
    "    print(f\"\\n\\nBy Test Year:\")\n",
    "    by_year = results_df.groupby('test_year')[['auc', 'precision', 'recall']].mean()\n",
    "    print(by_year)\n",
    "    \n",
    "    print(f\"\\n\\nTraining Progression:\")\n",
    "    print(results_df[['quarter_idx', 'train_year', 'train_quarter', 'training_iterations']].to_string(index=False))\n",
    "    \n",
    "    results_df.to_csv('incremental_link_prediction_results.csv', index=False)\n",
    "    print(f\"\\n✓ Results saved to incremental_link_prediction_results.csv\")\n",
    "    print(f\"✓ {len(results_df)} checkpoints saved to {checkpoints_dir}/\")\n",
    "    print(f\"✓ Final model saved to: final_incremental_model.pkl\")\n",
    "    print(f\"✓ Model weights accumulated over {len(chrono_quarters)} quarters (NEVER RESET)\")\n",
    "else:\n",
    "    print(\"No results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c72e9",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "Analyze incremental learning performance across quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display incremental results\n",
    "import os\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"INCREMENTAL LEARNING RESULTS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "incremental_results = None\n",
    "\n",
    "if os.path.exists('incremental_link_prediction_results.csv'):\n",
    "    incremental_results = pd.read_csv('incremental_link_prediction_results.csv')\n",
    "    print(f\"\\n✓ Loaded results: {len(incremental_results)} quarters evaluated\\n\")\n",
    "    \n",
    "    print(\"Performance by Quarter:\")\n",
    "    print(incremental_results[['quarter_idx', 'train_year', 'train_quarter', \n",
    "                               'test_year', 'test_quarter', 'auc', 'precision', 'recall']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n\\nOverall Statistics:\")\n",
    "    print(f\"  AUC:       {incremental_results['auc'].mean():.4f} ± {incremental_results['auc'].std():.4f}\")\n",
    "    print(f\"  Precision: {incremental_results['precision'].mean():.4f} ± {incremental_results['precision'].std():.4f}\")\n",
    "    print(f\"  Recall:    {incremental_results['recall'].mean():.4f} ± {incremental_results['recall'].std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n\\nModel Knowledge Accumulation:\")\n",
    "    print(f\"  Total quarters processed: {incremental_results['quarter_idx'].max()}\")\n",
    "    print(f\"  Final training iterations: {incremental_results['training_iterations'].iloc[-1]}\")\n",
    "    print(f\"  Cumulative edges by final quarter: {incremental_results['cumulative_edges'].iloc[-1]:,}\")\n",
    "else:\n",
    "    print(\"⚠ Results file not found: incremental_link_prediction_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
